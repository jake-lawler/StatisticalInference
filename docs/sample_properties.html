<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Properties of a Random Sample | Notes on ‘Statistical Inference’ by Casella &amp; Berger</title>
  <meta name="description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Properties of a Random Sample | Notes on ‘Statistical Inference’ by Casella &amp; Berger" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  <meta name="github-repo" content="jake-lawler/StatisticalInference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Properties of a Random Sample | Notes on ‘Statistical Inference’ by Casella &amp; Berger" />
  
  <meta name="twitter:description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  

<meta name="author" content="Jake Lawler" />


<meta name="date" content="2022-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multiple_variables.html"/>
<link rel="next" href="data_reduction.html"/>
<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="probability_theory.html"><a href="probability_theory.html"><i class="fa fa-check"></i><b>1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability_theory.html"><a href="probability_theory.html#chapter-notes"><i class="fa fa-check"></i><b>1.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#counting"><i class="fa fa-check"></i>Counting</a></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#inequalities-bonferronis-and-booles"><i class="fa fa-check"></i>Inequalities: Bonferroni’s and Boole’s</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability_theory.html"><a href="probability_theory.html#questions"><i class="fa fa-check"></i><b>1.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#ex-1.42"><i class="fa fa-check"></i>Ex 1.42</a></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#ex-1.43"><i class="fa fa-check"></i>Ex 1.43</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#further-reading"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and Expectations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="transformations.html"><a href="transformations.html#chapter-notes-1"><i class="fa fa-check"></i><b>2.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#monotonic-transformations"><i class="fa fa-check"></i>Monotonic Transformations</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#generating-random-samples"><i class="fa fa-check"></i>Generating Random Samples</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#moment-generating-functions"><i class="fa fa-check"></i>Moment Generating Functions</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#differentiating-under-an-integral-sign"><i class="fa fa-check"></i>Differentiating Under an Integral Sign</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="transformations.html"><a href="transformations.html#questions-1"><i class="fa fa-check"></i><b>2.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.1"><i class="fa fa-check"></i>Ex 2.1</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.3"><i class="fa fa-check"></i>Ex 2.3</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.6"><i class="fa fa-check"></i>Ex 2.6</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#further-reading-1"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="families.html"><a href="families.html"><i class="fa fa-check"></i><b>3</b> Common Families of Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="families.html"><a href="families.html#chapter-notes-2"><i class="fa fa-check"></i><b>3.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="families.html"><a href="families.html#discrete-distributions"><i class="fa fa-check"></i>Discrete Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="families.html"><a href="families.html#questions-2"><i class="fa fa-check"></i><b>3.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="families.html"><a href="families.html#further-reading-2"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple_variables.html"><a href="multiple_variables.html"><i class="fa fa-check"></i><b>4</b> Multiples Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple_variables.html"><a href="multiple_variables.html#chapter-notes-3"><i class="fa fa-check"></i><b>4.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#independence"><i class="fa fa-check"></i>Independence</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#bivariate-transformations"><i class="fa fa-check"></i>Bivariate Transformations</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#hierarchical-models-and-mixture-distributions"><i class="fa fa-check"></i>Hierarchical Models and Mixture Distributions</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#covariance-and-correlation"><i class="fa fa-check"></i>Covariance and Correlation</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#multivariate-distribution"><i class="fa fa-check"></i>Multivariate Distribution</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#inequalities"><i class="fa fa-check"></i>Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple_variables.html"><a href="multiple_variables.html#questions-3"><i class="fa fa-check"></i><b>4.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#ex-4.39"><i class="fa fa-check"></i>Ex 4.39</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#further-reading-3"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sample_properties.html"><a href="sample_properties.html"><i class="fa fa-check"></i><b>5</b> Properties of a Random Sample</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sample_properties.html"><a href="sample_properties.html#chapter-notes-4"><i class="fa fa-check"></i><b>5.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i>Sums of Random Variables from a Random Sample</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#sums-from-the-normal-distribution"><i class="fa fa-check"></i>Sums from the Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#convergence-concepts"><i class="fa fa-check"></i>Convergence Concepts</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#proving-tools"><i class="fa fa-check"></i>Proving Tools</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#generating-a-random-sample"><i class="fa fa-check"></i>Generating a Random Sample</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sample_properties.html"><a href="sample_properties.html#questions-4"><i class="fa fa-check"></i><b>5.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#further-reading-4"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data_reduction.html"><a href="data_reduction.html"><i class="fa fa-check"></i><b>6</b> Principles of Data Reduction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data_reduction.html"><a href="data_reduction.html#chapter-notes-5"><i class="fa fa-check"></i><b>6.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-sufficiency-principle"><i class="fa fa-check"></i>The Sufficiency Principle</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-likelihood-principle"><i class="fa fa-check"></i>The Likelihood Principle</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-equivariance-principle"><i class="fa fa-check"></i>The Equivariance Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data_reduction.html"><a href="data_reduction.html#questions-5"><i class="fa fa-check"></i><b>6.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#further-reading-5"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="point_estimation.html"><a href="point_estimation.html"><i class="fa fa-check"></i><b>7</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="point_estimation.html"><a href="point_estimation.html#chapter-notes-6"><i class="fa fa-check"></i><b>7.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#method-of-moments"><i class="fa fa-check"></i>Method of Moments</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i>Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#bayes-estimators"><i class="fa fa-check"></i>Bayes Estimators</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#mean-squared-error"><i class="fa fa-check"></i>Mean Squared Error</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#best-unbiased-estimators"><i class="fa fa-check"></i>Best Unbiased Estimators</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#sufficiency-and-unbiasedness"><i class="fa fa-check"></i>Sufficiency and Unbiasedness</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#loss-function-optimality"><i class="fa fa-check"></i>Loss Function Optimality</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="point_estimation.html"><a href="point_estimation.html#questions-6"><i class="fa fa-check"></i><b>7.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#further-reading-6"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html"><i class="fa fa-check"></i><b>8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#chapter-notes-7"><i class="fa fa-check"></i><b>8.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#likelihood-ratio-tests"><i class="fa fa-check"></i>Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#bayesian-tests"><i class="fa fa-check"></i>Bayesian Tests</a></li>
<li class="chapter" data-level="" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#error-probabilities-and-the-power-function"><i class="fa fa-check"></i>Error Probabilities and the Power Function</a></li>
<li class="chapter" data-level="" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#most-powerful-tests"><i class="fa fa-check"></i>Most Powerful Tests</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#questions-7"><i class="fa fa-check"></i><b>8.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="hypothesis_testing.html"><a href="hypothesis_testing.html#further-reading-7"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on ‘Statistical Inference’ by Casella &amp; Berger</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sample_properties" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Properties of a Random Sample</h1>
<div id="chapter-notes-4" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Chapter Notes</h2>
<p>A random sample of size <span class="math inline">\(n\)</span> means we have <span class="math inline">\(n\)</span> mutually independent and identically distributed random variables <span class="math inline">\(X_1, \dots, X_n\)</span>. Because they are all independent and have the same marginal pdf, their joint pdf is given by:</p>
<p><span class="math display">\[
f(x_1, \dots x_n) = f(x_1)f(x_2)\dots f(x_n) = \prod_{i=1}^n f(x_i)
\]</span>
and we can use it to calculate the probabilities related to our sample.</p>
<p>This definition of a random sample is called sampling from an <em>infinite</em> population. However we’re very often interested in working with <em>finite</em> populations. If we’re sampling with replacement - the conditions of our definition of a random sample are met, but if we’re sampling without replacement from a finite population, our random variables are no longer independent.</p>
<p>Sampling from a finite population without replacement is sometimes called <em>simple random sampling</em>. It doesn’t meet the criteria of our definition above, but if population size <span class="math inline">\(N\)</span> is large enough relative to our sample size <span class="math inline">\(n\)</span>, we might say that the samples are <em>nearly independent</em>. This means that the conditional distribution of <span class="math inline">\(X_i\)</span> given the other <span class="math inline">\(n-1\)</span> random variables is not too dissimilar from the marginal distribution of <span class="math inline">\(X_i\)</span>.</p>
<div id="sums-of-random-variables-from-a-random-sample" class="section level3 unnumbered">
<h3>Sums of Random Variables from a Random Sample</h3>
<p>So what can we do with these random samples? The chapter introduces the concept of a <em>statistic</em></p>
<p>If we have a real-valued (or vector-valued) function <span class="math inline">\(T\)</span> whose domain includes the sample space of (<span class="math inline">\(X_1, \dots, X_n\)</span>), then we call the random variable (or random vector) <span class="math inline">\(Y = T(X_1, \dots, X_n)\)</span> a statistic. We call the probability distribution of <span class="math inline">\(Y\)</span> the <em>sampling distribution</em> of <span class="math inline">\(Y\)</span>.</p>
<p>The chapter notes that the definition of a statistic is very broad, but notes that it cannot be a function of a parameter - it is a function of the sample only. Examples might include the average sample value, the highest and lowest values in the sample, some measure of variability in the sample etc.</p>
<p>The sample mean statistic is:
<span class="math display">\[
\overline X = \frac{1}{n} \sum_{i=1}^n X_i
\]</span>
The sample variance statistic is:
<span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i -\overline X)^2 = \frac{1}{n-1}\left( \sum_{i=1}^n X_i^2 - n\overline X^2 \right)
\]</span></p>
<p>with the last equality because the sum of all <span class="math inline">\(x_i - \overline x\)</span> is 0.</p>
<p>We have the <span class="math inline">\(n-1\)</span> is the denominator of the sample variance. The following theorem shows why:</p>
<p><strong>Theorem 5.2.6 - Unbiased Estimators</strong></p>
<p>Let <span class="math inline">\(X_1, \dots X_n\)</span> be a random sample from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2 &lt; \infty\)</span>. Then:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(E\overline X = \mu\)</span>
b.<span class="math inline">\(\text{Var}(\overline X) = \frac{\sigma^2}{n}\)</span></li>
<li><span class="math inline">\(E S^2 = \sigma^2\)</span></li>
</ol>
<p>I’ll focus on part c - that the sample variance defined above is an <em>unbiased estimator</em> of the population variance. Here’s the proof:</p>
<p><span class="math display">\[
\begin{aligned}
ES^2 &amp;= E\left( \frac{1}{n-1} \left[ \sum_{i=1}^n X_i^2 - n\overline X^2 \right] \right) \\
&amp;= \frac{1}{n-1} \left( nEX_1^2 - n E\overline X^2 \right)  \\
&amp;= \frac{1}{n-1} \left( n(\sigma^2 + \mu^2) - n \left( \frac{\sigma^2}{n} + \mu^2\right) \right)  \\
&amp;= \frac{n\sigma^2}{n-1} + \frac{n\mu^2}{n-1} -  \frac{\sigma^2}{n-1} - \frac{n\mu^2}{n-1} =  \frac{(n-1)\sigma^2}{n-1}  \\
&amp;= \sigma^2
\end{aligned}
\]</span></p>
<p>The third equality comes from rearranging the property of variances <span class="math inline">\(\text{Var}(X) = E(X^2) - E(X)^2\)</span>, and from the expression for the variance of the sample mean in part b of the theorem. Note that if the sampling variance had used an <span class="math inline">\(n\)</span> in the denominator rather than an <span class="math inline">\(n-1\)</span>, the expected value of <span class="math inline">\(S^2\)</span> would be <span class="math inline">\(\frac{n-1}{n}\sigma^2\)</span>.</p>
</div>
<div id="sums-from-the-normal-distribution" class="section level3 unnumbered">
<h3>Sums from the Normal Distribution</h3>
<p>The properties of our sample mean and variance above required very few conditions. When we add the additional assumptions of normality, we can derive a lot more.</p>
<p><strong>Theorem 5.3.1</strong> in the book states that if we have a random sample drawn from a <span class="math inline">\(\text{Normal}(\mu, \sigma^2)\)</span> distribution, we have:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\overline X\)</span> and <span class="math inline">\(S^2\)</span> are independent random variables</li>
<li><span class="math inline">\(\overline X\)</span> has a a <span class="math inline">\(\text{Normal}(\mu, \sigma^2/n)\)</span> distribution</li>
<li><span class="math inline">\(\frac{(n-1)S^2}{\sigma^2}\)</span> has a chi-squared distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</li>
</ol>
<p>Here are some facts about chi-squared random variables from <strong>Lemma 5.3.2</strong></p>
<ol style="list-style-type: lower-alpha">
<li>The square of a standard normal (<span class="math inline">\(\text{Normal}(0,1)\)</span>) random variable is a chi-squared random variable with degree 1 (<span class="math inline">\(Z^2 \sim \chi^2_1\)</span>).</li>
<li>Independent chi-squared variables add to a chi-squared variable, and their degrees of freedom also add: <span class="math inline">\(X_1 + \dots + X_n \sim \chi^2_{p_1 + \dots + p_n}\)</span> if <span class="math inline">\(X_i \sim \chi^2_{p_i}\)</span> and the <span class="math inline">\(X_i\)</span>s are independent.</li>
</ol>
<p>In most practical cases, our population variance <span class="math inline">\(\sigma^2\)</span> is unknown. So if we want to estimate the variability of <span class="math inline">\(\overline X\)</span> as part of making inferences about <span class="math inline">\(\mu\)</span>, we need a way to estimate <span class="math inline">\(\sigma^2\)</span>.</p>
<p>For a random sample of size <span class="math inline">\(n\)</span> from a <span class="math inline">\(\text{Normal}(\mu,\sigma^2)\)</span> distribution, we know that</p>
<p><span class="math display">\[
\frac{\overline X - \mu}{\sigma/\sqrt n}
\]</span></p>
<p>has a <span class="math inline">\(\text{Normal}(0,1)\)</span> distribution. W.S. Gosset introduced the Student’s t-distribution, and showed that</p>
<p><span class="math display">\[
\frac{\overline X - \mu}{S/\sqrt n}
\]</span></p>
<p>has Student’s t distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. The pdf with <span class="math inline">\(p\)</span> degrees of freedom is:</p>
<p><span class="math display">\[
f_{T}(t)=\frac{\Gamma\left(\frac{p-1}{2}\right)}{\Gamma\left(\frac{p}{2}\right)} \frac{1}{(p \pi)^{1 / 2}} \frac{1}{\left(1+t^{2} / p\right)^{(p+1) / 2}}, \quad-\infty&lt;t&lt;\infty
\]</span></p>
<p>We have properties:</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{E} T_{p}&amp;=0, \quad \text{if } p&gt;1 \\
\operatorname{Var} T_{p}&amp;=\frac{p}{p-2}, \quad \text{if } p&gt;2
\end{aligned}
\]</span></p>
<p>The chapter also introduces Snedecor’s F distribution, which is used for comparing the variability of two different populations. Say we have <span class="math inline">\(n\)</span> random variables <span class="math inline">\(X_i\)</span> from a <span class="math inline">\(\text{Normal}(\mu_X,\sigma^2_X)\)</span> distribution, and <span class="math inline">\(m\)</span> random variables <span class="math inline">\(Y_i\)</span> from a <span class="math inline">\(\text{Normal}(\mu_Y,\sigma^2_Y)\)</span> distribution. The random variable:</p>
<p><span class="math display">\[
F = \frac{S_{X}^{2} / S_{Y}^{2}}{\sigma_{X}^{2} / \sigma_{Y}^{2}}=\frac{S_{X}^{2} / \sigma_{X}^{2}}{S_{Y}^{2} / \sigma_{Y}^{2}}
\]</span></p>
<p>has F distribution with <span class="math inline">\(n-1\)</span> and <span class="math inline">\(m-1\)</span> degrees of freedom. Here’s the pdf of a <span class="math inline">\(F_{p,q}\)</span> distribution:</p>
<p><span class="math display">\[
f_{F}(x)=\frac{\Gamma\left(\frac{p+q}{2}\right)}{\Gamma\left(\frac{p}{2}\right) \Gamma\left(\frac{q}{2}\right)}\left(\frac{p}{q}\right)^{p / 2} \frac{x^{(p / 2)-1}}{[1+(p / q) x]^{(p+q) / 2}}, \quad 0&lt;x&lt;\infty
\]</span></p>
</div>
<div id="convergence-concepts" class="section level3 unnumbered">
<h3>Convergence Concepts</h3>
<p>Here’s a nice summary from the chapter about what this section is all about:</p>
<blockquote>
<p>This section treats the somewhat fanciful idea of allowing the sample size to approach infinity and investigates the behavior of certain sample quantities as this happens. Although the notion of an infinite sample size is a theoretical artifact, it can often provide us with some useful approximations for the finite-sample case, since it usually happens that expressions become simplified in the limit.</p>
</blockquote>
<p>We’ll see three notions of convergence: convergence in probability, almost sure convergence, and convergence in distribution.</p>
<p><strong>Definition 5.5.1 Convergence in Probability</strong></p>
<p>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> convergences in probability to a random variable <span class="math inline">\(X\)</span> if for every <span class="math inline">\(\epsilon &gt;0\)</span></p>
<p><span class="math display">\[
\lim_{n \to \infty} P(|X_n - X| \geq \epsilon) = 0
\]</span>
or equivalently</p>
<p><span class="math display">\[
\lim_{n \to \infty} P(|X_n - X| &lt; \epsilon) = 1
\]</span></p>
<p>The <span class="math inline">\(X_i\)</span>s here are typically not iid, the distributions change as the sequence progresses.</p>
<p>The Weak Law of Large Numbers concerns a situation where <span class="math inline">\(X\)</span> is a constant, and the random variables in the sequence are sample means:</p>
<p><strong>Theorem 5.5.2 The Weak Law of Large Numbers</strong></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid with <span class="math inline">\(EX_i = \mu\)</span> and <span class="math inline">\(\text{Var}(X_i) = \sigma^2 &lt; \infty\)</span>. Define <span class="math inline">\(\overline X = 1/n \sum_{i=1}^n X_i\)</span>. Then for every <span class="math inline">\(\epsilon &gt; e\)</span>,</p>
<p><span class="math display">\[
\lim_{n \to \infty} P(|\overline X_n - \mu| &lt; \epsilon) = 1
\]</span>
I.e. our sample means converge in probability to <span class="math inline">\(\mu\)</span>.</p>
<p>The proof is a two-line application of Chebychev’s Inequality. Revisit once I’ve returned to the inequality section of chapter four.</p>
<p>If random variables <span class="math inline">\(X_1, X_2, \dots\)</span> converge in probability to <span class="math inline">\(X\)</span>, then <span class="math inline">\(h(X_1), h(X_2), \dots\)</span> converges in probability to <span class="math inline">\(h(X)\)</span>, for continous function <span class="math inline">\(h\)</span>.</p>
<p><strong>Definition 5.5.6 Almost Sure Convergence</strong></p>
<p>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> convergences almost surely to a random variable <span class="math inline">\(X\)</span> if for every <span class="math inline">\(\epsilon &gt;0\)</span></p>
<p><span class="math display">\[
P(\lim_{n \to \infty} |X_n - X| &lt; \epsilon) = 1
\]</span></p>
<p>This is a much stronger condition, and almost-sure convergence implies convergence in probability. From the chapter:</p>
<blockquote>
<p>To understand almost sure convergence, we must recall the basic definition of a random variable as given in Definition 1.4.1. A random variable is a real-valued function defined on a sample space <span class="math inline">\(S\)</span>. If a sample space <span class="math inline">\(S\)</span> has elements denoted by <span class="math inline">\(s\)</span>, then <span class="math inline">\(X_n(s)\)</span> and <span class="math inline">\(X(s)\)</span> are all functions defined on <span class="math inline">\(S\)</span>. Definition 5.5.6 states that <span class="math inline">\(X_n\)</span> converges to <span class="math inline">\(X\)</span> almost surely if the functions <span class="math inline">\(X_n(s)\)</span> converge to <span class="math inline">\(X(s)\)</span> for all <span class="math inline">\(s \in S\)</span> except perhaps for <span class="math inline">\(s \in N\)</span>, where <span class="math inline">\(N \subset S\)</span> and <span class="math inline">\(P(N) = 0\)</span>.</p>
</blockquote>
<p>As stated in that paragraph, convergence need not happen on sets with probability 0, which is why this is called “almost-sure” convergence.</p>
<p>As an example, the chapter uses <span class="math inline">\(X_n(s) = s+s^n\)</span> and <span class="math inline">\(X(s)=s\)</span>, where <span class="math inline">\(s \in [0,1]\)</span>. <span class="math inline">\(X_n\)</span> converges to <span class="math inline">\(X\)</span> everywhere except 1.</p>
<p>As an example of a sequence that converges in probability but not almost surely, the chapter uses <span class="math inline">\(X(s) = s\)</span> and:</p>
<p><span class="math display">\[
\begin{aligned}
X_1(s) &amp;= s+I_{[0,1]}(s) \\
X_2(s) &amp;= s+I_{[0,\frac{1}{2}]}(s) \\
X_3(s) &amp;= s+I_{[\frac{1}{2},1]}(s) \\
X_4(s) &amp;= s+I_{[0,\frac{1}{3}]}(s) \\
X_5(s) &amp;= s+I_{[\frac{1}{3},\frac{2}{3}]}(s) \\
X_6(s) &amp;= s+I_{[\frac{2}{3},1]}(s) \\
&amp;\vdots
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(s \in S = [0,1]\)</span>.</p>
<p>The probability that <span class="math inline">\(|X_n - X| \geq \epsilon\)</span> is the probability of an interval with length going to 0 - so the sequence converges in probability.</p>
<p>However the sequence does not converge almost surely, and in fact there is no <span class="math inline">\(s\)</span> such that <span class="math inline">\(X_n(s) \to s = X(s)\)</span>. For any <span class="math inline">\(s\)</span> the sequence will alternate between <span class="math inline">\(s\)</span> and <span class="math inline">\(s+1\)</span> infinitely often. Almost sure convergence requires convergence everywhere except perhaps sets with probability 0. The sequence above has convergence nowhere.</p>
<p><strong>Theorem 5.5.9 The Strong Law of Large Numbers</strong></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots\)</span> be iid with <span class="math inline">\(EX_i = \mu\)</span> and <span class="math inline">\(\text{Var}(X_i) = \sigma^2 &lt; \infty\)</span>. Define <span class="math inline">\(\overline X = 1/n \sum_{i=1}^n X_i\)</span>. Then for every <span class="math inline">\(\epsilon &gt; e\)</span>,</p>
<p><span class="math display">\[
P(\lim_{n \to \infty} |\overline X_n - \mu| &lt; \epsilon) = 1
\]</span>
I.e. our sample means converge almost surely to <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Definition 5.5.6 Convergence in Distribution</strong></p>
<p>A sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span> convergences in distribution to a random variable <span class="math inline">\(X\)</span> if</p>
<p><span class="math display">\[
\lim_{n \to \infty} F_{X_n}(x) = F_X(x)
\]</span></p>
<p>at all points <span class="math inline">\(x\)</span> where <span class="math inline">\(F_X(x)\)</span> is continuous.</p>
<p>It’s really the cdfs that converge and not the random variables. Convergence in probability implies convergence in distribution.</p>
<p>With the concept of convergence in distribution in hand, the chapter introduces the Central Limit Theorem:</p>
<p><strong>Theorem 5.5.14 The Central Limit Theorem (Stronger Form)</strong></p>
<p>Let <span class="math inline">\(X_1,X_2, \dots\)</span> be a sequence of iid random variables with <span class="math inline">\(EX_i = \mu\)</span> and <span class="math inline">\(0 &lt; \text{Var}X_i = \sigma^2 &lt; \infty\)</span>.
Define <span class="math inline">\(\overline X = 1/n \sum_{i=1}^n X_i\)</span>. Let G_n(x) denote the cdf of <span class="math inline">\(\sqrt n (\overline X_n - \mu)/ \sigma\)</span> Then, for any <span class="math inline">\(x\)</span>, <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>.</p>
<p><span class="math display">\[
\lim_{n \to \infty} G_n(x) = \int^x_{-\infty} \frac{1}{\sqrt{2 \pi}} e^{-y^2/2} dy
\]</span>
that is, <span class="math inline">\(\frac{\sqrt n (\overline X - \mu)}{\sigma}\)</span> has a limiting standard normal distribution.</p>
<p>What does this mean? The chapter stresses that we have arrived at normality from only very general starting assumptions, pretty impressive!</p>
<p><strong>Example 5.5.16 Normal Approximation to the Negative Binomial</strong></p>
<p>Say we have a random sample of size <span class="math inline">\(n\)</span> from a negative binomial<span class="math inline">\((r,p\)</span> distribution. Recall that</p>
<p><span class="math display">\[
\begin{aligned}
EX &amp;= \frac{r(1-p)}{p} \\
\text{Var}X &amp;= \frac{r(1-p)}{p^2}
\end{aligned}
\]</span></p>
<p>and so by the Central Limit Theorem:</p>
<p><span class="math display">\[
\frac{\sqrt{n}(\overline{X}-r(1-p) / p)}{\sqrt{r(1-p) / p^{2}}}
\]</span></p>
<p>is approximately <span class="math inline">\(\text{Normal}(0,1)\)</span>.</p>
</div>
<div id="proving-tools" class="section level3 unnumbered">
<h3>Proving Tools</h3>
<p>The chapter doesn’t call these proving tools, that terminology comes from the <a href="https://oliverychen.github.io/files/doc/CB.pdf">handbook</a> I’m referring to alongside <em>Statistical Inference</em> itself. It uses the term “proving tools” to refer to Slutsky’s theorem, the Delta method, and Taylor expansion.</p>
<p>The chapter <em>does</em> call Slutsky’s Theorem “[a]n approximation tool that can be used in conjunction with the Central Limit Theorem”.</p>
<p><strong>Theorem 5.5.17 Slutsky’s Theorem</strong></p>
<p>If <span class="math inline">\(X_n \to X\)</span> in distribution and <span class="math inline">\(Y_n \to a\)</span>, a constant, in probability, then</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Y_nX_n \to aX\)</span> in distribution</li>
<li><span class="math inline">\(X_n + Y_n \to X+a\)</span> in distribution.</li>
</ol>
<p>Revist: Delta Method, Taylor expansion.</p>
</div>
<div id="generating-a-random-sample" class="section level3 unnumbered">
<h3>Generating a Random Sample</h3>
<p>Starting from generated uniform iid random variables, we want methods to transform these into random variables from a target distribution.</p>
<p>We saw the inverse CDF method in chapter 2. This is a direct method, in the sense that it gives us a closed-form function for generating our random variable. Unfortunately, sometime closed-form functions for the inverse cdf do not exist and we’d need to solve a (perhaps very complicated) integral to get our random variable. This is the case for generating a <span class="math inline">\(\chi^2_1\)</span> random variable, which would get us a <span class="math inline">\(\text{Normal}(0,1)\)</span> variable.</p>
<p>What are indirect methods?</p>
<p>Revisit: page 251 - add accept/ reject algorithm and metropolis algorithm.</p>
</div>
</div>
<div id="questions-4" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Questions</h2>
</div>
<div id="further-reading-4" class="section level2 unnumbered">
<h2>Further Reading</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multiple_variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data_reduction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-sample_properties.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

[["index.html", "Notes on Statistical Inference by Casella &amp; Berger Preface", " Notes on Statistical Inference by Casella &amp; Berger Jake Lawler 2022-03-07 Preface These are my notes on the second edition of George Casella and Roger Bergers Statistical Inference. They are incomplete, Im still working my way through the book. Im broadly following this handbook by Oliver Y. Ch√©n which goes through each chapter and highlights the important concepts and theorems, as well as suggesting some questions to attempt. However most of my chapter notes so far include a little more detail than is suggested by the handbook, as well as some plots and animations to present some of the points visually. To revisit: Chapter 1 Animating the markers for unordered with replacement counts Question 1.4.3 (b) Chapter 2 Point 10 from the handbook on Lipschitz condition Questions from handbook Chapter 3 More animations / maybe a Shiny app Chapter 4 Expand the section on inequalities. Exercise 4.39 Chapter 5 - 12 not yet complete. "],["probability_theory.html", "Chapter 1 Probability Theory 1.1 Chapter Notes 1.2 Questions Further Reading", " Chapter 1 Probability Theory 1.1 Chapter Notes This chapter contains fairly standard material on basic probability theory. I wont summarise it here, but will instead pick out a couple of points to explore further. Counting In the section on counting, we find this table: with \\[ \\binom{n}{r} = \\frac{n!}{r!(n-r)!}. \\] The formula for unordered, with replacement counting is the one I find difficult to recreate, so Ill go into more detail on it here. The chapter describes choosing, say, 6 balls from 44 numbered lottery balls. We want to count how many arrangements of these six balls there are, assuming we are choosing with replacement and we dont care about the order. The chapter encourages re-interpreting this problem as one where you are tasked with distributing 6 markers among 44 labelled buckets. Each bucket can have more than one marker (we are sampling with replacement). Heres a little illustration I made: # Can return to this to attempt to animate it. 10 iterations of six draws each. # Tricky points: # What if more than one marker in a single bucket? Need to adjust y aes of geom_point # How best to reset to blank after each 6 draws? # Consider using 14 buckets instead of 44, a little easier to see what&#39;s happening and a simple change. set.seed(71) draws &lt;- sample.int(44, size = 6*10, replace = TRUE) # we create 10 samples of 6 draws each sample_num &lt;- rep(1:10, each= 6) # label the samples 1-6 data_count &lt;- tibble(sample_num = sample_num, draw_num = 1:60, draws = draws) # drawing the buckets plot_count_background &lt;- ggplot()+ geom_segment(aes(x=0.5:44.5,y=rep(0,45),xend=0.5:44.5,yend=rep(2,45)))+ geom_path(aes(x=c(0.5,44.5), y=c(0,0)))+ scale_x_continuous(limits = c(0,45), breaks = 2*0:22, name= &quot;Buckets&quot;)+ scale_y_continuous(limits = c(0,3), breaks = NULL, name=NULL)+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank()) # placing the markers in the buckets plot_count_background+ geom_point(data =data_count[1:6,],aes(x=draws,y=rep(0.2,times=6)), size=3, colour= tol_light[[4]]) How many ways can we arrange these 6 markers across the 44 buckets? We think about taking each of the markers and the bucket walls and placing them in a big bag. Then we draw each of the objects in turn and place them on the table from left to right. In doing this we build up a picture that is equivalent to the one above # I&#39;m attempting an animation here. It does not work yet. # Main issue is having the plot remember the previous states and add to them rather than writing over # This was easier in the bar chart example, could just increment the height up one # This time I&#39;m trying to e.g. add entirely new points set.seed(73) draw_num &lt;- 1:49 draws &lt;- sample(c(rep(0,6),rep(1,43)),size=49, replace=FALSE) # we draw 6 markers (label 0) and 43 walls (label 1) data_count &lt;- tibble(draw_num = draw_num, draws = draws, wall_count = cumsum(draws))%&gt;% rowwise()%&gt;% mutate(point_x = ifelse(draws==0,draw_num,-5), # shove the points I don&#39;t want off-screen (to -5). v inelegant. point_y = 0.2, segment_x = ifelse(draws==1,wall_count+0.5,-5), segment_y = 0, segment_xend = segment_x, segment_yend = 2) # geom_segment(aes(x=0.5:44.5,y=rep(0,45),xend=0.5:44.5,yend=rep(2,45)))+ # drawing the buckets pcb_2 &lt;- ggplot(data_count)+ geom_segment(data=tibble(x=c(0.5,44.5),y=rep(0,2),xend=c(0.5,44.5),yend=rep(2,2)), aes(x=x,y=y,xend=xend,yend=yend))+ geom_path(data= tibble(x=c(0.5,44.5), y=c(0,0)), aes(x=x,y=y))+ scale_x_continuous(limits = c(0,45), breaks = 2*0:22, name= &quot;Buckets&quot;)+ scale_y_continuous(limits = c(0,3), breaks = NULL, name=NULL)+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank()) # placing the markers in the buckets plot_draws &lt;- pcb_2 + geom_point(aes(x=point_x,y=point_y), size=3, colour= tol_light[[4]])+ geom_segment(aes(x=segment_x,y=segment_y,xend=segment_xend,yend=segment_yend))+ facet_wrap(~draw_num) anim_draws &lt;- plot_draws + facet_null()+ # but instead of facetting by that variable... transition_reveal(draw_num) # we include the variable in a gganimate function animate(anim_draws,nframes=350,fps=20, end_pause = 50) We dont need to consider the end walls, as they dont help define a bucket, so we have 45-2=43 walls and 6 markers. 49 objects, that can be arranged in 49! ways. However this over-counts. Say we have a marker in bucket 15. We dont care if this is marker 1, marker 4, or marker 6, since we dont care about ordering. We have 6 choices for the first marker, 5 for the second, and so on, and so we have over-counted by a factor of 6!. Our count is now \\(\\frac{49!}{6!}\\). The same argument applies to the walls - we dont care about order here either, and so we are still over-counting by a factor of 43!. Our final count of the number of arrangements is: \\[ \\frac{49!}{6!43!} = \\binom{n+r-1}{r} \\] Inequalities: Bonferronis and Booles Shortly after probability functions are introduced, the chapter derives the following statement from the axioms of probability: \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] Since probabilities are at most one (and so \\(P(A \\cup B) \\leq 1\\)), this gives us what the chapter calls a special case of Bonferronis inequality: \\[ P(A \\cap B) \\geq P(A) + P(B) - 1. \\] The probability of two events occurring is bounded below by the sum of the individual probabilities less one. The chapter also contains a proof of Booles inequality, which is closely related on Bonferronis: \\[ \\begin{aligned} P\\left( \\bigcup_{i=1}^{\\infty}A_i \\right) \\leq \\sum_{i=1}^{\\infty} P(A_i), &amp;&amp; \\text{(Boole&#39;s Inequality)} \\end{aligned} \\] The proof starts by defining disjoint sets \\(A^*_i\\) as follows: \\[ \\begin{aligned} A^*_1 = A_1, &amp;&amp; A^*_i = A_i - \\bigcup_{j=1}^{i-1}A_j, &amp;&amp; i=2,3, \\dots, \\end{aligned} \\] Since by this definition we have \\(\\cup_{i=1}^{\\infty}A_i= \\cup_{i=1}^{\\infty}A^*_i\\) and the \\(A^*_i\\)s are disjoint, we have: \\[ \\begin{aligned} P\\left( \\bigcup_{i=1}^{\\infty}A_i \\right) = P\\left( \\bigcup_{i=1}^{\\infty}A^*_i \\right) = \\sum_{i=1}^{\\infty} P(A^*_i) \\end{aligned} \\] but by the construction of the \\(A^*_i\\)s this last sum must be less than or equal to \\(\\sum_{i=1}^{\\infty} P(A_i)\\), which gives the inequality. Booles inequality is closely relating to Bonferronis in that we can derive Bonferronis from Booles using De Morgans laws: \\[ \\begin{aligned} P\\left( \\bigcup_{i=1}^{n}A_i^c \\right) &amp;\\leq \\sum_{i=1}^{n} P(A_i^c), &amp;&amp; \\text{(Boole&#39;s inequality applied to complements of } A_i) \\\\ \\implies1-P\\left( \\bigcap_{i=1}^{n}A_i \\right) &amp;\\leq n- \\sum_{i=1}^{n} P(A_i), &amp;&amp; \\text{(De Morgan&#39;s law and } P(A^c)=1-P(A)) \\\\ \\implies P\\left( \\bigcap_{i=1}^{n}A_i \\right) &amp;\\geq \\sum_{i=1}^{n} P(A_i) - (n-1), &amp;&amp; \\text{(Rearranging terms)} \\end{aligned} \\] which is a more general version of Bonferronis inequality introduced above. We bound below the probability of \\(n\\) events all occurring by the sum of the individual probabilities, less \\(n-1\\). By the definition of intersections, we can also bound the probability of the intersection above by the minimum probability of any of the individual events \\(A_i\\). \\[ \\min_{0\\leq i\\leq n} P(A_i) \\geq P\\left( \\bigcap_{i=1}^{n}A_i \\right) \\geq \\sum_{i=1}^{n} P(A_i) - (n-1) \\] In the chapter appendix, there is a section on an extension of Booles inequality for the probability of a union which makes the bounds more precise. We start by defining some sums of probabilities of nested intersections: \\[ \\begin{aligned} P_1 &amp;= \\sum_{i=1}^n P(A_i) \\\\ P_2 &amp;= \\sum_{1\\leq i&lt;j \\leq n}^n P(A_i \\cap A_j) \\\\ P_3 &amp;= \\sum_{1\\leq i&lt;j&lt;k \\leq n}^n P(A_i \\cap A_j \\cap A_k) \\\\ \\vdots\\\\ P_n &amp;= P(A_1 \\cap A_2 \\cap \\dots \\cap A_n) \\\\ \\end{aligned} \\] Then \\[ P(A_1 \\cup A_2 \\cup \\dots \\cup A_n) = P_1 -P_2 +P_3 -P_4 +\\dots \\pm P_n\\\\ \\] This uses the inclusion-exclusion principles, the extension of: \\[ P(A \\cup B) = P(A)+P(B)-P(A \\cap B) \\] to \\(n\\) sets. Lets assume for now that \\(P_i \\geq P_j\\) if \\(i \\leq j\\) (well prove this momentarily - its left as an exercise in the chapter). Then we have a series of increasingly tight bounds for the probability of the union: \\[ \\begin{aligned} P_1 &amp;\\geq P\\left(\\bigcup_{i=1}^n A_i \\right) \\geq P_1 - P_2 \\\\ P_1 - P_2 + P_3 &amp;\\geq P\\left(\\bigcup_{i=1}^n A_i \\right) \\geq P_1 - P_2 + P_3 -P_4 \\\\ &amp;\\vdots \\end{aligned} \\] Which is an extension of Booles inequality, which only expressed that the probability of the union was bounded by \\(P_1\\): \\[ \\begin{aligned} P_1=\\sum_{i=1}^{\\infty} P(A_i) \\geq P\\left( \\bigcup_{i=1}^{\\infty}A_i \\right) \\end{aligned} \\] Now we just need to prove that \\(P_i \\geq P_j\\) if \\(i \\leq j\\). This is question 1.43 (b) in the chapter, and is done in the question section. 1.2 Questions Ex 1.42 Question The inclusion-exclusion identity of Miscellanea 1.8.1 gets it name from the fact that it is proved by the method of inclusion and exclusion (Feller 1968, Section IV.1). Here we go into the details. The probability \\(P(\\cup_{i=1}^n A_i)\\) is the sum of the probabilities of all the sample points that are contained in at least one of the \\(A_i\\)s. The method of inclusion and exclusion is a recipe for counting these points. Let \\(E_k\\) denote the set of all sample points that are contained in exactly \\(k\\) of the events \\(A_1, \\dots, A_n\\). Show that \\(P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^n P(E_i)\\). If \\(E_1\\) is not empty, show that \\(P(E_1) = \\sum_{i=1}^n P(A_i)\\) Without loss of generality, assume that \\(E_k\\) is contained in \\(A_1, A_2, \\dots , A_k\\). Show that \\(P(E_k)\\) appears \\(k\\) times in the sum \\(P_1\\), \\(\\binom{k}{2}\\) times in the sum \\(P_2\\), \\(\\binom{k}{3}\\) times in the sum \\(P_3\\), etc. Show that \\[ k - \\binom{k}{2} + \\binom{k}{3} - \\dots \\pm \\binom{k}{k} =1 \\] (See Exercise 1.27.) Show that parts (a) - (c) imply \\(\\sum_{i=1}^n P(E_i) = P_1 -P_2 +P_3 - \\dots \\pm P_n\\), establishing the inclusion-exclusion identity. Answer Ex 1.43 Question For the inclusion-exclusion identity of Miscellanea 1.8.1: Derive both Booles and Bonferronis Inequality from the inclusion-exclusion identity. Show that the \\(P_i\\) satisfy \\(P_i \\geq P_j\\) if \\(i \\leq j\\) and that the sequence of bounds in Miscellanea 1.8.1 improves as the number of terms increases. Typically as the number of terms in the bound increases, the bound becomes more useful. However, Schwager (1984) cautions that there are some cases where there is not much improvement, in particular if the \\(A_i\\)s are highly correlated. Examine what happens to the sequence of bounds in the extreme case when \\(A_i = A\\) for every \\(i\\). (See Worsley 1982 and the correspondence of Worsley 1985 and Schwager 1985.) Answer Further Reading "],["transformations.html", "Chapter 2 Transformations and Expectations 2.1 Chapter Notes 2.2 Questions Further Reading", " Chapter 2 Transformations and Expectations 2.1 Chapter Notes Chapter 2 is all about the behaviour of functions of a random variable \\(X\\). We get a new random variable \\(Y = g(X)\\) and if \\(g\\) is nice enough we can find simple expressions for the cdf and pdf of \\(Y\\) in terms of \\(g\\) and the cdf or pdf of \\(X\\). Monotonic Transformations For example, if \\(g\\) is monotonic, we have the following theorem: Theorem 2.1.3 Let \\(X\\) have cdf \\(F_X(x)\\), let \\(Y=g(X)\\), and define the support sets of \\(X\\) and \\(Y\\) as follows: \\[ \\begin{aligned} \\mathcal{X} &amp;= \\{x: f_X(x) &gt;0\\}\\\\ \\mathcal{Y} &amp;= \\{y: y=g(x) \\text{ for some } x \\in X\\ \\} \\end{aligned} \\] Then: If \\(g\\) is an increasing function on \\(\\mathcal{X}\\), \\(F_Y(y) = F_X(g^{-1}(y))\\) for \\(y \\in \\mathcal{Y}\\) If \\(g\\) is a decreasing function on \\(\\mathcal{X}\\) and \\(X\\) is a continuous random variable, \\(F_Y(y) = 1 -F_X(g^{-1}(y))\\) for \\(y \\in \\mathcal{Y}\\). Because \\(g(x)\\) is monotone it is both one-to-one and onto from \\(\\mathcal{X} \\to \\mathcal{Y}\\). So \\(g^{-1}(x)\\) is well-defined. For example, here is the cdf of the uniform(0,1) distribution: ggplot()+ geom_function(fun=punif, args = list(min = 0, max = 1), colour=tol_light[[4]], lwd=1)+ xlim(0,1)+ ylab(&quot;Cumulative Probability&quot;) Its just \\(F_X(x) = x\\). And we want to use the transformation \\(Y = g(X) = -\\log X\\) which is decreasing over the support of \\(X\\), \\((0,1)\\): ggplot()+ geom_function(fun=function(x){-log(x)},colour=tol_light[[3]], lwd=1)+ xlim(0,1)+ ylab(&quot;- log X&quot;) As \\(X\\) ranges from 0 to 1, \\(Y\\) ranges from 0 to \\(\\infty\\). By Theorem 2.1.3, \\[ F_Y(y) = 1 -F_X(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y} \\] This is the cdf of the exponential distribution with rate parameter 1. Ive plotted the cdfs of the uniform and exponential below.Ive only plotted the range \\((0,4)\\) for the exponential. # not convinced of the utility of this animation - particularly the timing on each side # since the exponential should really be ranging from 0 - infinity. data_exp &lt;- tibble(count = 1:1001, uniform=seq(0,1,length.out=1001), exponential=seq(0,4,length.out=1001))%&gt;% pivot_longer(c(uniform,exponential),names_to=&quot;variable&quot;,values_to=&quot;seq&quot;)%&gt;% mutate(cdf=if_else(variable == &quot;uniform&quot;, seq,1-exp(-seq))) anim_exp &lt;- ggplot(data_exp, aes(seq,cdf,group=variable))+ geom_line()+ facet_grid(cols = vars(fct_rev(variable)), scales = &quot;free&quot;)+ xlab(&quot;Random Variable&quot;)+ ylab(&quot;Cumulative Probability&quot;)+ transition_reveal(count) # problem to fix: colour and line width specification doesn&#39;t work animate(anim_exp,end_pause = 50) We wont always be dealing with transformations that are monotonic over the entire support set, however we usually can partition \\(\\mathcal X\\) into intervals such that \\(g\\) is monotonic over each interval, and build the cdf of \\(Y\\) this way. Generating Random Samples The chapter introduces a method of generating random samples from a distribution. We will need the following theorem: Theorem 2.1.10 - Probability Integral Transformation Let \\(X\\) have continuous cdf \\(F_X(x)\\) and define the random variable \\(Y\\) as \\(Y=F_X(x).\\) Then \\(Y\\) is uniformly distributed on \\((0,1),\\) that is \\(P(Y \\leq y)=y\\) for \\(0&lt;y&lt;1\\). There is a bit of subtlety here around defining inverse cdfs in cases where the cdf is not strictly increasing, but stays level over some range. In these cases we define \\[ F_x^{-1}(y) = \\inf\\{x: F_X(X)=y\\} \\] Aside from this, it is fairly straightforward to show that \\(P(Y \\leq y)=y\\). Now, if we can generate random samples from uniform\\((0,1)\\), we can create observations from a population with cdf \\(F_X(x)\\) and solve \\(F_X(x)=u\\). Moment Generating Functions The chapter introduces expectations, and then moments. For each integer \\(n\\), the \\(n\\)th moment of \\(X\\), \\(\\mu&#39;_n\\), is \\[ \\mu&#39;_n = E(X^n) \\] The \\(n\\)th central moment of \\(X\\), \\(\\mu_n\\), is \\[ \\mu_n = E((X-\\mu)^n) \\] where \\(\\mu = \\mu&#39;_1=E(X)\\). The second central moment is the variance. The chapter introduces moment generating functions, saying that often it is easier to calculate the moments directly, but mgfs are still useful for characterising distributions (see Theorem 2.3.12). For a random variable \\(X\\) with cdf \\(F_X(x)\\), the mgf is \\[ M_X(t)=E(e^{tX}). \\] We generate moments from this functions as follows: Theorem 2.3.7 If \\(X\\) has mgf \\(M_X(t)\\) then \\[ E(X^n)=M_X^{(n)}(0)=\\frac{d^n}{dt^n}M_X(t)\\vert_{t=0} \\] Convergence (in a neighbourhood of 0) of mgfs to an mgf implies convergence of cdfs: Theorem 2.3.12 - Convergence of mgfs Suppose \\(\\{X_i, i=1,2,\\dots\\}\\) is a sequence of random variables, each with mgf \\(M_{X_i}(t).\\) And suppose that \\[ \\begin{aligned} \\lim_{i\\to\\infty} M_{X_i}(t) = M_X(t),&amp;&amp; \\text{for all } t \\text{ in a neighbourhood of 0} \\end{aligned} \\] and \\(M_X(t)\\) is an mgf. There there exists a unique cdf \\(F_X(x)\\) whose moments are determined by \\(M_X(t)\\) and, for all \\(x\\) where \\(F_X(x)\\) is continuous, we have \\[ \\lim_{i\\to\\infty} F_{X_i}(x) = F_X(x). \\] As a example, the chapter goes on to show that mgfs of the binomial\\((n,p)\\) distribution converge to the mgf of the Poisson(\\(\\lambda\\)) distribution. When we consider \\(p=\\lambda/n\\) and let \\(n\\to \\infty\\). This is a common approximation, usually advised when \\(n\\) is large and \\(np\\) is small, and Theorem 2.3.12 explains its use. Heres an example with \\(n=15\\) and \\(p=0.3\\): n=15 p=0.3 data_pois &lt;- tibble(x=0:n, Binomial=dbinom(x,n,p), Poisson=dpois(x,lambda=n*p))%&gt;% pivot_longer(c(Binomial,Poisson),names_to = &quot;Distribution&quot;,values_to = &quot;pmf&quot;) ggplot(data_pois, aes(x=x,y=pmf,group=Distribution))+ geom_col(aes(fill=Distribution),position = &quot;dodge&quot;)+ ylab(&quot;Probability Mass&quot;)+ scale_x_continuous(breaks=0:15) This might be interesting as a Shiny app. To show convergence to the mgf of the Poisson, we need the following lemma. Lemma 2.3.14 Let \\(a_1, a_2, \\dots\\) be a sequence of numbers converging to \\(a\\), that is \\(\\lim_{n \\to\\infty} =a\\). Then \\[ \\lim_{n \\to\\infty} \\left( 1 + \\frac{a_n}{n} \\right)^n = e^a \\] In the binomial/Poisson example, we have: \\[ \\begin{aligned} M_X(t) &amp;= [pe^t + (1-p)]^n &amp;&amp;\\text{mgf for distribution Binomial}(n,p) \\\\ &amp;=[1+ \\frac{1}{n}(e^t -1)(np)]^n &amp;&amp; \\text{rearranging}\\\\ &amp;= [1+ \\frac{1}{n}(e^t -1)\\lambda]^n. \\end{aligned} \\] Now we set \\(a_n = a = (e^t-1)\\lambda\\) and use the Lemma to get \\[ \\begin{aligned} \\lim_{n\\to \\infty} M_X(t) &amp;= e^{\\lambda(e^t-1)}\\\\ &amp;=M_Y(t) &amp;&amp;\\text{i.e. the mgf of distribution Poisson}(\\lambda) \\end{aligned} \\] Differentiating Under an Integral Sign The chapter takes a brief dip into calculus, noting that the scenario where we want to reverse the order of integration and differentiation comes up fairly often in theoretical statistics. Leibnizs Rule (an application of the Fundamental Theorem of Calculus and the chain rule) is introduced; Theorem 2.4.1 - Leibnizs Rule If \\(f(x,\\theta)\\), \\(a(\\theta)\\), and \\(b(\\theta)\\) are differentiable with respect to \\(\\theta\\) then. \\[ \\frac{d}{d\\theta}\\int^{b(\\theta)}_{a(\\theta)} f(x,\\theta) dx = f(b(\\theta),\\theta)\\frac{d}{d\\theta}b(\\theta) - f(a(\\theta),\\theta)\\frac{d}{d\\theta}a(\\theta) + \\int^{b(\\theta)}_{a(\\theta)} \\frac{\\partial}{\\partial\\theta} f(x,\\theta) dx \\] If \\(a\\) and \\(b\\) are constants the first two terms of the RHS disappear. So we can differentiate the integral with no problem if we have the integral of a differentiable function over a finite range. Problems can arise with infinite range. The chapter points out that the question of reversing the order of differentiation and integration is a question about reversing the order of limits and integration, since a derivative is a limit. Some important results presented in the chapter follow from Lesbesgues Dominated Convergence Theorem: Theorem 2.4.2 - Lesbesgues Dominated Convergence Theorem Suppose function \\(h(x,y)\\) is continuous at \\(y_0\\) for each \\(x\\), and there exists a function \\(g(x)\\) satisfying \\(|h(x,y)|\\leq g(x)\\) for all \\(x\\) and \\(y\\), \\(\\int^\\infty_{-\\infty} g(x) &lt; \\infty\\) Then \\[ \\lim_{y\\to y_0}\\int^\\infty_{-\\infty} h(x,y)dx = \\int^\\infty_{-\\infty} \\lim_{y\\to y_0} h(x,y) dx \\] Bounding \\(h\\) by a function \\(g\\) with a finite limit ensures that the integral cant behave too badly. 2.2 Questions Ex 2.1 Question In each of the following find the pdf of \\(Y\\). Show that the pdf integrates to 1. \\(Y = X^3\\) and \\(f_X(x) = 42x^5(1-x)\\), \\(0 &lt; x &lt; 1\\) \\(Y = 4X +3\\) and \\(f_X(x) = 7e^{-7x}\\), \\(0 &lt; x &lt; \\infty\\) \\(Y = X^2\\) and \\(f_X(x) = 30x^2(1-x)^2\\), \\(0 &lt; x &lt; 1\\) Answer Will need Theorem 2.1.5 on page 51/77. Our transformation \\(Y=X^3\\) is monotone on \\(0 &lt; x &lt; 1\\) and so by Theorem 2.1.5 our pdf is: \\[ \\begin{aligned} f_Y(y) &amp;= f_X(y^{1/3}) |\\frac{d}{dy}y^{1/3}| \\\\ &amp; = 42(y^{1/3})^5(1-y^{1/3})\\frac{1}{3}y^{\\left(-2/3\\right)}\\\\ &amp;= \\end{aligned} \\] Ex 2.3 Question Suppose \\(X\\) has the geometric pmf \\(f_X(x) = \\frac{1}{3}(\\frac{2}{3})^x\\), \\(x = 0, 1, 2, \\dots\\). Determine the probability distribution of \\(Y = X/(X + 1)\\). Note that here both \\(X\\) and \\(Y\\) are discrete random variables. To specify the probability distribution of \\(Y\\), specify its pmf. Answer Ex 2.6 Question In each of the following find the pdf of \\(Y\\) and show that the pdf integrates to 1. \\(f_X(x) = \\frac{1}{2}e^{-|x|}\\), \\(-\\infty &lt; x &lt; \\infty\\); \\(Y = |X|^3\\) \\(f_X(x) = \\frac{3}{8} (x+1)^2\\), \\(-1 &lt; x &lt; 1\\); \\(Y = 1-X^2\\) \\(f_X(x) = \\frac{1}{2} (x+1)^2\\), \\(-1 &lt; x &lt; 1\\); \\(Y = 1-X^2\\) if \\(X\\leq 0\\) and \\(Y=1-X\\) if \\(X&gt;0\\) Answer Further Reading "],["families.html", "Chapter 3 Common Families of Distributions 3.1 Chapter Notes 3.2 Questions Further Reading", " Chapter 3 Common Families of Distributions 3.1 Chapter Notes This chapter introduces a lot of common distributions. Im going to try to use R Shiny and animation to illustrate some of them. Discrete Distributions Hypergeometric We have an urn with \\(N\\) balls, \\(M\\) of which are red and \\(N-M\\) green. We draw \\(K\\) of the balls without replacement. What is the probability that exactly \\(X\\) of them are red? \\[ \\begin{align} P(X= x | N, M, K) = \\frac{\\binom{M}{x} \\binom{N-M}{K-x}}{\\binom{N}{K}}, &amp;&amp; x = 0, 1, \\dots, K \\end{align} \\] Where does this formula come from? The numerator counts the number of ways to choose \\(K\\) balls such that \\(x\\) are red. The denominator counts the total number of ways to choose \\(K\\) red balls. There are \\(\\binom{M}{x}\\) ways to choose \\(x\\) red balls from \\(M\\) total red ones. For each of these ways, there are \\(\\binom{N-M}{K-x}\\) ways to choose the other \\(K-x\\) balls so that they are all green. Multiplying these terms together gives the total number of ways to choose exactly \\(x\\) red balls from the urn in \\(K\\) draws. We then divide this number by the total number of ways to draw \\(K\\) balls from the urn, \\(\\binom{N}{K}\\). Heres an example. We set up an urn with 100 coloured balls, 70 of them red. We draw 20 of these balls without replacement and mark down how many of them are red. We then put those balls back in the urn and shake it up. We repeat this process a total of 150 times and animate the counts: N &lt;- 100 # 100 total balls M &lt;- 70 # 70 red K &lt;- 20 # We draw 20 balls to count how many are red reps &lt;- 150 # We repeat this a total of 150 times # A vector of the 150 results, each a number between 0 and 20 representing the number of red balls drawn. set.seed(71) sim_hyp &lt;- rhyper(reps, M, N-M, K) # Animation requires we have a data frame that contains the distribution of results at each draw 1-150 # So the data frame below is a little convoluted results_hyp &lt;- tibble( pos_num_red = rep(0:K,reps), draw_num = rep(1:reps,each=K+1))%&gt;% rowwise()%&gt;% mutate(count_obs = sum(sim_hyp[1:draw_num]==pos_num_red)) plot_hyp &lt;- ggplot(results_hyp)+ geom_col(aes(x = pos_num_red, y=count_obs), fill = tol_light[[4]])+ xlab(&quot;Number of Red Balls in the Draw&quot;)+ ylab(&quot;Counts of Results&quot;)+ facet_wrap(~draw_num) # the code required to animate is the same as if we wanted to facet by some variable anim_hyp &lt;- plot_hyp + facet_null()+ # but instead of facetting by that variable... geom_text(x = 4 , y = 28, aes(label = paste0(&quot;Draws Performed: &quot;,as.character(draw_num))), size = 5)+ transition_states(draw_num, # we include the variable in a gganimate function transition_length = 3, state_length = 1) animate(anim_hyp,nframes=350,fps=20, end_pause = 50) As we might expected, the most common results we see are those close to \\(70/100 \\times 20 = 14\\) red balls, reflecting the proportion we know are in the urn. 3.2 Questions Further Reading The chapter points to the four volumes of Johnson, N. L., and Kotz, S. (1969-1972). Distributions in Statistics as a more comprehensive catalogue of statistical distributions, as well as the updated volumes Johnson, N. L., Kotz, S., and Balakrishnan, N. (1994). Continuous Univariate Distributions, Volume 1 Johnson, N. L., Kotz, S., and Balakrishnan, N. (1995). Continuous Univariate Distributions, Volume 2 Johnson, N. L., Kotz, S., and Kemp, A. W. (1992). Univariate Discrete Distributions "],["multiple_variables.html", "Chapter 4 Multiples Random Variables 4.1 Chapter Notes 4.2 Questions Further Reading", " Chapter 4 Multiples Random Variables 4.1 Chapter Notes This chapter describes probability concepts when we have more than one random variable. Introduced are: N-dimensional random vectors Joint probability mass functions Expectations with joint pmfs Marginal pmfs Joint probability density functions Expectations with joint pdfs Marginal pdfs - with some useful tips for performing the integrations required Joint cdfs Conditional pmfs and pdfs Independence We can: Check whether two random variables \\(X\\) and \\(Y\\) are independent given a joint pmf/pdf, or Define a model in which \\(X\\) and \\(Y\\) are independent using the definition below. Definition 4.2.5 - Independence Let \\((X,Y)\\) be a bivariate random vector with joint pmf/pdf \\(f(x,y)\\) and marginal pmfs/pdfs \\(f_X(x)\\) and \\(f_Y(y)\\). We say \\(X\\) and \\(Y\\) are independent random variables if for every \\(x \\in \\mathcal{R}\\), \\(y \\in \\mathcal{R}\\) \\[ f(x,y)=f_X(x)f_Y(y). \\] Heres a Lemma that makes checking independence easier: Lemma 4.2.7 Let \\((X,Y)\\) be a bivariate random vector with joint pmf/pdf \\(f(x,y)\\). Then \\(X\\) and \\(Y\\) are independent random variables if and only if there exist functions \\(g(x)\\) and \\(h(y)\\) such that for every \\(x \\in \\mathcal{R}\\), \\(y \\in \\mathcal{R}\\), \\[ f(x,y)=g(x)h(y). \\] The proof is surprisingly straightforward (p153.) The expectation of the product of functions of independent random variables is the product of the expectations: \\[ E(g(X)h(y))=(Eg(X))(Eh(y)) \\] Bivariate Transformations This section is about the distribution of functions of bivariate random vectors. As an example the chapter demonstrates that if \\(X \\sim \\text{Poisson}(\\theta)\\) and \\(Y \\sim \\text{Poisson}(\\lambda)\\), and \\(X\\) and \\(Y\\) are independent then \\(X +Y \\sim \\text{Poisson}(\\theta+\\lambda)\\). There are similar cases in the chapter for the product of beta variables, and the sum, differences, and ratio of normal variables. Hierarchical Models and Mixture Distributions The first example in this section is the Binomial-Poisson hierarchy. Say an insect lays a large number of eggs (we might model this with a Poisson distribution). Each of the eggs have, say, probability \\(p\\) of surviving (we use a binomial). How many surviving eggs are there? If we use \\(X\\) to mean the number of surviving eggs and \\(Y\\) to mean the number of eggs laid we can write down a hierarchical model: \\[ \\begin{aligned} X|Y &amp;\\sim \\text{binomial}(Y,p) \\\\ Y &amp;\\sim \\text{Poisson}(\\lambda) \\end{aligned} \\] Here we model \\(Y\\) with a Poisson distribution and then use \\(Y\\) inside on a binomial distribution. When our distribution depends on a quantity that also has a distribution we call it a mixture distribution. The section contains a couple of useful theorems for conditional expectations and variances. Theorem 4.4.3 - Conditional Expectation If \\(X\\) and \\(Y\\) are any two random variables, then \\[ EX = E(E(X|Y)) \\] provided that the expectations exist. \\(E\\) here stands for a couple different things. There shouldnt be confusion but if we want to nitpick we should write: \\[ E_XX = E_Y(E_{X|Y}(X|Y)). \\] The proof starts by writing down the definition of \\(EX\\) and then using the (rearranged) definition of conditional probability \\(f(x,y)=f(x|y)f(y)\\): \\[ \\begin{aligned} E_XX = \\int\\int x f(x,y) dxdy &amp;= \\int \\left[ \\int xf(x|y)dx\\right]f_Y(y)dy \\\\ &amp;= \\int \\left[E_{X|Y}(X|y)\\right]f_Y(y)dy\\\\ &amp;= E_Y(E_{X|Y}(X|Y)). \\end{aligned} \\] Applying this to the insect example, our expected number of survivors becomes: \\[ \\begin{aligned} EX &amp;= E(E(X|Y))\\\\ &amp;= E(pY) &amp;&amp; (\\text{since } X|Y \\sim \\text{binomial}(Y,p))\\\\ &amp;= p \\lambda &amp;&amp; (\\text{since } Y \\sim \\text{Poisson}(\\lambda)). \\end{aligned} \\] What about conditional variances? Theorem 4.4.7 - Conditional Variance If \\(X\\) and \\(Y\\) are any two random variables, then \\[ \\text{Var}(X) = E( \\text{Var}(X|Y)) + \\text{Var}(E(X|Y)), \\] provided the expectations exist. Covariance and Correlation This section introduces measures of the strength of the relationship between two random variables. We have the covariance: \\[ \\begin{aligned} \\text{Cov}(X,Y) &amp;= E((X-\\mu_x)(Y-\\mu_y)) \\\\ &amp;= EXY-\\mu_x\\mu_Y &amp;&amp; \\text{this is Theorem 4.5.3 in the chapter} \\end{aligned} \\] and the correlation: \\[ \\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_x \\sigma_Y} \\] Independence implies covariance and correlation of 0, but the reverse is not true (Theorem 4.5.5). Covariance is important for measuring variation in the sums of random variables: Theorem 4.5.6 If \\(X\\) and \\(Y\\) are any two random variables and \\(a\\) and \\(b\\) are any two constants then \\[ \\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X,Y) \\] Multivariate Distribution The chapter extends discussion to multivariate random vectors \\((X_1, \\dots\\ X_n)\\). One example is the multinomial - generalising the binomial into situations with \\(n\\) (instead of 2) outcomes. Definition 4.6.2 - The Multinomial Distribution Let \\(n\\) and \\(m\\) be positive integers and let \\(p_1, \\dots, p_n\\) be numbers satisfying \\(0\\leq p_i \\leq 1\\) and \\(\\sum_{i=1}^n p_i = 1\\). Then the random vector \\((X_1, \\dots, X_n)\\) has a multinomial distribution with \\(m\\) trials and cell probabilities \\(p_1, \\dots, p_n\\) if the joint pmf of \\((X_1, \\dots, X_n)\\) is: \\[ f(x_1, \\dots, x_n) = \\frac{m!}{x_1!\\dots x_n!}p_1^{x_1} \\dots p_n^{x_n} = m! \\prod_{i=1}^n \\frac{p_i^{x_i}}{x_i!} \\] on the set of \\((x_1, \\dots, x_n)\\) such that each \\(x_i\\) is a nonnegative integer and they sum to \\(m\\). The factor \\(\\frac{m!}{x_1!\\dots x_n!}\\) is the multinomial coefficient. The number of ways that M objects can be divided into \\(n\\) groups with \\(x_i\\) in the \\(i\\)th group. We also have the multinomial theorem as an extension of the binomial theorem: Theorem 4.6.4 - The Multinomial Theorem Let \\(n\\) and \\(m\\) be positive integers and let \\(\\mathcal A\\) be the set of vectors \\(\\mathbf{x} = (x_1, \\dots, x_n)\\) such that each \\(x_i\\) is a nonnegative integer and they sum to \\(m\\). Then, for any real numbers \\(p_1, \\dots, p_n\\), \\[ (p_1, \\dots, p_n)^m = \\sum_{\\mathbf{x} \\in \\mathcal A}\\frac{m!}{x_1!\\dots x_n!}p_1^{x_1} \\dots p_n^{x_n} \\] Marginalising the multinomial distribution returns the binomial, as youd expect. The distribution of \\((X_1, \\dots X_{n-1})\\) given \\(X_n = x_n\\) is multinomial. Exercise 4.39 (which I hope to do below) shows that all the pairwise covariances of our \\(X_i\\)s are negative and are given by: \\[ \\text{Cov}(X_i,X_j) = E[(X_i -p_i)(X_j - p_j)] = -mp_ip_j \\] The negative correlation is greater for variables with higher success probabilities. Inequalities The chapter closes with a section introducing a number of hugely important inequalities: Holders Inequality Cauchy-Schwarz Inequality Covariance Inequality Liapounovs Inequality Minkowskis Inequality Jensens Inequality 4.2 Questions Ex 4.39 Question Let (\\(X_1, \\dots , X_n\\)) have a multinomial distribution with \\(m\\) trials and cell probabilities \\(p_1, \\dots ,p_n\\) (see Definition 4.6.2). Show that, for every \\(i\\) and \\(j\\), \\[ \\begin{aligned} X_i | X_j = x_j &amp;\\sim \\text{binomial} \\left( m-x_j, \\frac{p_i}{1-p_j} \\right) \\\\ X_j &amp;\\sim \\text{binomial}\\ (m, p_j) \\end{aligned} \\] and that \\(\\text{Cov}(X_i, X_j) = -mp_ip_j\\). Answer Further Reading "],["sample_properties.html", "Chapter 5 Properties of a Random Sample 5.1 Chapter Notes 5.2 Questions Further Reading", " Chapter 5 Properties of a Random Sample 5.1 Chapter Notes random sample - iid, joint pdf, sampling without replacement from finite nearly like def 5.1.1 statistic def 5.2.1 theorem 5.2.6 - unbiased section 5.3 5.3.1 section 5.3.2 - t and f - all of this section up to theorem 5.3.8 skip 5.4 5.5 conver in prob 5.5.2 5.5.2 almost sure conv strong law conv in distribution 5.5.14 - central limit stronger fotrm - 5.5.15 example 5.5.16 taylor series into delta method 5.2 Questions Further Reading "],["data_reduction.html", "Chapter 6 Principles of Data Reduction 6.1 Chapter Notes 6.2 Questions Further Reading", " Chapter 6 Principles of Data Reduction 6.1 Chapter Notes 6.2 Questions Further Reading "],["point_estimation.html", "Chapter 7 Point Estimation 7.1 Chapter Notes 7.2 Questions Further Reading", " Chapter 7 Point Estimation 7.1 Chapter Notes 7.2 Questions Further Reading "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["index.html", "Notes on Statistical Inference by Casella &amp; Berger Preface", " Notes on Statistical Inference by Casella &amp; Berger Jake Lawler 2022-01-27 Preface Notes on second edition. "],["probability_theory.html", "Chapter 1 Probability Theory 1.1 Chapter Notes 1.2 Questions Further Reading", " Chapter 1 Probability Theory 1.1 Chapter Notes Deeper dive into unordered with replacement counting. 1.2 Questions Further Reading "],["transformations.html", "Chapter 2 Transformations and Expectations 2.1 Chapter Notes 2.2 Questions Further Reading", " Chapter 2 Transformations and Expectations 2.1 Chapter Notes 2.2 Questions Ex 2.1 Question In each of the following find the pdf of \\(Y\\). Show that the pdf integrates to 1. \\(Y = X^3\\) and \\(f_X(x) = 42x^5(1-x)\\), \\(0 &lt; x &lt; 1\\) \\(Y = 4X +3\\) and \\(f_X(x) = 7e^{-7x}\\), \\(0 &lt; x &lt; \\infty\\) \\(Y = X^2\\) and \\(f_X(x) = 30x^2(1-x)^2\\), \\(0 &lt; x &lt; 1\\) Answer Will need Theorem 2.1.5 on page 51/77. Our transformation \\(Y=X^3\\) is monotone on \\(0 &lt; x &lt; 1\\) and so by Theorem 2.1.5 our pdf is: \\[ \\begin{aligned} f_Y(y) &amp;= f_X(y^{1/3}) |\\frac{d}{dy}y^{1/3}| \\\\ &amp; = 42(y^{1/3})^5(1-y^{1/3})\\frac{1}{3}y^{\\left(-2/3\\right)}\\\\ &amp;= \\end{aligned} \\] Ex 2.3 Question Suppose \\(X\\) has the geometric pmf \\(f_X(x) = \\frac{1}{3}(\\frac{2}{3})^x\\), \\(x = 0, 1, 2, \\dots\\). Determine the probability distribution of \\(Y = X/(X + 1)\\). Note that here both \\(X\\) and \\(Y\\) are discrete random variables. To specify the probability distribution of \\(Y\\), specify its pmf. Answer Ex 2.6 Question In each of the following find the pdf of \\(Y\\) and show that the pdf integrates to 1. \\(f_X(x) = \\frac{1}{2}e^{-|x|}\\), \\(-\\infty &lt; x &lt; \\infty\\); \\(Y = |X|^3\\) \\(f_X(x) = \\frac{3}{8} (x+1)^2\\), \\(-1 &lt; x &lt; 1\\); \\(Y = 1-X^2\\) \\(f_X(x) = \\frac{1}{2} (x+1)^2\\), \\(-1 &lt; x &lt; 1\\); \\(Y = 1-X^2\\) if \\(X\\leq 0\\) and \\(Y=1-X\\) if \\(X&gt;0\\) Answer Further Reading "],["families.html", "Chapter 3 Common Families of Distributions 3.1 Chapter Notes 3.2 Questions Further Reading", " Chapter 3 Common Families of Distributions 3.1 Chapter Notes This chapter introduces a lot of common distributions. Im going to try to use R Shiny and animation to illustrate some of them. Discrete Distributions Hypergeometric We have an urn with \\(N\\) balls, \\(M\\) of which are red and \\(N-M\\) green. We draw \\(K\\) of the balls without replacement. What is the probability that exactly \\(X\\) of them are red? \\[ \\begin{align} P(X= x | N, M, K) = \\frac{\\binom{M}{x} \\binom{N-M}{K-x}}{\\binom{N}{K}}, &amp;&amp; x = 0, 1, \\dots, K \\end{align} \\] Where does this formula come from? The numerator counts the number of ways to choose \\(K\\) balls such that \\(x\\) are red. The denominator counts the total number of ways to choose \\(K\\) red balls. There are \\(\\binom{M}{x}\\) ways to choose \\(x\\) red balls from \\(M\\) total red ones. For each of these ways, there are \\(\\binom{N-M}{K-x}\\) ways to choose the other \\(K-x\\) balls so that they are all green. Multiplying these terms together gives the total number of ways to choose exactly \\(x\\) red balls from the urn in \\(K\\) draws. We then divide this number by the total number of ways to draw \\(K\\) balls from the urn, \\(\\binom{N}{K}\\). Heres an example. We set up an urn with 100 coloured balls, 70 of them red. We draw 20 of these balls without replacement and mark down how many of them are red. We then put those balls back in the urn and shake it up. We repeat this process a total of 150 times and animate the counts: N &lt;- 100 # 100 total balls M &lt;- 70 # 70 red K &lt;- 20 # We draw 20 balls to count how many are red reps &lt;- 150 # We repeat this a total of 150 times # A vector of the 150 results, each a number between 0 and 20 representing the number of red balls drawn. set.seed(71) sim_hyp &lt;- rhyper(reps, M, N-M, K) # Animation requires we have a data frame that contains the distribution of results at each draw 1-150 # So the data frame below is a little convoluted results_hyp &lt;- tibble( pos_num_red = rep(0:K,reps), draw_num = rep(1:reps,each=K+1))%&gt;% rowwise()%&gt;% mutate(count_obs = sum(sim_hyp[1:draw_num]==pos_num_red)) plot_hyp &lt;- ggplot(results_hyp)+ geom_col(aes(x = pos_num_red, y=count_obs), fill = tol_light[[4]])+ xlab(&quot;Number of Red Balls in the Draw&quot;)+ ylab(&quot;Counts of Results&quot;)+ facet_wrap(~draw_num) # the code required to animate is the same as if we wanted to facet by some variable anim_hyp &lt;- plot_hyp + facet_null()+ # but instead of facetting by that variable... geom_text(x = 4 , y = 28, aes(label = paste0(&quot;Draws Performed: &quot;,as.character(draw_num))), size = 5)+ transition_states(draw_num, # we include the variable in a gganimate function transition_length = 3, state_length = 1) animate(anim_hyp,nframes=350,fps=20, end_pause = 50) As we might expected, the most common results we see are those close to \\(70/100 \\times 20 = 14\\) red balls, reflecting the proportion we know are in the urn. 3.2 Questions Further Reading The chapter points to the four volumes of Johnson, N. L., and Kotz, S. (1969-1972). Distributions in Statistics as a more comprehensive catalogue of statistical distributions, as well as the updated volumes Johnson, N. L., Kotz, S., and Balakrishnan, N. (1994). Continuous Univariate Distributions, Volume 1 Johnson, N. L., Kotz, S., and Balakrishnan, N. (1995). Continuous Univariate Distributions, Volume 2 Johnson, N. L., Kotz, S., and Kemp, A. W. (1992). Univariate Discrete Distributions "],["multiple_variables.html", "Chapter 4 Multiples Random Variables 4.1 Chapter Notes 4.2 Questions Further Reading", " Chapter 4 Multiples Random Variables 4.1 Chapter Notes 4.2 Questions Ex 4.39 Question Let (\\(X_1, \\dots , X_n\\)) have a multinomial distribution with \\(m\\) trials and cell probabilities \\(p_1, \\dots ,p_n\\) (see Definition 4.6.2). Show that, for every \\(i\\) and \\(j\\), \\[ \\begin{aligned} X_i | X_j = x_j &amp;\\sim \\text{binomial} \\left( m-x_j, \\frac{p_i}{1-p_j} \\right) \\\\ X_j &amp;\\sim \\text{binomial}\\ (m, p_j) \\end{aligned} \\] and that \\(\\text{Cov}(X_i, X_j) = -mp_ip_j\\). Answer Further Reading "],["sample_properties.html", "Chapter 5 Properties of a Random Sample 5.1 Chapter Notes 5.2 Questions Further Reading", " Chapter 5 Properties of a Random Sample 5.1 Chapter Notes 5.2 Questions Further Reading "],["data_reduction.html", "Chapter 6 Principles of Data Reduction 6.1 Chapter Notes 6.2 Questions Further Reading", " Chapter 6 Principles of Data Reduction 6.1 Chapter Notes 6.2 Questions Further Reading "],["point_estimation.html", "Chapter 7 Point Estimation 7.1 Chapter Notes 7.2 Questions Further Reading", " Chapter 7 Point Estimation 7.1 Chapter Notes 7.2 Questions Further Reading "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

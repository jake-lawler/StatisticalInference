[["index.html", "Notes on Statistical Inference by Casella &amp; Berger Preface", " Notes on Statistical Inference by Casella &amp; Berger Jake Lawler 2022-03-16 Preface These are my notes on the second edition of George Casella and Roger Bergers Statistical Inference. They are incomplete, Im still working my way through the book. Im broadly following this handbook by Oliver Y. Chén which goes through each chapter and highlights the important concepts and theorems, as well as suggesting some questions to attempt. However most of my chapter notes so far include a little more detail than is suggested by the handbook, as well as some plots and animations to present some of the points visually. To revisit: Chapter 1 Animating the markers for unordered with replacement counts Question 1.4.3 (b) Chapter 2 Point 10 from the handbook on Lipschitz condition Questions from handbook Chapter 3 More animations / maybe a Shiny app Chapter 4 Expand the section on inequalities. Exercise 4.39 Chapter 5 Proof of WLLN Expand the section on the Delta method Indirect random variable generation section add some animations. e.g. example 5.5.16 (normal approx to neg binomial) or direct &amp; indirect random variable generation Chapter 7- 12 not yet complete. "],["probability_theory.html", "Chapter 1 Probability Theory 1.1 Chapter Notes 1.2 Questions Further Reading", " Chapter 1 Probability Theory 1.1 Chapter Notes This chapter contains fairly standard material on basic probability theory. I wont summarise it here, but will instead pick out a couple of points to explore further. Counting In the section on counting, we find this table: with \\[ \\binom{n}{r} = \\frac{n!}{r!(n-r)!}. \\] The formula for unordered, with replacement counting is the one I find difficult to recreate, so Ill go into more detail on it here. The chapter describes choosing, say, 6 balls from 44 numbered lottery balls. We want to count how many arrangements of these six balls there are, assuming we are choosing with replacement and we dont care about the order. The chapter encourages re-interpreting this problem as one where you are tasked with distributing 6 markers among 44 labelled buckets. Each bucket can have more than one marker (we are sampling with replacement). Heres a little illustration I made: # Can return to this to attempt to animate it. 10 iterations of six draws each. # Tricky points: # What if more than one marker in a single bucket? Need to adjust y aes of geom_point # How best to reset to blank after each 6 draws? # Consider using 14 buckets instead of 44, a little easier to see what&#39;s happening and a simple change. set.seed(71) draws &lt;- sample.int(44, size = 6*10, replace = TRUE) # we create 10 samples of 6 draws each sample_num &lt;- rep(1:10, each= 6) # label the samples 1-6 data_count &lt;- tibble(sample_num = sample_num, draw_num = 1:60, draws = draws) # drawing the buckets plot_count_background &lt;- ggplot()+ geom_segment(aes(x=0.5:44.5,y=rep(0,45),xend=0.5:44.5,yend=rep(2,45)))+ geom_path(aes(x=c(0.5,44.5), y=c(0,0)))+ scale_x_continuous(limits = c(0,45), breaks = 2*0:22, name= &quot;Buckets&quot;)+ scale_y_continuous(limits = c(0,3), breaks = NULL, name=NULL)+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank()) # placing the markers in the buckets plot_count_background+ geom_point(data =data_count[1:6,],aes(x=draws,y=rep(0.2,times=6)), size=3, colour= tol_light[[4]]) How many ways can we arrange these 6 markers across the 44 buckets? We think about taking each of the markers and the bucket walls and placing them in a big bag. Then we draw each of the objects in turn and place them on the table from left to right. In doing this we build up a picture that is equivalent to the one above # I&#39;m attempting an animation here. It does not work yet. # Main issue is having the plot remember the previous states and add to them rather than writing over # This was easier in the bar chart example, could just increment the height up one # This time I&#39;m trying to e.g. add entirely new points set.seed(73) draw_num &lt;- 1:49 draws &lt;- sample(c(rep(0,6),rep(1,43)),size=49, replace=FALSE) # we draw 6 markers (label 0) and 43 walls (label 1) data_count &lt;- tibble(draw_num = draw_num, draws = draws, wall_count = cumsum(draws))%&gt;% rowwise()%&gt;% mutate(point_x = ifelse(draws==0,draw_num,-5), # shove the points I don&#39;t want off-screen (to -5). v inelegant. point_y = 0.2, segment_x = ifelse(draws==1,wall_count+0.5,-5), segment_y = 0, segment_xend = segment_x, segment_yend = 2) # geom_segment(aes(x=0.5:44.5,y=rep(0,45),xend=0.5:44.5,yend=rep(2,45)))+ # drawing the buckets pcb_2 &lt;- ggplot(data_count)+ geom_segment(data=tibble(x=c(0.5,44.5),y=rep(0,2),xend=c(0.5,44.5),yend=rep(2,2)), aes(x=x,y=y,xend=xend,yend=yend))+ geom_path(data= tibble(x=c(0.5,44.5), y=c(0,0)), aes(x=x,y=y))+ scale_x_continuous(limits = c(0,45), breaks = 2*0:22, name= &quot;Buckets&quot;)+ scale_y_continuous(limits = c(0,3), breaks = NULL, name=NULL)+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank()) # placing the markers in the buckets plot_draws &lt;- pcb_2 + geom_point(aes(x=point_x,y=point_y), size=3, colour= tol_light[[4]])+ geom_segment(aes(x=segment_x,y=segment_y,xend=segment_xend,yend=segment_yend))+ facet_wrap(~draw_num) anim_draws &lt;- plot_draws + facet_null()+ # but instead of facetting by that variable... transition_reveal(draw_num) # we include the variable in a gganimate function animate(anim_draws,nframes=350,fps=20, end_pause = 50) We dont need to consider the end walls, as they dont help define a bucket, so we have 45-2=43 walls and 6 markers. 49 objects, that can be arranged in 49! ways. However this over-counts. Say we have a marker in bucket 15. We dont care if this is marker 1, marker 4, or marker 6, since we dont care about ordering. We have 6 choices for the first marker, 5 for the second, and so on, and so we have over-counted by a factor of 6!. Our count is now \\(\\frac{49!}{6!}\\). The same argument applies to the walls - we dont care about order here either, and so we are still over-counting by a factor of 43!. Our final count of the number of arrangements is: \\[ \\frac{49!}{6!43!} = \\binom{n+r-1}{r} \\] Inequalities: Bonferronis and Booles Shortly after probability functions are introduced, the chapter derives the following statement from the axioms of probability: \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\] Since probabilities are at most one (and so \\(P(A \\cup B) \\leq 1\\)), this gives us what the chapter calls a special case of Bonferronis inequality: \\[ P(A \\cap B) \\geq P(A) + P(B) - 1. \\] The probability of two events occurring is bounded below by the sum of the individual probabilities less one. The chapter also contains a proof of Booles inequality, which is closely related on Bonferronis: \\[ \\begin{aligned} P\\left( \\bigcup_{i=1}^{\\infty}A_i \\right) \\leq \\sum_{i=1}^{\\infty} P(A_i), &amp;&amp; \\text{(Boole&#39;s Inequality)} \\end{aligned} \\] The proof starts by defining disjoint sets \\(A^*_i\\) as follows: \\[ \\begin{aligned} A^*_1 = A_1, &amp;&amp; A^*_i = A_i - \\bigcup_{j=1}^{i-1}A_j, &amp;&amp; i=2,3, \\dots, \\end{aligned} \\] Since by this definition we have \\(\\cup_{i=1}^{\\infty}A_i= \\cup_{i=1}^{\\infty}A^*_i\\) and the \\(A^*_i\\)s are disjoint, we have: \\[ \\begin{aligned} P\\left( \\bigcup_{i=1}^{\\infty}A_i \\right) = P\\left( \\bigcup_{i=1}^{\\infty}A^*_i \\right) = \\sum_{i=1}^{\\infty} P(A^*_i) \\end{aligned} \\] but by the construction of the \\(A^*_i\\)s this last sum must be less than or equal to \\(\\sum_{i=1}^{\\infty} P(A_i)\\), which gives the inequality. Booles inequality is closely relating to Bonferronis in that we can derive Bonferronis from Booles using De Morgans laws: \\[ \\begin{aligned} P\\left( \\bigcup_{i=1}^{n}A_i^c \\right) &amp;\\leq \\sum_{i=1}^{n} P(A_i^c), &amp;&amp; \\text{(Boole&#39;s inequality applied to complements of } A_i) \\\\ \\implies1-P\\left( \\bigcap_{i=1}^{n}A_i \\right) &amp;\\leq n- \\sum_{i=1}^{n} P(A_i), &amp;&amp; \\text{(De Morgan&#39;s law and } P(A^c)=1-P(A)) \\\\ \\implies P\\left( \\bigcap_{i=1}^{n}A_i \\right) &amp;\\geq \\sum_{i=1}^{n} P(A_i) - (n-1), &amp;&amp; \\text{(Rearranging terms)} \\end{aligned} \\] which is a more general version of Bonferronis inequality introduced above. We bound below the probability of \\(n\\) events all occurring by the sum of the individual probabilities, less \\(n-1\\). By the definition of intersections, we can also bound the probability of the intersection above by the minimum probability of any of the individual events \\(A_i\\). \\[ \\min_{0\\leq i\\leq n} P(A_i) \\geq P\\left( \\bigcap_{i=1}^{n}A_i \\right) \\geq \\sum_{i=1}^{n} P(A_i) - (n-1) \\] In the chapter appendix, there is a section on an extension of Booles inequality for the probability of a union which makes the bounds more precise. We start by defining some sums of probabilities of nested intersections: \\[ \\begin{aligned} P_1 &amp;= \\sum_{i=1}^n P(A_i) \\\\ P_2 &amp;= \\sum_{1\\leq i&lt;j \\leq n}^n P(A_i \\cap A_j) \\\\ P_3 &amp;= \\sum_{1\\leq i&lt;j&lt;k \\leq n}^n P(A_i \\cap A_j \\cap A_k) \\\\ \\vdots\\\\ P_n &amp;= P(A_1 \\cap A_2 \\cap \\dots \\cap A_n) \\\\ \\end{aligned} \\] Then \\[ P(A_1 \\cup A_2 \\cup \\dots \\cup A_n) = P_1 -P_2 +P_3 -P_4 +\\dots \\pm P_n\\\\ \\] This uses the inclusion-exclusion principles, the extension of: \\[ P(A \\cup B) = P(A)+P(B)-P(A \\cap B) \\] to \\(n\\) sets. Lets assume for now that \\(P_i \\geq P_j\\) if \\(i \\leq j\\) (well prove this momentarily - its left as an exercise in the chapter). Then we have a series of increasingly tight bounds for the probability of the union: \\[ \\begin{aligned} P_1 &amp;\\geq P\\left(\\bigcup_{i=1}^n A_i \\right) \\geq P_1 - P_2 \\\\ P_1 - P_2 + P_3 &amp;\\geq P\\left(\\bigcup_{i=1}^n A_i \\right) \\geq P_1 - P_2 + P_3 -P_4 \\\\ &amp;\\vdots \\end{aligned} \\] Which is an extension of Booles inequality, which only expressed that the probability of the union was bounded by \\(P_1\\): \\[ \\begin{aligned} P_1=\\sum_{i=1}^{\\infty} P(A_i) \\geq P\\left( \\bigcup_{i=1}^{\\infty}A_i \\right) \\end{aligned} \\] Now we just need to prove that \\(P_i \\geq P_j\\) if \\(i \\leq j\\). This is question 1.43 (b) in the chapter, and is done in the question section. 1.2 Questions Ex 1.42 Question The inclusion-exclusion identity of Miscellanea 1.8.1 gets it name from the fact that it is proved by the method of inclusion and exclusion (Feller 1968, Section IV.1). Here we go into the details. The probability \\(P(\\cup_{i=1}^n A_i)\\) is the sum of the probabilities of all the sample points that are contained in at least one of the \\(A_i\\)s. The method of inclusion and exclusion is a recipe for counting these points. Let \\(E_k\\) denote the set of all sample points that are contained in exactly \\(k\\) of the events \\(A_1, \\dots, A_n\\). Show that \\(P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^n P(E_i)\\). If \\(E_1\\) is not empty, show that \\(P(E_1) = \\sum_{i=1}^n P(A_i)\\) Without loss of generality, assume that \\(E_k\\) is contained in \\(A_1, A_2, \\dots , A_k\\). Show that \\(P(E_k)\\) appears \\(k\\) times in the sum \\(P_1\\), \\(\\binom{k}{2}\\) times in the sum \\(P_2\\), \\(\\binom{k}{3}\\) times in the sum \\(P_3\\), etc. Show that \\[ k - \\binom{k}{2} + \\binom{k}{3} - \\dots \\pm \\binom{k}{k} =1 \\] (See Exercise 1.27.) Show that parts (a) - (c) imply \\(\\sum_{i=1}^n P(E_i) = P_1 -P_2 +P_3 - \\dots \\pm P_n\\), establishing the inclusion-exclusion identity. Answer Ex 1.43 Question For the inclusion-exclusion identity of Miscellanea 1.8.1: Derive both Booles and Bonferronis Inequality from the inclusion-exclusion identity. Show that the \\(P_i\\) satisfy \\(P_i \\geq P_j\\) if \\(i \\leq j\\) and that the sequence of bounds in Miscellanea 1.8.1 improves as the number of terms increases. Typically as the number of terms in the bound increases, the bound becomes more useful. However, Schwager (1984) cautions that there are some cases where there is not much improvement, in particular if the \\(A_i\\)s are highly correlated. Examine what happens to the sequence of bounds in the extreme case when \\(A_i = A\\) for every \\(i\\). (See Worsley 1982 and the correspondence of Worsley 1985 and Schwager 1985.) Answer Further Reading "],["transformations.html", "Chapter 2 Transformations and Expectations 2.1 Chapter Notes 2.2 Questions Further Reading", " Chapter 2 Transformations and Expectations 2.1 Chapter Notes Chapter 2 is all about the behaviour of functions of a random variable \\(X\\). We get a new random variable \\(Y = g(X)\\) and if \\(g\\) is nice enough we can find simple expressions for the cdf and pdf of \\(Y\\) in terms of \\(g\\) and the cdf or pdf of \\(X\\). Monotonic Transformations For example, if \\(g\\) is monotonic, we have the following theorem: Theorem 2.1.3 Let \\(X\\) have cdf \\(F_X(x)\\), let \\(Y=g(X)\\), and define the support sets of \\(X\\) and \\(Y\\) as follows: \\[ \\begin{aligned} \\mathcal{X} &amp;= \\{x: f_X(x) &gt;0\\}\\\\ \\mathcal{Y} &amp;= \\{y: y=g(x) \\text{ for some } x \\in X\\ \\} \\end{aligned} \\] Then: If \\(g\\) is an increasing function on \\(\\mathcal{X}\\), \\(F_Y(y) = F_X(g^{-1}(y))\\) for \\(y \\in \\mathcal{Y}\\) If \\(g\\) is a decreasing function on \\(\\mathcal{X}\\) and \\(X\\) is a continuous random variable, \\(F_Y(y) = 1 -F_X(g^{-1}(y))\\) for \\(y \\in \\mathcal{Y}\\). Because \\(g(x)\\) is monotone it is both one-to-one and onto from \\(\\mathcal{X} \\to \\mathcal{Y}\\). So \\(g^{-1}(x)\\) is well-defined. For example, here is the cdf of the uniform(0,1) distribution: ggplot()+ geom_function(fun=punif, args = list(min = 0, max = 1), colour=tol_light[[4]], lwd=1)+ xlim(0,1)+ ylab(&quot;Cumulative Probability&quot;) Its just \\(F_X(x) = x\\). And we want to use the transformation \\(Y = g(X) = -\\log X\\) which is decreasing over the support of \\(X\\), \\((0,1)\\): ggplot()+ geom_function(fun=function(x){-log(x)},colour=tol_light[[3]], lwd=1)+ xlim(0,1)+ ylab(&quot;- log X&quot;) As \\(X\\) ranges from 0 to 1, \\(Y\\) ranges from 0 to \\(\\infty\\). By Theorem 2.1.3, \\[ F_Y(y) = 1 -F_X(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y} \\] This is the cdf of the exponential distribution with rate parameter 1. Ive plotted the cdfs of the uniform and exponential below.Ive only plotted the range \\((0,4)\\) for the exponential. # not convinced of the utility of this animation - particularly the timing on each side # since the exponential should really be ranging from 0 - infinity. data_exp &lt;- tibble(count = 1:1001, uniform=seq(0,1,length.out=1001), exponential=seq(0,4,length.out=1001))%&gt;% pivot_longer(c(uniform,exponential),names_to=&quot;variable&quot;,values_to=&quot;seq&quot;)%&gt;% mutate(cdf=if_else(variable == &quot;uniform&quot;, seq,1-exp(-seq))) anim_exp &lt;- ggplot(data_exp, aes(seq,cdf,group=variable))+ geom_line()+ facet_grid(cols = vars(fct_rev(variable)), scales = &quot;free&quot;)+ xlab(&quot;Random Variable&quot;)+ ylab(&quot;Cumulative Probability&quot;)+ transition_reveal(count) # problem to fix: colour and line width specification doesn&#39;t work animate(anim_exp,end_pause = 50) We wont always be dealing with transformations that are monotonic over the entire support set, however we usually can partition \\(\\mathcal X\\) into intervals such that \\(g\\) is monotonic over each interval, and build the cdf of \\(Y\\) this way. Generating Random Samples The chapter introduces a method of generating random samples from a distribution. We will need the following theorem: Theorem 2.1.10 - Probability Integral Transformation Let \\(X\\) have continuous cdf \\(F_X(x)\\) and define the random variable \\(Y\\) as \\(Y=F_X(x).\\) Then \\(Y\\) is uniformly distributed on \\((0,1),\\) that is \\(P(Y \\leq y)=y\\) for \\(0&lt;y&lt;1\\). There is a bit of subtlety here around defining inverse cdfs in cases where the cdf is not strictly increasing, but stays level over some range. In these cases we define \\[ F_x^{-1}(y) = \\inf\\{x: F_X(X)=y\\} \\] Aside from this, it is fairly straightforward to show that \\(P(Y \\leq y)=y\\). Now, if we can generate random samples from uniform\\((0,1)\\), we can create observations from a population with cdf \\(F_X(x)\\) and solve \\(F_X(x)=u\\). Moment Generating Functions The chapter introduces expectations, and then moments. For each integer \\(n\\), the \\(n\\)th moment of \\(X\\), \\(\\mu&#39;_n\\), is \\[ \\mu&#39;_n = E(X^n) \\] The \\(n\\)th central moment of \\(X\\), \\(\\mu_n\\), is \\[ \\mu_n = E((X-\\mu)^n) \\] where \\(\\mu = \\mu&#39;_1=E(X)\\). The second central moment is the variance. The chapter introduces moment generating functions, saying that often it is easier to calculate the moments directly, but mgfs are still useful for characterising distributions (see Theorem 2.3.12). For a random variable \\(X\\) with cdf \\(F_X(x)\\), the mgf is \\[ M_X(t)=E(e^{tX}). \\] We generate moments from this functions as follows: Theorem 2.3.7 If \\(X\\) has mgf \\(M_X(t)\\) then \\[ E(X^n)=M_X^{(n)}(0)=\\frac{d^n}{dt^n}M_X(t)\\vert_{t=0} \\] Convergence (in a neighbourhood of 0) of mgfs to an mgf implies convergence of cdfs: Theorem 2.3.12 - Convergence of mgfs Suppose \\(\\{X_i, i=1,2,\\dots\\}\\) is a sequence of random variables, each with mgf \\(M_{X_i}(t).\\) And suppose that \\[ \\begin{aligned} \\lim_{i\\to\\infty} M_{X_i}(t) = M_X(t),&amp;&amp; \\text{for all } t \\text{ in a neighbourhood of 0} \\end{aligned} \\] and \\(M_X(t)\\) is an mgf. There there exists a unique cdf \\(F_X(x)\\) whose moments are determined by \\(M_X(t)\\) and, for all \\(x\\) where \\(F_X(x)\\) is continuous, we have \\[ \\lim_{i\\to\\infty} F_{X_i}(x) = F_X(x). \\] As a example, the chapter goes on to show that mgfs of the binomial\\((n,p)\\) distribution converge to the mgf of the Poisson(\\(\\lambda\\)) distribution. When we consider \\(p=\\lambda/n\\) and let \\(n\\to \\infty\\). This is a common approximation, usually advised when \\(n\\) is large and \\(np\\) is small, and Theorem 2.3.12 explains its use. Heres an example with \\(n=15\\) and \\(p=0.3\\): n=15 p=0.3 data_pois &lt;- tibble(x=0:n, Binomial=dbinom(x,n,p), Poisson=dpois(x,lambda=n*p))%&gt;% pivot_longer(c(Binomial,Poisson),names_to = &quot;Distribution&quot;,values_to = &quot;pmf&quot;) ggplot(data_pois, aes(x=x,y=pmf,group=Distribution))+ geom_col(aes(fill=Distribution),position = &quot;dodge&quot;)+ ylab(&quot;Probability Mass&quot;)+ scale_x_continuous(breaks=0:15) This might be interesting as a Shiny app. To show convergence to the mgf of the Poisson, we need the following lemma. Lemma 2.3.14 Let \\(a_1, a_2, \\dots\\) be a sequence of numbers converging to \\(a\\), that is \\(\\lim_{n \\to\\infty} =a\\). Then \\[ \\lim_{n \\to\\infty} \\left( 1 + \\frac{a_n}{n} \\right)^n = e^a \\] In the binomial/Poisson example, we have: \\[ \\begin{aligned} M_X(t) &amp;= [pe^t + (1-p)]^n &amp;&amp;\\text{mgf for distribution Binomial}(n,p) \\\\ &amp;=[1+ \\frac{1}{n}(e^t -1)(np)]^n &amp;&amp; \\text{rearranging}\\\\ &amp;= [1+ \\frac{1}{n}(e^t -1)\\lambda]^n. \\end{aligned} \\] Now we set \\(a_n = a = (e^t-1)\\lambda\\) and use the Lemma to get \\[ \\begin{aligned} \\lim_{n\\to \\infty} M_X(t) &amp;= e^{\\lambda(e^t-1)}\\\\ &amp;=M_Y(t) &amp;&amp;\\text{i.e. the mgf of distribution Poisson}(\\lambda) \\end{aligned} \\] Differentiating Under an Integral Sign The chapter takes a brief dip into calculus, noting that the scenario where we want to reverse the order of integration and differentiation comes up fairly often in theoretical statistics. Leibnizs Rule (an application of the Fundamental Theorem of Calculus and the chain rule) is introduced; Theorem 2.4.1 - Leibnizs Rule If \\(f(x,\\theta)\\), \\(a(\\theta)\\), and \\(b(\\theta)\\) are differentiable with respect to \\(\\theta\\) then. \\[ \\frac{d}{d\\theta}\\int^{b(\\theta)}_{a(\\theta)} f(x,\\theta) dx = f(b(\\theta),\\theta)\\frac{d}{d\\theta}b(\\theta) - f(a(\\theta),\\theta)\\frac{d}{d\\theta}a(\\theta) + \\int^{b(\\theta)}_{a(\\theta)} \\frac{\\partial}{\\partial\\theta} f(x,\\theta) dx \\] If \\(a\\) and \\(b\\) are constants the first two terms of the RHS disappear. So we can differentiate the integral with no problem if we have the integral of a differentiable function over a finite range. Problems can arise with infinite range. The chapter points out that the question of reversing the order of differentiation and integration is a question about reversing the order of limits and integration, since a derivative is a limit. Some important results presented in the chapter follow from Lesbesgues Dominated Convergence Theorem: Theorem 2.4.2 - Lesbesgues Dominated Convergence Theorem Suppose function \\(h(x,y)\\) is continuous at \\(y_0\\) for each \\(x\\), and there exists a function \\(g(x)\\) satisfying \\(|h(x,y)|\\leq g(x)\\) for all \\(x\\) and \\(y\\), \\(\\int^\\infty_{-\\infty} g(x) &lt; \\infty\\) Then \\[ \\lim_{y\\to y_0}\\int^\\infty_{-\\infty} h(x,y)dx = \\int^\\infty_{-\\infty} \\lim_{y\\to y_0} h(x,y) dx \\] Bounding \\(h\\) by a function \\(g\\) with a finite limit ensures that the integral cant behave too badly. 2.2 Questions Ex 2.1 Question In each of the following find the pdf of \\(Y\\). Show that the pdf integrates to 1. \\(Y = X^3\\) and \\(f_X(x) = 42x^5(1-x)\\), \\(0 &lt; x &lt; 1\\) \\(Y = 4X +3\\) and \\(f_X(x) = 7e^{-7x}\\), \\(0 &lt; x &lt; \\infty\\) \\(Y = X^2\\) and \\(f_X(x) = 30x^2(1-x)^2\\), \\(0 &lt; x &lt; 1\\) Answer Will need Theorem 2.1.5 on page 51/77. Our transformation \\(Y=X^3\\) is monotone on \\(0 &lt; x &lt; 1\\) and so by Theorem 2.1.5 our pdf is: \\[ \\begin{aligned} f_Y(y) &amp;= f_X(y^{1/3}) |\\frac{d}{dy}y^{1/3}| \\\\ &amp; = 42(y^{1/3})^5(1-y^{1/3})\\frac{1}{3}y^{\\left(-2/3\\right)}\\\\ &amp;= \\end{aligned} \\] Ex 2.3 Question Suppose \\(X\\) has the geometric pmf \\(f_X(x) = \\frac{1}{3}(\\frac{2}{3})^x\\), \\(x = 0, 1, 2, \\dots\\). Determine the probability distribution of \\(Y = X/(X + 1)\\). Note that here both \\(X\\) and \\(Y\\) are discrete random variables. To specify the probability distribution of \\(Y\\), specify its pmf. Answer Ex 2.6 Question In each of the following find the pdf of \\(Y\\) and show that the pdf integrates to 1. \\(f_X(x) = \\frac{1}{2}e^{-|x|}\\), \\(-\\infty &lt; x &lt; \\infty\\); \\(Y = |X|^3\\) \\(f_X(x) = \\frac{3}{8} (x+1)^2\\), \\(-1 &lt; x &lt; 1\\); \\(Y = 1-X^2\\) \\(f_X(x) = \\frac{1}{2} (x+1)^2\\), \\(-1 &lt; x &lt; 1\\); \\(Y = 1-X^2\\) if \\(X\\leq 0\\) and \\(Y=1-X\\) if \\(X&gt;0\\) Answer Further Reading "],["families.html", "Chapter 3 Common Families of Distributions 3.1 Chapter Notes 3.2 Questions Further Reading", " Chapter 3 Common Families of Distributions 3.1 Chapter Notes This chapter introduces a lot of common distributions. Im going to try to use R Shiny and animation to illustrate some of them. Discrete Distributions Hypergeometric We have an urn with \\(N\\) balls, \\(M\\) of which are red and \\(N-M\\) green. We draw \\(K\\) of the balls without replacement. What is the probability that exactly \\(X\\) of them are red? \\[ \\begin{align} P(X= x | N, M, K) = \\frac{\\binom{M}{x} \\binom{N-M}{K-x}}{\\binom{N}{K}}, &amp;&amp; x = 0, 1, \\dots, K \\end{align} \\] Where does this formula come from? The numerator counts the number of ways to choose \\(K\\) balls such that \\(x\\) are red. The denominator counts the total number of ways to choose \\(K\\) red balls. There are \\(\\binom{M}{x}\\) ways to choose \\(x\\) red balls from \\(M\\) total red ones. For each of these ways, there are \\(\\binom{N-M}{K-x}\\) ways to choose the other \\(K-x\\) balls so that they are all green. Multiplying these terms together gives the total number of ways to choose exactly \\(x\\) red balls from the urn in \\(K\\) draws. We then divide this number by the total number of ways to draw \\(K\\) balls from the urn, \\(\\binom{N}{K}\\). Heres an example. We set up an urn with 100 coloured balls, 70 of them red. We draw 20 of these balls without replacement and mark down how many of them are red. We then put those balls back in the urn and shake it up. We repeat this process a total of 150 times and animate the counts: N &lt;- 100 # 100 total balls M &lt;- 70 # 70 red K &lt;- 20 # We draw 20 balls to count how many are red reps &lt;- 150 # We repeat this a total of 150 times # A vector of the 150 results, each a number between 0 and 20 representing the number of red balls drawn. set.seed(71) sim_hyp &lt;- rhyper(reps, M, N-M, K) # Animation requires we have a data frame that contains the distribution of results at each draw 1-150 # So the data frame below is a little convoluted results_hyp &lt;- tibble( pos_num_red = rep(0:K,reps), draw_num = rep(1:reps,each=K+1))%&gt;% rowwise()%&gt;% mutate(count_obs = sum(sim_hyp[1:draw_num]==pos_num_red)) plot_hyp &lt;- ggplot(results_hyp)+ geom_col(aes(x = pos_num_red, y=count_obs), fill = tol_light[[4]])+ xlab(&quot;Number of Red Balls in the Draw&quot;)+ ylab(&quot;Counts of Results&quot;)+ facet_wrap(~draw_num) # the code required to animate is the same as if we wanted to facet by some variable anim_hyp &lt;- plot_hyp + facet_null()+ # but instead of facetting by that variable... geom_text(x = 4 , y = 28, aes(label = paste0(&quot;Draws Performed: &quot;,as.character(draw_num))), size = 5)+ transition_states(draw_num, # we include the variable in a gganimate function transition_length = 3, state_length = 1) animate(anim_hyp,nframes=350,fps=20, end_pause = 50) As we might expected, the most common results we see are those close to \\(70/100 \\times 20 = 14\\) red balls, reflecting the proportion we know are in the urn. 3.2 Questions Further Reading The chapter points to the four volumes of Johnson, N. L., and Kotz, S. (1969-1972). Distributions in Statistics as a more comprehensive catalogue of statistical distributions, as well as the updated volumes Johnson, N. L., Kotz, S., and Balakrishnan, N. (1994). Continuous Univariate Distributions, Volume 1 Johnson, N. L., Kotz, S., and Balakrishnan, N. (1995). Continuous Univariate Distributions, Volume 2 Johnson, N. L., Kotz, S., and Kemp, A. W. (1992). Univariate Discrete Distributions "],["multiple_variables.html", "Chapter 4 Multiples Random Variables 4.1 Chapter Notes 4.2 Questions Further Reading", " Chapter 4 Multiples Random Variables 4.1 Chapter Notes This chapter describes probability concepts when we have more than one random variable. Introduced are: N-dimensional random vectors Joint probability mass functions Expectations with joint pmfs Marginal pmfs Joint probability density functions Expectations with joint pdfs Marginal pdfs - with some useful tips for performing the integrations required Joint cdfs Conditional pmfs and pdfs Independence We can: Check whether two random variables \\(X\\) and \\(Y\\) are independent given a joint pmf/pdf, or Define a model in which \\(X\\) and \\(Y\\) are independent using the definition below. Definition 4.2.5 - Independence Let \\((X,Y)\\) be a bivariate random vector with joint pmf/pdf \\(f(x,y)\\) and marginal pmfs/pdfs \\(f_X(x)\\) and \\(f_Y(y)\\). We say \\(X\\) and \\(Y\\) are independent random variables if for every \\(x \\in \\mathcal{R}\\), \\(y \\in \\mathcal{R}\\) \\[ f(x,y)=f_X(x)f_Y(y). \\] Heres a Lemma that makes checking independence easier: Lemma 4.2.7 Let \\((X,Y)\\) be a bivariate random vector with joint pmf/pdf \\(f(x,y)\\). Then \\(X\\) and \\(Y\\) are independent random variables if and only if there exist functions \\(g(x)\\) and \\(h(y)\\) such that for every \\(x \\in \\mathcal{R}\\), \\(y \\in \\mathcal{R}\\), \\[ f(x,y)=g(x)h(y). \\] The proof is surprisingly straightforward (p153.) The expectation of the product of functions of independent random variables is the product of the expectations: \\[ E(g(X)h(y))=(Eg(X))(Eh(y)) \\] Bivariate Transformations This section is about the distribution of functions of bivariate random vectors. As an example the chapter demonstrates that if \\(X \\sim \\text{Poisson}(\\theta)\\) and \\(Y \\sim \\text{Poisson}(\\lambda)\\), and \\(X\\) and \\(Y\\) are independent then \\(X +Y \\sim \\text{Poisson}(\\theta+\\lambda)\\). There are similar cases in the chapter for the product of beta variables, and the sum, differences, and ratio of normal variables. Hierarchical Models and Mixture Distributions The first example in this section is the Binomial-Poisson hierarchy. Say an insect lays a large number of eggs (we might model this with a Poisson distribution). Each of the eggs have, say, probability \\(p\\) of surviving (we use a binomial). How many surviving eggs are there? If we use \\(X\\) to mean the number of surviving eggs and \\(Y\\) to mean the number of eggs laid we can write down a hierarchical model: \\[ \\begin{aligned} X|Y &amp;\\sim \\text{binomial}(Y,p) \\\\ Y &amp;\\sim \\text{Poisson}(\\lambda) \\end{aligned} \\] Here we model \\(Y\\) with a Poisson distribution and then use \\(Y\\) inside on a binomial distribution. When our distribution depends on a quantity that also has a distribution we call it a mixture distribution. The section contains a couple of useful theorems for conditional expectations and variances. Theorem 4.4.3 - Conditional Expectation If \\(X\\) and \\(Y\\) are any two random variables, then \\[ EX = E(E(X|Y)) \\] provided that the expectations exist. \\(E\\) here stands for a couple different things. There shouldnt be confusion but if we want to nitpick we should write: \\[ E_XX = E_Y(E_{X|Y}(X|Y)). \\] The proof starts by writing down the definition of \\(EX\\) and then using the (rearranged) definition of conditional probability \\(f(x,y)=f(x|y)f(y)\\): \\[ \\begin{aligned} E_XX = \\int\\int x f(x,y) dxdy &amp;= \\int \\left[ \\int xf(x|y)dx\\right]f_Y(y)dy \\\\ &amp;= \\int \\left[E_{X|Y}(X|y)\\right]f_Y(y)dy\\\\ &amp;= E_Y(E_{X|Y}(X|Y)). \\end{aligned} \\] Applying this to the insect example, our expected number of survivors becomes: \\[ \\begin{aligned} EX &amp;= E(E(X|Y))\\\\ &amp;= E(pY) &amp;&amp; (\\text{since } X|Y \\sim \\text{binomial}(Y,p))\\\\ &amp;= p \\lambda &amp;&amp; (\\text{since } Y \\sim \\text{Poisson}(\\lambda)). \\end{aligned} \\] What about conditional variances? Theorem 4.4.7 - Conditional Variance If \\(X\\) and \\(Y\\) are any two random variables, then \\[ \\text{Var}(X) = E( \\text{Var}(X|Y)) + \\text{Var}(E(X|Y)), \\] provided the expectations exist. Covariance and Correlation This section introduces measures of the strength of the relationship between two random variables. We have the covariance: \\[ \\begin{aligned} \\text{Cov}(X,Y) &amp;= E((X-\\mu_x)(Y-\\mu_y)) \\\\ &amp;= EXY-\\mu_x\\mu_Y &amp;&amp; \\text{this is Theorem 4.5.3 in the chapter} \\end{aligned} \\] and the correlation: \\[ \\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_x \\sigma_Y} \\] Independence implies covariance and correlation of 0, but the reverse is not true (Theorem 4.5.5). Covariance is important for measuring variation in the sums of random variables: Theorem 4.5.6 If \\(X\\) and \\(Y\\) are any two random variables and \\(a\\) and \\(b\\) are any two constants then \\[ \\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X,Y) \\] Multivariate Distribution The chapter extends discussion to multivariate random vectors \\((X_1, \\dots\\ X_n)\\). One example is the multinomial - generalising the binomial into situations with \\(n\\) (instead of 2) outcomes. Definition 4.6.2 - The Multinomial Distribution Let \\(n\\) and \\(m\\) be positive integers and let \\(p_1, \\dots, p_n\\) be numbers satisfying \\(0\\leq p_i \\leq 1\\) and \\(\\sum_{i=1}^n p_i = 1\\). Then the random vector \\((X_1, \\dots, X_n)\\) has a multinomial distribution with \\(m\\) trials and cell probabilities \\(p_1, \\dots, p_n\\) if the joint pmf of \\((X_1, \\dots, X_n)\\) is: \\[ f(x_1, \\dots, x_n) = \\frac{m!}{x_1!\\dots x_n!}p_1^{x_1} \\dots p_n^{x_n} = m! \\prod_{i=1}^n \\frac{p_i^{x_i}}{x_i!} \\] on the set of \\((x_1, \\dots, x_n)\\) such that each \\(x_i\\) is a nonnegative integer and they sum to \\(m\\). The factor \\(\\frac{m!}{x_1!\\dots x_n!}\\) is the multinomial coefficient. The number of ways that M objects can be divided into \\(n\\) groups with \\(x_i\\) in the \\(i\\)th group. We also have the multinomial theorem as an extension of the binomial theorem: Theorem 4.6.4 - The Multinomial Theorem Let \\(n\\) and \\(m\\) be positive integers and let \\(\\mathcal A\\) be the set of vectors \\(\\mathbf{x} = (x_1, \\dots, x_n)\\) such that each \\(x_i\\) is a nonnegative integer and they sum to \\(m\\). Then, for any real numbers \\(p_1, \\dots, p_n\\), \\[ (p_1, \\dots, p_n)^m = \\sum_{\\mathbf{x} \\in \\mathcal A}\\frac{m!}{x_1!\\dots x_n!}p_1^{x_1} \\dots p_n^{x_n} \\] Marginalising the multinomial distribution returns the binomial, as youd expect. The distribution of \\((X_1, \\dots X_{n-1})\\) given \\(X_n = x_n\\) is multinomial. Exercise 4.39 (which I hope to do below) shows that all the pairwise covariances of our \\(X_i\\)s are negative and are given by: \\[ \\text{Cov}(X_i,X_j) = E[(X_i -p_i)(X_j - p_j)] = -mp_ip_j \\] The negative correlation is greater for variables with higher success probabilities. Inequalities The chapter closes with a section introducing a number of hugely important inequalities: Holders Inequality Cauchy-Schwarz Inequality Covariance Inequality Liapounovs Inequality Minkowskis Inequality Jensens Inequality 4.2 Questions Ex 4.39 Question Let (\\(X_1, \\dots , X_n\\)) have a multinomial distribution with \\(m\\) trials and cell probabilities \\(p_1, \\dots ,p_n\\) (see Definition 4.6.2). Show that, for every \\(i\\) and \\(j\\), \\[ \\begin{aligned} X_i | X_j = x_j &amp;\\sim \\text{binomial} \\left( m-x_j, \\frac{p_i}{1-p_j} \\right) \\\\ X_j &amp;\\sim \\text{binomial}\\ (m, p_j) \\end{aligned} \\] and that \\(\\text{Cov}(X_i, X_j) = -mp_ip_j\\). Answer Further Reading "],["sample_properties.html", "Chapter 5 Properties of a Random Sample 5.1 Chapter Notes 5.2 Questions Further Reading", " Chapter 5 Properties of a Random Sample 5.1 Chapter Notes A random sample of size \\(n\\) means we have \\(n\\) mutually independent and identically distributed random variables \\(X_1, \\dots, X_n\\). Because they are all independent and have the same marginal pdf, their joint pdf is given by: \\[ f(x_1, \\dots x_n) = f(x_1)f(x_2)\\dots f(x_n) = \\prod_{i=1}^n f(x_i) \\] and we can use it to calculate the probabilities related to our sample. This definition of a random sample is called sampling from an infinite population. However were very often interested in working with finite populations. If were sampling with replacement - the conditions of our definition of a random sample are met, but if were sampling without replacement from a finite population, our random variables are no longer independent. Sampling from a finite population without replacement is sometimes called simple random sampling. It doesnt meet the criteria of our definition above, but if population size \\(N\\) is large enough relative to our sample size \\(n\\), we might say that the samples are nearly independent. This means that the conditional distribution of \\(X_i\\) given the other \\(n-1\\) random variables is not too dissimilar from the marginal distribution of \\(X_i\\). Sums of Random Variables from a Random Sample So what can we do with these random samples? The chapter introduces the concept of a statistic If we have a real-valued (or vector-valued) function \\(T\\) whose domain includes the sample space of (\\(X_1, \\dots, X_n\\)), then we call the random variable (or random vector) \\(Y = T(X_1, \\dots, X_n)\\) a statistic. We call the probability distribution of \\(Y\\) the sampling distribution of \\(Y\\). The chapter notes that the definition of a statistic is very broad, but notes that it cannot be a function of a parameter - it is a function of the sample only. Examples might include the average sample value, the highest and lowest values in the sample, some measure of variability in the sample etc. The sample mean statistic is: \\[ \\overline X = \\frac{1}{n} \\sum_{i=1}^n X_i \\] The sample variance statistic is: \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i -\\overline X)^2 = \\frac{1}{n-1}\\left( \\sum_{i=1}^n X_i^2 - n\\overline X^2 \\right) \\] with the last equality because the sum of all \\(x_i - \\overline x\\) is 0. We have the \\(n-1\\) is the denominator of the sample variance. The following theorem shows why: Theorem 5.2.6 - Unbiased Estimators Let \\(X_1, \\dots X_n\\) be a random sample from a population with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\). Then: \\(E\\overline X = \\mu\\) b.\\(\\text{Var}(\\overline X) = \\frac{\\sigma^2}{n}\\) \\(E S^2 = \\sigma^2\\) Ill focus on part c - that the sample variance defined above is an unbiased estimator of the population variance. Heres the proof: \\[ \\begin{aligned} ES^2 &amp;= E\\left( \\frac{1}{n-1} \\left[ \\sum_{i=1}^n X_i^2 - n\\overline X^2 \\right] \\right) \\\\ &amp;= \\frac{1}{n-1} \\left( nEX_1^2 - n E\\overline X^2 \\right) \\\\ &amp;= \\frac{1}{n-1} \\left( n(\\sigma^2 + \\mu^2) - n \\left( \\frac{\\sigma^2}{n} + \\mu^2\\right) \\right) \\\\ &amp;= \\frac{n\\sigma^2}{n-1} + \\frac{n\\mu^2}{n-1} - \\frac{\\sigma^2}{n-1} - \\frac{n\\mu^2}{n-1} = \\frac{(n-1)\\sigma^2}{n-1} \\\\ &amp;= \\sigma^2 \\end{aligned} \\] The third equality comes from rearranging the property of variances \\(\\text{Var}(X) = E(X^2) - E(X)^2\\), and from the expression for the variance of the sample mean in part b of the theorem. Note that if the sampling variance had used an \\(n\\) in the denominator rather than an \\(n-1\\), the expected value of \\(S^2\\) would be \\(\\frac{n-1}{n}\\sigma^2\\). Sums from the Normal Distribution The properties of our sample mean and variance above required very few conditions. When we add the additional assumptions of normality, we can derive a lot more. Theorem 5.3.1 in the book states that if we have a random sample drawn from a \\(\\text{Normal}(\\mu, \\sigma^2)\\) distribution, we have: \\(\\overline X\\) and \\(S^2\\) are independent random variables \\(\\overline X\\) has a a \\(\\text{Normal}(\\mu, \\sigma^2/n)\\) distribution \\(\\frac{(n-1)S^2}{\\sigma^2}\\) has a chi-squared distribution with \\(n-1\\) degrees of freedom. Here are some facts about chi-squared random variables from Lemma 5.3.2 The square of a standard normal (\\(\\text{Normal}(0,1)\\)) random variable is a chi-squared random variable with degree 1 (\\(Z^2 \\sim \\chi^2_1\\)). Independent chi-squared variables add to a chi-squared variable, and their degrees of freedom also add: \\(X_1 + \\dots + X_n \\sim \\chi^2_{p_1 + \\dots + p_n}\\) if \\(X_i \\sim \\chi^2_{p_i}\\) and the \\(X_i\\)s are independent. In most practical cases, our population variance \\(\\sigma^2\\) is unknown. So if we want to estimate the variability of \\(\\overline X\\) as part of making inferences about \\(\\mu\\), we need a way to estimate \\(\\sigma^2\\). For a random sample of size \\(n\\) from a \\(\\text{Normal}(\\mu,\\sigma^2)\\) distribution, we know that \\[ \\frac{\\overline X - \\mu}{\\sigma/\\sqrt n} \\] has a \\(\\text{Normal}(0,1)\\) distribution. W.S. Gosset introduced the Students t-distribution, and showed that \\[ \\frac{\\overline X - \\mu}{S/\\sqrt n} \\] has Students t distribution with \\(n-1\\) degrees of freedom. The pdf with \\(p\\) degrees of freedom is: \\[ f_{T}(t)=\\frac{\\Gamma\\left(\\frac{p-1}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right)} \\frac{1}{(p \\pi)^{1 / 2}} \\frac{1}{\\left(1+t^{2} / p\\right)^{(p+1) / 2}}, \\quad-\\infty&lt;t&lt;\\infty \\] We have properties: \\[ \\begin{aligned} \\mathrm{E} T_{p}&amp;=0, \\quad \\text{if } p&gt;1 \\\\ \\operatorname{Var} T_{p}&amp;=\\frac{p}{p-2}, \\quad \\text{if } p&gt;2 \\end{aligned} \\] The chapter also introduces Snedecors F distribution, which is used for comparing the variability of two different populations. Say we have \\(n\\) random variables \\(X_i\\) from a \\(\\text{Normal}(\\mu_X,\\sigma^2_X)\\) distribution, and \\(m\\) random variables \\(Y_i\\) from a \\(\\text{Normal}(\\mu_Y,\\sigma^2_Y)\\) distribution. The random variable: \\[ F = \\frac{S_{X}^{2} / S_{Y}^{2}}{\\sigma_{X}^{2} / \\sigma_{Y}^{2}}=\\frac{S_{X}^{2} / \\sigma_{X}^{2}}{S_{Y}^{2} / \\sigma_{Y}^{2}} \\] has F distribution with \\(n-1\\) and \\(m-1\\) degrees of freedom. Heres the pdf of a \\(F_{p,q}\\) distribution: \\[ f_{F}(x)=\\frac{\\Gamma\\left(\\frac{p+q}{2}\\right)}{\\Gamma\\left(\\frac{p}{2}\\right) \\Gamma\\left(\\frac{q}{2}\\right)}\\left(\\frac{p}{q}\\right)^{p / 2} \\frac{x^{(p / 2)-1}}{[1+(p / q) x]^{(p+q) / 2}}, \\quad 0&lt;x&lt;\\infty \\] Convergence Concepts Heres a nice summary from the chapter about what this section is all about: This section treats the somewhat fanciful idea of allowing the sample size to approach infinity and investigates the behavior of certain sample quantities as this happens. Although the notion of an infinite sample size is a theoretical artifact, it can often provide us with some useful approximations for the finite-sample case, since it usually happens that expressions become simplified in the limit. Well see three notions of convergence: convergence in probability, almost sure convergence, and convergence in distribution. Definition 5.5.1 Convergence in Probability A sequence of random variables \\(X_1, X_2, \\dots\\) convergences in probability to a random variable \\(X\\) if for every \\(\\epsilon &gt;0\\) \\[ \\lim_{n \\to \\infty} P(|X_n - X| \\geq \\epsilon) = 0 \\] or equivalently \\[ \\lim_{n \\to \\infty} P(|X_n - X| &lt; \\epsilon) = 1 \\] The \\(X_i\\)s here are typically not iid, the distributions change as the sequence progresses. The Weak Law of Large Numbers concerns a situation where \\(X\\) is a constant, and the random variables in the sequence are sample means: Theorem 5.5.2 The Weak Law of Large Numbers Let \\(X_1, X_2, \\dots\\) be iid with \\(EX_i = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\). Define \\(\\overline X = 1/n \\sum_{i=1}^n X_i\\). Then for every \\(\\epsilon &gt; e\\), \\[ \\lim_{n \\to \\infty} P(|\\overline X_n - \\mu| &lt; \\epsilon) = 1 \\] I.e. our sample means converge in probability to \\(\\mu\\). The proof is a two-line application of Chebychevs Inequality. Revisit once Ive returned to the inequality section of chapter four. If random variables \\(X_1, X_2, \\dots\\) converge in probability to \\(X\\), then \\(h(X_1), h(X_2), \\dots\\) converges in probability to \\(h(X)\\), for continous function \\(h\\). Definition 5.5.6 Almost Sure Convergence A sequence of random variables \\(X_1, X_2, \\dots\\) convergences almost surely to a random variable \\(X\\) if for every \\(\\epsilon &gt;0\\) \\[ P(\\lim_{n \\to \\infty} |X_n - X| &lt; \\epsilon) = 1 \\] This is a much stronger condition, and almost-sure convergence implies convergence in probability. From the chapter: To understand almost sure convergence, we must recall the basic definition of a random variable as given in Definition 1.4.1. A random variable is a real-valued function defined on a sample space \\(S\\). If a sample space \\(S\\) has elements denoted by \\(s\\), then \\(X_n(s)\\) and \\(X(s)\\) are all functions defined on \\(S\\). Definition 5.5.6 states that \\(X_n\\) converges to \\(X\\) almost surely if the functions \\(X_n(s)\\) converge to \\(X(s)\\) for all \\(s \\in S\\) except perhaps for \\(s \\in N\\), where \\(N \\subset S\\) and \\(P(N) = 0\\). As stated in that paragraph, convergence need not happen on sets with probability 0, which is why this is called almost-sure convergence. As an example, the chapter uses \\(X_n(s) = s+s^n\\) and \\(X(s)=s\\), where \\(s \\in [0,1]\\). \\(X_n\\) converges to \\(X\\) everywhere except 1. As an example of a sequence that converges in probability but not almost surely, the chapter uses \\(X(s) = s\\) and: \\[ \\begin{aligned} X_1(s) &amp;= s+I_{[0,1]}(s) \\\\ X_2(s) &amp;= s+I_{[0,\\frac{1}{2}]}(s) \\\\ X_3(s) &amp;= s+I_{[\\frac{1}{2},1]}(s) \\\\ X_4(s) &amp;= s+I_{[0,\\frac{1}{3}]}(s) \\\\ X_5(s) &amp;= s+I_{[\\frac{1}{3},\\frac{2}{3}]}(s) \\\\ X_6(s) &amp;= s+I_{[\\frac{2}{3},1]}(s) \\\\ &amp;\\vdots \\end{aligned} \\] for \\(s \\in S = [0,1]\\). The probability that \\(|X_n - X| \\geq \\epsilon\\) is the probability of an interval with length going to 0 - so the sequence converges in probability. However the sequence does not converge almost surely, and in fact there is no \\(s\\) such that \\(X_n(s) \\to s = X(s)\\). For any \\(s\\) the sequence will alternate between \\(s\\) and \\(s+1\\) infinitely often. Almost sure convergence requires convergence everywhere except perhaps sets with probability 0. The sequence above has convergence nowhere. Theorem 5.5.9 The Strong Law of Large Numbers Let \\(X_1, X_2, \\dots\\) be iid with \\(EX_i = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\). Define \\(\\overline X = 1/n \\sum_{i=1}^n X_i\\). Then for every \\(\\epsilon &gt; e\\), \\[ P(\\lim_{n \\to \\infty} |\\overline X_n - \\mu| &lt; \\epsilon) = 1 \\] I.e. our sample means converge almost surely to \\(\\mu\\). Definition 5.5.6 Convergence in Distribution A sequence of random variables \\(X_1, X_2, \\dots\\) convergences in distribution to a random variable \\(X\\) if \\[ \\lim_{n \\to \\infty} F_{X_n}(x) = F_X(x) \\] at all points \\(x\\) where \\(F_X(x)\\) is continuous. Its really the cdfs that converge and not the random variables. Convergence in probability implies convergence in distribution. With the concept of convergence in distribution in hand, the chapter introduces the Central Limit Theorem: Theorem 5.5.14 The Central Limit Theorem (Stronger Form) Let \\(X_1,X_2, \\dots\\) be a sequence of iid random variables with \\(EX_i = \\mu\\) and \\(0 &lt; \\text{Var}X_i = \\sigma^2 &lt; \\infty\\). Define \\(\\overline X = 1/n \\sum_{i=1}^n X_i\\). Let G_n(x) denote the cdf of \\(\\sqrt n (\\overline X_n - \\mu)/ \\sigma\\) Then, for any \\(x\\), \\(-\\infty &lt; x &lt; \\infty\\). \\[ \\lim_{n \\to \\infty} G_n(x) = \\int^x_{-\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-y^2/2} dy \\] that is, \\(\\frac{\\sqrt n (\\overline X - \\mu)}{\\sigma}\\) has a limiting standard normal distribution. What does this mean? The chapter stresses that we have arrived at normality from only very general starting assumptions, pretty impressive! Example 5.5.16 Normal Approximation to the Negative Binomial Say we have a random sample of size \\(n\\) from a negative binomial\\((r,p\\) distribution. Recall that \\[ \\begin{aligned} EX &amp;= \\frac{r(1-p)}{p} \\\\ \\text{Var}X &amp;= \\frac{r(1-p)}{p^2} \\end{aligned} \\] and so by the Central Limit Theorem: \\[ \\frac{\\sqrt{n}(\\overline{X}-r(1-p) / p)}{\\sqrt{r(1-p) / p^{2}}} \\] is approximately \\(\\text{Normal}(0,1)\\). Proving Tools The chapter doesnt call these proving tools, that terminology comes from the handbook Im referring to alongside Statistical Inference itself. It uses the term proving tools to refer to Slutskys theorem, the Delta method, and Taylor expansion. The chapter does call Slutskys Theorem [a]n approximation tool that can be used in conjunction with the Central Limit Theorem. Theorem 5.5.17 Slutskys Theorem If \\(X_n \\to X\\) in distribution and \\(Y_n \\to a\\), a constant, in probability, then \\(Y_nX_n \\to aX\\) in distribution \\(X_n + Y_n \\to X+a\\) in distribution. Revist: Delta Method, Taylor expansion. Generating a Random Sample Starting from generated uniform iid random variables, we want methods to transform these into random variables from a target distribution. We saw the inverse CDF method in chapter 2. This is a direct method, in the sense that it gives us a closed-form function for generating our random variable. Unfortunately, sometime closed-form functions for the inverse cdf do not exist and wed need to solve a (perhaps very complicated) integral to get our random variable. This is the case for generating a \\(\\chi^2_1\\) random variable, which would get us a \\(\\text{Normal}(0,1)\\) variable. What are indirect methods? Revisit: page 251 - add accept/ reject algorithm and metropolis algorithm. 5.2 Questions Further Reading "],["data_reduction.html", "Chapter 6 Principles of Data Reduction 6.1 Chapter Notes 6.2 Questions Further Reading", " Chapter 6 Principles of Data Reduction 6.1 Chapter Notes This chapter is all about simplifying the process of making inferences from a sample - i.e. restricting the kind of analyses we perform in some principled way. Its really about norms of statistical analysis, with these norms justified by deduction from certain plausible-sounding assumptions. Well come across the Sufficiency Principle, the Likelihood Principle, and the Equivariance Principle - all norms that restrict in a useful way the permissible inferences from a sample. All are justified by straightforward-seeming assumptions, but none are universally accepted among statisticians, and many common statistical procedures violate one or more of them. The chapter begins by noting that a statistic is a kind of data reduction technique, in the sense that if you base decisions or inferences on the statistic, you will treat two samples equivalently so long as they produce the same value of the statistic. In this way, statistics partition the sample space. The Sufficiency Principle Data reduction of some kind is going to be necessary, but how can we know that by summarising the data weve collected in certain way, were not throwing away information about the parameter of interest \\(\\theta\\)? The chapter introduces the concept of a sufficient statistic: Definition 6.2.1 A Sufficient Statistic A statistic \\(T(\\mathbf X)\\) is a sufficient statistic for \\(\\theta\\) if the conditional distribution of the sample \\(\\mathbf X\\) given \\(T(\\mathbf X)\\) does not depend on \\(\\theta\\). A sufficient statistic contains all the information about \\(\\theta\\) that is present in the sample - once you know the sufficient statistic \\(T(\\mathbf X)\\), you dont get any more information about \\(\\theta\\) by also knowing the sample. This is the sufficiency principle: Sufficiency Principle: If \\(T(\\mathbf{X})\\) is a sufficient statistic for \\(\\theta\\), then any inference about \\(\\theta\\) should depend on the sample \\(\\mathbf{X}\\) only through the value \\(T(\\mathbf{X})\\). That is, if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two sample points such that \\(T(\\mathbf{x})=T(\\mathbf{y})\\), then the inference about \\(\\theta\\) should be the same whether \\(\\mathbf{X}=\\mathbf{x}\\) or \\(\\mathbf{X}=\\mathbf{y}\\) is observed. For example, say you have iid random variables \\(X_1, \\dots, X_n\\) that are each Bernoulli distributed with unknown parameter \\(\\theta\\). Then the statistic \\(T( \\mathbf X ) = X_ + \\dots + X_n\\) (i.e. the sum of all the 1s in the sample) is a sufficient statistic for the parameter \\(\\theta\\). Once you know the sum of the random variables, you dont get any more information about \\(\\theta\\) by knowing, say, whether \\(X_3\\) is a zero or a one. Another example is the sample mean for a normal distribution. Say you have random variables \\(X_1, \\dots, X_n\\), each iid \\(\\text{Normal}(\\mu, \\sigma^2)\\) variables. Then the sample mean \\(T(\\mathbf X) = \\overline {\\mathbf X} = (X_1 + \\dots + X_n)/n\\) is a sufficient statistic for \\(\\mu\\). These examples are in some sense unusual - the chapter states that sufficient statistics that have a smaller dimension than the size of the sample are not common outside of distributions in the exponential family. The factorisation theorem is useful for finding sufficient statistics by looking at the sample pdf/pmf: Theorem 6.2.6 Factorisation Theorem Let \\(f(\\mathbf x|\\theta)\\) be the joint pdf/pmf of the sample \\(\\mathbf X\\). A statistic \\(T(\\mathbf X)\\) is a sufficient statistic if and only if there exist functions \\(g(t | \\theta)\\) and \\(h(\\mathbf X)\\) such that for all sample points \\(\\mathbf x\\) and parameter points \\(\\theta\\) \\[ f(\\mathbf x | \\theta) = g( T(\\mathbf X)| \\theta)h(\\mathbf x) \\] We factor the pdf/pmf into two parts, only one of which depends on \\(\\theta\\). The part that depends on \\(\\theta\\) depends on the sample \\(\\text x\\) only through function \\(T(\\mathbf X)\\) and this function is a sufficient statistic for \\(\\theta\\). Since were using sufficient statistics for data reduction, we want to know if some sufficient statistics do a better job of reducing the data than others. A minimal sufficient statistic is one that reduces the data as much as possible. Sufficient statistics \\(T(\\mathbf X)\\) is called a minimal sufficient statistic if, for any other sufficient statistic \\(T&#39;(\\mathbf X)\\), \\(T&#39;(\\mathbf x)=T&#39;(\\mathbf y) \\implies T(\\mathbf x) = T(\\mathbf y)\\). Looking back to the notion of a statistic as partitioning the sample space, we can say that a minimal statistic is the coarsest possible partition that retains all information about \\(\\theta\\). The chapter also introduces the concept of an ancillary statistic. An ancillary statistic for \\(\\theta\\) is one whose distribution does not depend on \\(\\theta\\). For example, if \\(X_1, \\dots, X_n\\) are iid uniform variables on the interval \\((\\theta, \\theta + 1), \\ -\\infty &lt; \\theta &lt; \\infty\\) then the range statistic \\(R = X_{(n)}-X_{(1)}\\) is an ancillary statistic. Here \\(X_{(1)}, \\dots X_{(n)}\\) refer to the order statistics (e.g. \\(X_{(1)} = \\min(X_1, \\dots, X_n)\\) and \\(X_{(n)} = \\max(X_1, \\dots, X_n)\\)). More generally, if \\(\\theta\\) is a a location parameter in any location parameter family distribution, the range statistic is ancillary. It isnt necessarily the case that minimally sufficient statistics for \\(\\theta\\) are independent of ancillary statistics for \\(\\theta\\). The chaptetr gives this toy example: Example 6.2.20 Ancillary Precision Let \\(X_1\\) and \\(X_2\\) be iid observations from the discrete distribution that satisfies \\[ P_\\theta(X=\\theta)=P_\\theta(X=\\theta+1)=P_\\theta(X=\\theta+2)=\\frac{1}{3}, \\] where \\(\\theta\\), the unknown parameter, is any integer. Then \\((R, M)\\), where \\(R=X_{(2)}-X_{(1)}\\) and \\(M=\\left(X_{(1)}+X_{(2)}\\right) / 2\\), is a minimal sufficient statistic. Since this is a location parameter family, \\(R\\) is an ancillary statistic. \\(R\\) is ancillary, but can still give information about \\(\\theta\\) if we know a sample point \\((r,m)\\). The chapter then introduces complete statistics. I had difficulty figuring out what was going on when I first saw the chapter definition (and Im still not super confident!). Complete statistics are first described as useful for a description of situations in which [a minimal sufficient statistic is independent of any ancillary statistic]. Heres the definition: Definition 6.2.21 Complete Statistics Let \\(f(t | \\theta)\\) be a family of pdfs or pmfs for a statistic \\(T(\\mathbf{X})\\). The family of probability distributions is called complete if \\(E_{\\theta} g(T)=0\\) for all \\(\\theta\\) implies \\(P_{\\theta}(g(T)=0)=1\\) for all \\(\\theta\\). Equivalently, \\(T(\\mathbf{X})\\) is called a complete statistic. Whats going on here? You have some function \\(g\\), of a complete statistic \\(T(\\mathbf{X})\\), and you specify that you want \\(g(T)\\) to be an unbiased estimator of 0. Then \\(g(T)\\) must equal 0 with probability 1 for all \\(\\theta\\) in your family of distributions. The only unbiased estimator of 0 is the 0 function. I still dont really understand the motivation for this definition. It ends up being useful (via Basus theorem) for determining that two statistics are independent without needing to examine their joint distribution. For example you can show that \\(\\overline{X}\\) and \\(S^2\\) are independent when sampling from a \\(\\text{Normal}(\\mu,\\sigma^2)\\) population. Heres Basus theorem: Theorem 6.2.24 Basus Theorem If \\(T(\\mathbf{X})\\) is a complete and minimal sufficient statistic, then \\(T(\\mathbf{X})\\) is independent of every ancillary statistic. Although the chapter later notes that including minimal here is slightly redundant, since it can be shown that if a minimal sufficient statistic exists then every complete statistic is also a minimal sufficient statistic. The Likelihood Principle Definition 6.3.1 The Likelihood Function Given a joint pdf/pmf of a sample \\(f(\\mathbf{x} | \\theta)\\) and an observation \\(\\mathbf{X}=\\mathbf{x}\\), the function of \\(\\theta\\) defined by \\[ L(\\theta \\mid \\mathbf{x})=f(\\mathbf{x} | \\theta) \\] is called the likelihood function. The important difference between a likelihood and a pdf/pmf is that with a likelihood the data is considered fixed, and we vary the parameter \\(\\theta\\). The likelihood principle is a proposal for using the likelihood function to reduce/ summarise data. The Likelihood Principle: If \\(\\mathbf x\\) and \\(\\mathbf y\\) are two sample points such that \\(L(\\theta | \\mathbf x)\\) is proportional to \\(L(\\theta | \\mathbf y)\\), that is, there exists a constant \\(C(\\mathbf x, \\mathbf y)\\) such that \\(L(\\theta | \\mathbf x)=C(\\mathbf x, \\mathbf y) L(\\theta | \\mathbf y)\\) for all \\(\\theta\\), then the conclusions drawn from \\(\\mathbf x\\) and \\(\\mathbf y\\) should be identical. Notice that \\(C(\\mathbf x, \\mathbf y)\\) does not depend on \\(\\theta\\), but it can be different for different values of \\(\\mathbf x\\) and \\(\\mathbf y\\). The idea here is that the likelihood is going to be used to compare the plausibility of different \\(\\theta\\) values, and so if you have two sample points that produce likelihood functions in a fixed ratio with each other for all \\(\\theta\\), the relative plausibility of each \\(\\theta\\) value should be the same for both. I.e. the same inferences should be drawn about \\(\\theta\\), no matter if you observed \\(\\mathbf x\\) or \\(\\mathbf y\\). Section 6.3.2 of the chapter derives the likelihood principles from two simpler principles. We start with the concept of an experiment. The chapter defines an experiment \\(E\\) as a triple \\((\\mathbf X, \\theta, f(\\mathbf X, \\theta))\\). Here, \\(\\mathbf X\\) is a random vector with pmf \\(f(\\mathbf X, \\theta)\\) for some \\(\\theta\\) in parameter space \\(\\Theta\\). An experimenter will observe some particular sample \\(\\mathbf x\\) and draw some conclusion about \\(\\theta\\). We denote this conclusion \\(\\text{Ev}(E, \\mathbf x)\\) - the evidence about \\(\\theta\\) arising from \\(E\\) and \\(\\mathbf x\\). We now introduce the two principles well base the formal likelihood principle on. The first is familiar: The Formal Sufficiency Principle: Consider experiment \\(E=(\\mathbf X, \\theta, f(\\mathbf X, \\theta))\\) and sufficient statistic for \\(\\theta\\) \\(T(\\mathbf{X})\\).If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two sample points such that \\(T(\\mathbf{x})=T(\\mathbf{y})\\), then \\(\\text{Ev}(E, \\mathbf x)\\) = \\(\\text{Ev}(E, \\mathbf y)\\). By accepting the formal sufficiency principle, we are agreeing to equate evidence if the sufficient statistics match. The other principle is the following: Conditionality Principle: Suppose that \\(E_1=(\\mathbf X_1, \\theta, f_1(\\mathbf X_1, \\theta))\\) and \\(E_1=(\\mathbf X_2, \\theta, f_2(\\mathbf X_2, \\theta))\\) are two experiments, where only the unknown parameter \\(\\theta\\) need be common between the two experiments. Consider the mixed experiment in which the random variable \\(J\\) is observed, where \\(P(J=1)=P(J=2)=\\frac{1}{2}\\) (independent of \\(\\theta\\), \\(\\mathbf X_1\\), or \\(\\mathbf X_2\\)), and then experiment \\(E_J\\) is performed. Formally, the experiment performed is \\(E^*=(\\mathbf X^*, \\theta, f^*(\\mathbf X^*, \\theta))\\), where \\(\\mathbf X^*=(j, \\mathbf X_j)\\) and \\(f^*(\\mathbf x^* | \\theta)=f^*((j, \\mathbf x_j) | \\theta)=\\frac{1}{2} f_j(\\mathbf x_j | \\theta)\\). Then \\[ \\text{Ev}(E^*,(j, \\mathbf x_j))=\\text{Ev}(E_j, \\mathbf x_j) \\] What is this saying? If we flip a coin to decide between two experiments, and so end up performing experiment 1 (say) and observing data \\(\\mathbf x\\), the conclusions that we draw about \\(\\theta\\) should be the same as if we non-randomly chose to perform experiment 1 (and observed \\(\\mathbf x\\)) in the first place. The formal likelihood principle can be derived from these two principles. The Formal Likelihood Principle Suppose we have experiments \\(E_1=(\\mathbf X_1, \\theta, f_1(\\mathbf X_1, \\theta))\\) and \\(E_1=(\\mathbf X_2, \\theta, f_2(\\mathbf X_2, \\theta))\\), and sample points \\(\\mathbf x_1^*\\) and \\(\\mathbf x_2^*\\) from each experiment respectively. If \\[ L(\\theta | x_2^*) = CL(\\theta | x_1^*) \\] for all \\(\\theta\\) and constant \\(C(\\mathbf x_1^*, \\mathbf x_2^*)\\), then \\[ \\text{Ev}(E_1, \\mathbf x_1^*) = \\text{Ev}(E_2, \\mathbf x_2^*) \\] The main difference in this statement compared to our first statement of the likelihood principle at the start of the section is that this one concerns two experiments instead of only one. The corollary is that the evidence should depend on \\(E\\) and \\(x\\) only through the likelihood \\(L(\\theta | \\mathbf x)\\). If the likelihood is the same (or the ratio of likelihoods is the same), the conclusion is the same, no matter the experiment or the sample. Birnbaums Theorem states that the Formal Likelihood Principle follows from the Sufficiency Principle and the Conditionality Principle, and that the converse also holds. The rest of the section is an interesting discussion about how much trust we should put in to the likelihood principle. Both the sufficiency principle and the conditionality principle seem so straightforward as to be immediately obvious, but the likelihood principle that follows from them is violated in many common statistical procedures. For example, model checking by examining residuals violates the sufficiency principle, because residuals are not a sufficient statistic. Does this mean that residual checking is misguided? Not necessarily, the chapter states that one drawback of the sufficiency principle is that it is very model-dependent - e.g. if you believe in your normally distributed model, then its true that \\(\\overline X\\) and \\(S^2\\) are sufficient statistics. However you may have good reason to hold on to your model a little more tentatively than that. The discussion is interesting, and has plenty of links to further reading on the topic of these principles and their violation. The Equivariance Principle From the chapter: The Equivariance Principle describes a data reduction technique in a slightly different way [to the principles above]. In any application of the Equivariance Principle, a function \\(T(\\mathbf X)\\) is specified, but if \\(T(\\mathbf X)=T(\\mathbf Y)\\), then the Equivariance Principle states that the inference made if \\(\\mathbf x\\) is observed should have a certain relationship to the inference made if \\(\\mathbf y\\) is observed, although the two inferences may not be the same. We have two types of equivariance: Measurement equivariance - you should draw the same conclusions whether you measure in inches or metres Formal invariance - If two inference problems have the same formal structure in terms of the mathematical model used (the same parameter space \\(\\Theta\\), the same set of sample pdfs/pmfs \\(\\{f(\\mathbf x | \\theta) : \\theta \\in \\Theta\\}\\), the same allowable inferences and consequences of wrong inferences), then the same inference procedure should be used in both problems. The equivariance principle is then: The Equivariance Principle If \\(\\mathbf Y = g(\\mathbf X)\\) is a change of measurement scale such that the model for \\(\\mathbf Y\\) has the same formal structure as the model for \\(\\mathbf X\\), then an inference procedure should be both measurement equivariant and formally equivariant. For example, if youre observing a binomial process, whether youre attempting to estimate \\(p\\) or \\(q=1-p\\) your problem has the same formal structure, with just a different measurement scale. If \\(T(x)\\) is an estimate of \\(p\\) using the number of observed successes \\(x\\) and \\(T^*(y)\\) is the estimate of \\(q\\) using the number of observed failures \\(y = n- x\\), the equivariance principle demands that \\(T(z) = T^*(z)\\) for all \\(z = 0, \\dots, n\\). So we require \\[ T(x) = 1- T^*(n - x) = 1 - T(n - x). \\] This requirement restricts the space of estimators \\(T\\) that well consider using. The principle of measurement equivariance is fairly uncontroversial, but the requirement of formal invariance means that for any two problems with the same mathematical structure (even if the physical realities are very different) the same inference approach is appropriate. This is a stronger assumption. 6.2 Questions Further Reading "],["point_estimation.html", "Chapter 7 Point Estimation 7.1 Chapter Notes 7.2 Questions Further Reading", " Chapter 7 Point Estimation 7.1 Chapter Notes 7.2 Questions Further Reading "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Principles of Data Reduction | Notes on ‘Statistical Inference’ by Casella &amp; Berger</title>
  <meta name="description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Principles of Data Reduction | Notes on ‘Statistical Inference’ by Casella &amp; Berger" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  <meta name="github-repo" content="jake-lawler/StatisticalInference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Principles of Data Reduction | Notes on ‘Statistical Inference’ by Casella &amp; Berger" />
  
  <meta name="twitter:description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  

<meta name="author" content="Jake Lawler" />


<meta name="date" content="2022-03-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sample_properties.html"/>
<link rel="next" href="point_estimation.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="probability_theory.html"><a href="probability_theory.html"><i class="fa fa-check"></i><b>1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability_theory.html"><a href="probability_theory.html#chapter-notes"><i class="fa fa-check"></i><b>1.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#counting"><i class="fa fa-check"></i>Counting</a></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#inequalities-bonferronis-and-booles"><i class="fa fa-check"></i>Inequalities: Bonferroni’s and Boole’s</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability_theory.html"><a href="probability_theory.html#questions"><i class="fa fa-check"></i><b>1.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#ex-1.42"><i class="fa fa-check"></i>Ex 1.42</a></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#ex-1.43"><i class="fa fa-check"></i>Ex 1.43</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#further-reading"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and Expectations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="transformations.html"><a href="transformations.html#chapter-notes-1"><i class="fa fa-check"></i><b>2.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#monotonic-transformations"><i class="fa fa-check"></i>Monotonic Transformations</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#generating-random-samples"><i class="fa fa-check"></i>Generating Random Samples</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#moment-generating-functions"><i class="fa fa-check"></i>Moment Generating Functions</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#differentiating-under-an-integral-sign"><i class="fa fa-check"></i>Differentiating Under an Integral Sign</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="transformations.html"><a href="transformations.html#questions-1"><i class="fa fa-check"></i><b>2.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.1"><i class="fa fa-check"></i>Ex 2.1</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.3"><i class="fa fa-check"></i>Ex 2.3</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.6"><i class="fa fa-check"></i>Ex 2.6</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#further-reading-1"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="families.html"><a href="families.html"><i class="fa fa-check"></i><b>3</b> Common Families of Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="families.html"><a href="families.html#chapter-notes-2"><i class="fa fa-check"></i><b>3.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="families.html"><a href="families.html#discrete-distributions"><i class="fa fa-check"></i>Discrete Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="families.html"><a href="families.html#questions-2"><i class="fa fa-check"></i><b>3.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="families.html"><a href="families.html#further-reading-2"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple_variables.html"><a href="multiple_variables.html"><i class="fa fa-check"></i><b>4</b> Multiples Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple_variables.html"><a href="multiple_variables.html#chapter-notes-3"><i class="fa fa-check"></i><b>4.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#independence"><i class="fa fa-check"></i>Independence</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#bivariate-transformations"><i class="fa fa-check"></i>Bivariate Transformations</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#hierarchical-models-and-mixture-distributions"><i class="fa fa-check"></i>Hierarchical Models and Mixture Distributions</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#covariance-and-correlation"><i class="fa fa-check"></i>Covariance and Correlation</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#multivariate-distribution"><i class="fa fa-check"></i>Multivariate Distribution</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#inequalities"><i class="fa fa-check"></i>Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple_variables.html"><a href="multiple_variables.html#questions-3"><i class="fa fa-check"></i><b>4.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#ex-4.39"><i class="fa fa-check"></i>Ex 4.39</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#further-reading-3"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sample_properties.html"><a href="sample_properties.html"><i class="fa fa-check"></i><b>5</b> Properties of a Random Sample</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sample_properties.html"><a href="sample_properties.html#chapter-notes-4"><i class="fa fa-check"></i><b>5.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i>Sums of Random Variables from a Random Sample</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#sums-from-the-normal-distribution"><i class="fa fa-check"></i>Sums from the Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#convergence-concepts"><i class="fa fa-check"></i>Convergence Concepts</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#proving-tools"><i class="fa fa-check"></i>Proving Tools</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#generating-a-random-sample"><i class="fa fa-check"></i>Generating a Random Sample</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sample_properties.html"><a href="sample_properties.html#questions-4"><i class="fa fa-check"></i><b>5.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#further-reading-4"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data_reduction.html"><a href="data_reduction.html"><i class="fa fa-check"></i><b>6</b> Principles of Data Reduction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data_reduction.html"><a href="data_reduction.html#chapter-notes-5"><i class="fa fa-check"></i><b>6.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-sufficiency-principle"><i class="fa fa-check"></i>The Sufficiency Principle</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-likelihood-principle"><i class="fa fa-check"></i>The Likelihood Principle</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-equivariance-principle"><i class="fa fa-check"></i>The Equivariance Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data_reduction.html"><a href="data_reduction.html#questions-5"><i class="fa fa-check"></i><b>6.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#further-reading-5"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="point_estimation.html"><a href="point_estimation.html"><i class="fa fa-check"></i><b>7</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="point_estimation.html"><a href="point_estimation.html#chapter-notes-6"><i class="fa fa-check"></i><b>7.1</b> Chapter Notes</a></li>
<li class="chapter" data-level="7.2" data-path="point_estimation.html"><a href="point_estimation.html#questions-6"><i class="fa fa-check"></i><b>7.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#further-reading-6"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on ‘Statistical Inference’ by Casella &amp; Berger</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data_reduction" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Principles of Data Reduction</h1>
<div id="chapter-notes-5" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Chapter Notes</h2>
<p>This chapter is all about simplifying the process of making inferences from a sample - i.e. restricting the kind of analyses we perform in some principled way. It’s really about <em>norms</em> of statistical analysis, with these norms justified by deduction from certain plausible-sounding assumptions.</p>
<p>We’ll come across the Sufficiency Principle, the Likelihood Principle, and the Equivariance Principle - all norms that restrict in a useful way the permissible inferences from a sample. All are justified by straightforward-seeming assumptions, but none are universally accepted among statisticians, and many common statistical procedures violate one or more of them.</p>
<p>The chapter begins by noting that a statistic is a kind of data reduction technique, in the sense that if you base decisions or inferences on the statistic, you will treat two samples equivalently so long as they produce the same value of the statistic. In this way, statistics partition the sample space.</p>
<div id="the-sufficiency-principle" class="section level3 unnumbered">
<h3>The Sufficiency Principle</h3>
<p>Data reduction of some kind is going to be necessary, but how can we know that by summarising the data we’ve collected in certain way, we’re not throwing away information about the parameter of interest <span class="math inline">\(\theta\)</span>? The chapter introduces the concept of a <em>sufficient statistic</em>:</p>
<p><strong>Definition 6.2.1 A Sufficient Statistic</strong></p>
<p>A statistic <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> if the conditional distribution of the sample <span class="math inline">\(\mathbf X\)</span> given <span class="math inline">\(T(\mathbf X)\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>A sufficient statistic contains all the information about <span class="math inline">\(\theta\)</span> that is present in the sample - once you know the sufficient statistic <span class="math inline">\(T(\mathbf X)\)</span>, you don’t get any more information about <span class="math inline">\(\theta\)</span> by also knowing the sample. This is the sufficiency principle:</p>
<p><strong>Sufficiency Principle:</strong>
If <span class="math inline">\(T(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>, then any inference about <span class="math inline">\(\theta\)</span> should depend on the sample <span class="math inline">\(\mathbf{X}\)</span> only through the value <span class="math inline">\(T(\mathbf{X})\)</span>. That is, if <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are two sample points such that <span class="math inline">\(T(\mathbf{x})=T(\mathbf{y})\)</span>, then the inference about <span class="math inline">\(\theta\)</span> should be the same whether <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span> or <span class="math inline">\(\mathbf{X}=\mathbf{y}\)</span> is observed.</p>
<p>For example, say you have iid random variables <span class="math inline">\(X_1, \dots, X_n\)</span> that are each Bernoulli distributed with unknown parameter <span class="math inline">\(\theta\)</span>. Then the statistic <span class="math inline">\(T( \mathbf X ) = X_ + \dots + X_n\)</span> (i.e. the sum of all the 1s in the sample) is a sufficient statistic for the parameter <span class="math inline">\(\theta\)</span>. Once you know the sum of the random variables, you don’t get any more information about <span class="math inline">\(\theta\)</span> by knowing, say, whether <span class="math inline">\(X_3\)</span> is a zero or a one.</p>
<p>Another example is the sample mean for a normal distribution. Say you have random variables <span class="math inline">\(X_1, \dots, X_n\)</span>, each iid <span class="math inline">\(\text{Normal}(\mu, \sigma^2)\)</span> variables. Then the sample mean <span class="math inline">\(T(\mathbf X) = \overline {\mathbf X} = (X_1 + \dots + X_n)/n\)</span> is a sufficient statistic for <span class="math inline">\(\mu\)</span>.</p>
<p>These examples are in some sense unusual - the chapter states that sufficient statistics that have a smaller dimension than the size of the sample are not common outside of distributions in the exponential family.</p>
<p>The factorisation theorem is useful for finding sufficient statistics by looking at the sample pdf/pmf:</p>
<p><strong>Theorem 6.2.6 Factorisation Theorem</strong></p>
<p>Let <span class="math inline">\(f(\mathbf x|\theta)\)</span> be the joint pdf/pmf of the sample <span class="math inline">\(\mathbf X\)</span>. A statistic <span class="math inline">\(T(\mathbf X)\)</span> is a sufficient statistic if and only if there exist functions <span class="math inline">\(g(t | \theta)\)</span> and <span class="math inline">\(h(\mathbf X)\)</span> such that for all sample points <span class="math inline">\(\mathbf x\)</span> and parameter points <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
f(\mathbf x | \theta) = g( T(\mathbf X)| \theta)h(\mathbf x)
\]</span></p>
<p>We factor the pdf/pmf into two parts, only one of which depends on <span class="math inline">\(\theta\)</span>. The part that depends on <span class="math inline">\(\theta\)</span> depends on the sample <span class="math inline">\(\text x\)</span> only through function <span class="math inline">\(T(\mathbf X)\)</span> and this function is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>Since we’re using sufficient statistics for data reduction, we want to know if some sufficient statistics do a better job of reducing the data than others. A <em>minimal sufficient statistic</em> is one that reduces the data as much as possible. Sufficient statistics <span class="math inline">\(T(\mathbf X)\)</span> is called a minimal sufficient statistic if, for any other sufficient statistic <span class="math inline">\(T&#39;(\mathbf X)\)</span>, <span class="math inline">\(T&#39;(\mathbf x)=T&#39;(\mathbf y) \implies T(\mathbf x) = T(\mathbf y)\)</span>. Looking back to the notion of a statistic as partitioning the sample space, we can say that a minimal statistic is the <em>coarsest possible</em> partition that retains all information about <span class="math inline">\(\theta\)</span>.</p>
<p>The chapter also introduces the concept of an <em>ancillary statistic</em>. An ancillary statistic for <span class="math inline">\(\theta\)</span> is one whose distribution does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>For example, if <span class="math inline">\(X_1, \dots, X_n\)</span> are iid uniform variables on the interval <span class="math inline">\((\theta, \theta + 1), \ -\infty &lt; \theta &lt; \infty\)</span> then the range statistic <span class="math inline">\(R = X_{(n)}-X_{(1)}\)</span> is an ancillary statistic.</p>
<p>Here <span class="math inline">\(X_{(1)}, \dots X_{(n)}\)</span> refer to the <em>order statistics</em> (e.g. <span class="math inline">\(X_{(1)} = \min(X_1, \dots, X_n)\)</span> and <span class="math inline">\(X_{(n)} = \max(X_1, \dots, X_n)\)</span>).</p>
<p>More generally, if <span class="math inline">\(\theta\)</span> is a a location parameter in any location parameter family distribution, the range statistic is ancillary.</p>
<p>It isn’t necessarily the case that minimally sufficient statistics for <span class="math inline">\(\theta\)</span> are independent of ancillary statistics for <span class="math inline">\(\theta\)</span>. The chaptetr gives this toy example:</p>
<p><strong>Example 6.2.20 Ancillary Precision</strong></p>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be iid observations from the discrete distribution that satisfies
<span class="math display">\[
P_\theta(X=\theta)=P_\theta(X=\theta+1)=P_\theta(X=\theta+2)=\frac{1}{3},
\]</span>
where <span class="math inline">\(\theta\)</span>, the unknown parameter, is any integer.</p>
<p>Then <span class="math inline">\((R, M)\)</span>, where <span class="math inline">\(R=X_{(2)}-X_{(1)}\)</span> and <span class="math inline">\(M=\left(X_{(1)}+X_{(2)}\right) / 2\)</span>, is a minimal sufficient statistic. Since this is a location parameter family, <span class="math inline">\(R\)</span> is an ancillary statistic.</p>
<p><span class="math inline">\(R\)</span> is ancillary, but can still give information about <span class="math inline">\(\theta\)</span> if we know a sample point <span class="math inline">\((r,m)\)</span>.</p>
<p>The chapter then introduces <em>complete statistics</em>. I had difficulty figuring out what was going on when I first saw the chapter definition (and I’m still not super confident!). Complete statistics are first described as useful for “a description of situations in which [a minimal sufficient statistic is independent of any ancillary statistic].”</p>
<p>Here’s the definition:</p>
<p><strong>Definition 6.2.21 Complete Statistics</strong></p>
<p>Let <span class="math inline">\(f(t | \theta)\)</span> be a family of pdfs or pmfs for a statistic <span class="math inline">\(T(\mathbf{X})\)</span>. The family of probability distributions is called complete if <span class="math inline">\(E_{\theta} g(T)=0\)</span> for all <span class="math inline">\(\theta\)</span> implies <span class="math inline">\(P_{\theta}(g(T)=0)=1\)</span> for all <span class="math inline">\(\theta\)</span>. Equivalently, <span class="math inline">\(T(\mathbf{X})\)</span> is called a complete statistic.</p>
<p>What’s going on here? You have some function <span class="math inline">\(g\)</span>, of a complete statistic <span class="math inline">\(T(\mathbf{X})\)</span>, and you specify that you want <span class="math inline">\(g(T)\)</span> to be an unbiased estimator of 0. Then <span class="math inline">\(g(T)\)</span> must equal 0 with probability 1 for all <span class="math inline">\(\theta\)</span> in your family of distributions. The only unbiased estimator of 0 is the 0 function.</p>
<p>I still don’t really understand the motivation for this definition. It ends up being useful (via Basu’s theorem) for determining that two statistics are independent without needing to examine their joint distribution. For example you can show that <span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(S^2\)</span> are independent when sampling from a <span class="math inline">\(\text{Normal}(\mu,\sigma^2)\)</span> population.</p>
<p>Here’s Basu’s theorem:</p>
<p><strong>Theorem 6.2.24 Basu’s Theorem</strong></p>
<p>If <span class="math inline">\(T(\mathbf{X})\)</span> is a complete and minimal sufficient statistic, then <span class="math inline">\(T(\mathbf{X})\)</span> is independent of every ancillary statistic.</p>
<p>Although the chapter later notes that including “minimal” here is slightly redundant, since it can be shown that if a minimal sufficient statistic exists then every complete statistic is also a minimal sufficient statistic.</p>
</div>
<div id="the-likelihood-principle" class="section level3 unnumbered">
<h3>The Likelihood Principle</h3>
<p><strong>Definition 6.3.1 The Likelihood Function</strong></p>
<p>Given a joint pdf/pmf of a sample <span class="math inline">\(f(\mathbf{x} | \theta)\)</span> and an observation <span class="math inline">\(\mathbf{X}=\mathbf{x}\)</span>, the function of <span class="math inline">\(\theta\)</span> defined by</p>
<p><span class="math display">\[
L(\theta \mid \mathbf{x})=f(\mathbf{x} | \theta)
\]</span>
is called the likelihood function.</p>
<p>The important difference between a likelihood and a pdf/pmf is that with a likelihood the data is considered fixed, and we vary the parameter <span class="math inline">\(\theta\)</span>.</p>
<p>The likelihood principle is a proposal for using the likelihood function to reduce/ summarise data.</p>
<p><strong>The Likelihood Principle:</strong>
If <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> are two sample points such that <span class="math inline">\(L(\theta | \mathbf x)\)</span> is proportional to <span class="math inline">\(L(\theta | \mathbf y)\)</span>, that is, there exists a constant <span class="math inline">\(C(\mathbf x, \mathbf y)\)</span> such that <span class="math inline">\(L(\theta | \mathbf x)=C(\mathbf x, \mathbf y) L(\theta | \mathbf y)\)</span> for all <span class="math inline">\(\theta\)</span>, then the conclusions drawn from <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span> should be identical.</p>
<p>Notice that <span class="math inline">\(C(\mathbf x, \mathbf y)\)</span> does not depend on <span class="math inline">\(\theta\)</span>, but it can be different for different values of <span class="math inline">\(\mathbf x\)</span> and <span class="math inline">\(\mathbf y\)</span>.</p>
<p>The idea here is that the likelihood is going to be used to compare the plausibility of different <span class="math inline">\(\theta\)</span> values, and so if you have two sample points that produce likelihood functions in a fixed ratio with each other for all <span class="math inline">\(\theta\)</span>, the relative plausibility of each <span class="math inline">\(\theta\)</span> value should be the same for both. I.e. the same inferences should be drawn about <span class="math inline">\(\theta\)</span>, no matter if you observed <span class="math inline">\(\mathbf x\)</span> or <span class="math inline">\(\mathbf y\)</span>.</p>
<p>Section 6.3.2 of the chapter derives the likelihood principles from two simpler principles. We start with the concept of an experiment.</p>
<p>The chapter defines an experiment <span class="math inline">\(E\)</span> as a triple <span class="math inline">\((\mathbf X, \theta, f(\mathbf X, \theta))\)</span>. Here, <span class="math inline">\(\mathbf X\)</span> is a random vector with pmf <span class="math inline">\(f(\mathbf X, \theta)\)</span> for some <span class="math inline">\(\theta\)</span> in parameter space <span class="math inline">\(\Theta\)</span>. An experimenter will observe some particular sample <span class="math inline">\(\mathbf x\)</span> and draw some conclusion about <span class="math inline">\(\theta\)</span>. We denote this conclusion <span class="math inline">\(\text{Ev}(E, \mathbf x)\)</span> - the <em>evidence about <span class="math inline">\(\theta\)</span> arising from <span class="math inline">\(E\)</span> and <span class="math inline">\(\mathbf x\)</span></em>.</p>
<p>We now introduce the two principles we’ll base the formal likelihood principle on. The first is familiar:</p>
<p><strong>The Formal Sufficiency Principle:</strong>
Consider experiment <span class="math inline">\(E=(\mathbf X, \theta, f(\mathbf X, \theta))\)</span> and sufficient statistic for <span class="math inline">\(\theta\)</span> <span class="math inline">\(T(\mathbf{X})\)</span>.If <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are two sample points such that <span class="math inline">\(T(\mathbf{x})=T(\mathbf{y})\)</span>, then <span class="math inline">\(\text{Ev}(E, \mathbf x)\)</span> = <span class="math inline">\(\text{Ev}(E, \mathbf y)\)</span>.</p>
<p>By accepting the formal sufficiency principle, we are agreeing to equate evidence if the sufficient statistics match.</p>
<p>The other principle is the following:</p>
<p><strong>Conditionality Principle:</strong>
Suppose that <span class="math inline">\(E_1=(\mathbf X_1, \theta, f_1(\mathbf X_1, \theta))\)</span> and <span class="math inline">\(E_1=(\mathbf X_2, \theta, f_2(\mathbf X_2, \theta))\)</span> are two experiments, where only the unknown parameter <span class="math inline">\(\theta\)</span> need be common between the two experiments.
Consider the mixed experiment in which the random variable <span class="math inline">\(J\)</span> is observed, where <span class="math inline">\(P(J=1)=P(J=2)=\frac{1}{2}\)</span> (independent of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\mathbf X_1\)</span>, or <span class="math inline">\(\mathbf X_2\)</span>), and then experiment <span class="math inline">\(E_J\)</span> is performed.
Formally, the experiment performed is <span class="math inline">\(E^*=(\mathbf X^*, \theta, f^*(\mathbf X^*, \theta))\)</span>, where <span class="math inline">\(\mathbf X^*=(j, \mathbf X_j)\)</span> and <span class="math inline">\(f^*(\mathbf x^* | \theta)=f^*((j, \mathbf x_j) | \theta)=\frac{1}{2} f_j(\mathbf x_j | \theta)\)</span>. Then
<span class="math display">\[
\text{Ev}(E^*,(j, \mathbf x_j))=\text{Ev}(E_j, \mathbf x_j)
\]</span></p>
<p>What is this saying? If we flip a coin to decide between two experiments, and so end up performing experiment 1 (say) and observing data <span class="math inline">\(\mathbf x\)</span>, the conclusions that we draw about <span class="math inline">\(\theta\)</span> should be the same as if we non-randomly chose to perform experiment 1 (and observed <span class="math inline">\(\mathbf x\)</span>) in the first place.</p>
<p>The formal likelihood principle can be derived from these two principles.</p>
<p><strong>The Formal Likelihood Principle</strong></p>
<p>Suppose we have experiments <span class="math inline">\(E_1=(\mathbf X_1, \theta, f_1(\mathbf X_1, \theta))\)</span> and <span class="math inline">\(E_1=(\mathbf X_2, \theta, f_2(\mathbf X_2, \theta))\)</span>, and sample points <span class="math inline">\(\mathbf x_1^*\)</span> and <span class="math inline">\(\mathbf x_2^*\)</span> from each experiment respectively. If</p>
<p><span class="math display">\[
L(\theta | x_2^*) = CL(\theta | x_1^*)
\]</span>
for all <span class="math inline">\(\theta\)</span> and constant <span class="math inline">\(C(\mathbf x_1^*, \mathbf x_2^*)\)</span>, then</p>
<p><span class="math display">\[
\text{Ev}(E_1, \mathbf x_1^*) = \text{Ev}(E_2, \mathbf x_2^*)
\]</span></p>
<p>The main difference in this statement compared to our first statement of the likelihood principle at the start of the section is that this one concerns two experiments instead of only one.</p>
<p>The corollary is that the evidence should depend on <span class="math inline">\(E\)</span> and <span class="math inline">\(x\)</span> only through the likelihood <span class="math inline">\(L(\theta | \mathbf x)\)</span>. If the likelihood is the same (or the ratio of likelihoods is the same), the conclusion is the same, no matter the experiment or the sample.</p>
<p>Birnbaum’s Theorem states that the Formal Likelihood Principle follows from the Sufficiency Principle and the Conditionality Principle, and that the converse also holds.</p>
<p>The rest of the section is an interesting discussion about how much trust we should put in to the likelihood principle. Both the sufficiency principle and the conditionality principle seem so straightforward as to be immediately obvious, but the likelihood principle that follows from them is violated in many common statistical procedures. For example, model checking by examining residuals violates the sufficiency principle, because residuals are not a sufficient statistic. Does this mean that residual checking is misguided? Not necessarily, the chapter states that one drawback of the sufficiency principle is that it is very model-dependent - e.g. if you believe in your normally distributed model, then it’s true that <span class="math inline">\(\overline X\)</span> and <span class="math inline">\(S^2\)</span> are sufficient statistics. However you may have good reason to hold on to your model a little more tentatively than that.</p>
<p>The discussion is interesting, and has plenty of links to further reading on the topic of these principles and their violation.</p>
</div>
<div id="the-equivariance-principle" class="section level3 unnumbered">
<h3>The Equivariance Principle</h3>
<p>From the chapter:</p>
<blockquote>
<p>The Equivariance Principle describes a data reduction technique in a slightly different way [to the principles above]. In any application of the Equivariance Principle, a function <span class="math inline">\(T(\mathbf X)\)</span> is specified, but if <span class="math inline">\(T(\mathbf X)=T(\mathbf Y)\)</span>, then the Equivariance Principle states that the inference made if <span class="math inline">\(\mathbf x\)</span> is observed should have a <em>certain relationship</em> to the inference made if <span class="math inline">\(\mathbf y\)</span> is observed, although the two inferences may not be the same.</p>
</blockquote>
<p>We have two types of equivariance:</p>
<ul>
<li>Measurement equivariance - you should draw the same conclusions whether you measure in inches or metres</li>
<li>Formal invariance - If two inference problems have the same formal structure in terms of the mathematical model used (the same parameter space <span class="math inline">\(\Theta\)</span>, the same set of sample pdfs/pmfs <span class="math inline">\(\{f(\mathbf x | \theta) : \theta \in \Theta\}\)</span>, the same allowable inferences and consequences of wrong inferences), then the same inference procedure should be used in both problems.</li>
</ul>
<p>The equivariance principle is then:</p>
<p><strong>The Equivariance Principle</strong></p>
<p>If <span class="math inline">\(\mathbf Y = g(\mathbf X)\)</span> is a change of measurement scale such that the model for <span class="math inline">\(\mathbf Y\)</span> has the same formal structure as the model for <span class="math inline">\(\mathbf X\)</span>, then an inference procedure should be both measurement equivariant and formally equivariant.</p>
<p>For example, if you’re observing a binomial process, whether you’re attempting to estimate <span class="math inline">\(p\)</span> or <span class="math inline">\(q=1-p\)</span> your problem has the same formal structure, with just a different measurement scale. If <span class="math inline">\(T(x)\)</span> is an estimate of <span class="math inline">\(p\)</span> using the number of observed successes <span class="math inline">\(x\)</span> and <span class="math inline">\(T^*(y)\)</span> is the estimate of <span class="math inline">\(q\)</span> using the number of observed failures <span class="math inline">\(y = n- x\)</span>, the equivariance principle demands that <span class="math inline">\(T(z) = T^*(z)\)</span> for all <span class="math inline">\(z = 0, \dots, n\)</span>. So we require</p>
<p><span class="math display">\[
T(x) = 1- T^*(n - x) = 1 - T(n - x).
\]</span></p>
<p>This requirement restricts the space of estimators <span class="math inline">\(T\)</span> that we’ll consider using.</p>
<p>The principle of measurement equivariance is fairly uncontroversial, but the requirement of formal invariance means that for any two problems with the same mathematical structure (even if the physical realities are very different) the same inference approach is appropriate. This is a stronger assumption.</p>
</div>
</div>
<div id="questions-5" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Questions</h2>
</div>
<div id="further-reading-5" class="section level2 unnumbered">
<h2>Further Reading</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sample_properties.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="point_estimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-data_reduction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Point Estimation | Notes on ‘Statistical Inference’ by Casella &amp; Berger</title>
  <meta name="description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Point Estimation | Notes on ‘Statistical Inference’ by Casella &amp; Berger" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  <meta name="github-repo" content="jake-lawler/StatisticalInference" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Point Estimation | Notes on ‘Statistical Inference’ by Casella &amp; Berger" />
  
  <meta name="twitter:description" content="My notes on ‘Statistical Inference’ by George Casella and Roger Berger." />
  

<meta name="author" content="Jake Lawler" />


<meta name="date" content="2022-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data_reduction.html"/>

<script src="libs/header-attrs-2.13/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="probability_theory.html"><a href="probability_theory.html"><i class="fa fa-check"></i><b>1</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability_theory.html"><a href="probability_theory.html#chapter-notes"><i class="fa fa-check"></i><b>1.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#counting"><i class="fa fa-check"></i>Counting</a></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#inequalities-bonferronis-and-booles"><i class="fa fa-check"></i>Inequalities: Bonferroni’s and Boole’s</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability_theory.html"><a href="probability_theory.html#questions"><i class="fa fa-check"></i><b>1.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#ex-1.42"><i class="fa fa-check"></i>Ex 1.42</a></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#ex-1.43"><i class="fa fa-check"></i>Ex 1.43</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="probability_theory.html"><a href="probability_theory.html#further-reading"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>2</b> Transformations and Expectations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="transformations.html"><a href="transformations.html#chapter-notes-1"><i class="fa fa-check"></i><b>2.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#monotonic-transformations"><i class="fa fa-check"></i>Monotonic Transformations</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#generating-random-samples"><i class="fa fa-check"></i>Generating Random Samples</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#moment-generating-functions"><i class="fa fa-check"></i>Moment Generating Functions</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#differentiating-under-an-integral-sign"><i class="fa fa-check"></i>Differentiating Under an Integral Sign</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="transformations.html"><a href="transformations.html#questions-1"><i class="fa fa-check"></i><b>2.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.1"><i class="fa fa-check"></i>Ex 2.1</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.3"><i class="fa fa-check"></i>Ex 2.3</a></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#ex-2.6"><i class="fa fa-check"></i>Ex 2.6</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="transformations.html"><a href="transformations.html#further-reading-1"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="families.html"><a href="families.html"><i class="fa fa-check"></i><b>3</b> Common Families of Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="families.html"><a href="families.html#chapter-notes-2"><i class="fa fa-check"></i><b>3.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="families.html"><a href="families.html#discrete-distributions"><i class="fa fa-check"></i>Discrete Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="families.html"><a href="families.html#questions-2"><i class="fa fa-check"></i><b>3.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="families.html"><a href="families.html#further-reading-2"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple_variables.html"><a href="multiple_variables.html"><i class="fa fa-check"></i><b>4</b> Multiples Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple_variables.html"><a href="multiple_variables.html#chapter-notes-3"><i class="fa fa-check"></i><b>4.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#independence"><i class="fa fa-check"></i>Independence</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#bivariate-transformations"><i class="fa fa-check"></i>Bivariate Transformations</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#hierarchical-models-and-mixture-distributions"><i class="fa fa-check"></i>Hierarchical Models and Mixture Distributions</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#covariance-and-correlation"><i class="fa fa-check"></i>Covariance and Correlation</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#multivariate-distribution"><i class="fa fa-check"></i>Multivariate Distribution</a></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#inequalities"><i class="fa fa-check"></i>Inequalities</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple_variables.html"><a href="multiple_variables.html#questions-3"><i class="fa fa-check"></i><b>4.2</b> Questions</a>
<ul>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#ex-4.39"><i class="fa fa-check"></i>Ex 4.39</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple_variables.html"><a href="multiple_variables.html#further-reading-3"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sample_properties.html"><a href="sample_properties.html"><i class="fa fa-check"></i><b>5</b> Properties of a Random Sample</a>
<ul>
<li class="chapter" data-level="5.1" data-path="sample_properties.html"><a href="sample_properties.html#chapter-notes-4"><i class="fa fa-check"></i><b>5.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#sums-of-random-variables-from-a-random-sample"><i class="fa fa-check"></i>Sums of Random Variables from a Random Sample</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#sums-from-the-normal-distribution"><i class="fa fa-check"></i>Sums from the Normal Distribution</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#convergence-concepts"><i class="fa fa-check"></i>Convergence Concepts</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#proving-tools"><i class="fa fa-check"></i>Proving Tools</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#generating-a-random-sample"><i class="fa fa-check"></i>Generating a Random Sample</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sample_properties.html"><a href="sample_properties.html#questions-4"><i class="fa fa-check"></i><b>5.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="sample_properties.html"><a href="sample_properties.html#further-reading-4"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data_reduction.html"><a href="data_reduction.html"><i class="fa fa-check"></i><b>6</b> Principles of Data Reduction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data_reduction.html"><a href="data_reduction.html#chapter-notes-5"><i class="fa fa-check"></i><b>6.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-sufficiency-principle"><i class="fa fa-check"></i>The Sufficiency Principle</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-likelihood-principle"><i class="fa fa-check"></i>The Likelihood Principle</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#the-equivariance-principle"><i class="fa fa-check"></i>The Equivariance Principle</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data_reduction.html"><a href="data_reduction.html#questions-5"><i class="fa fa-check"></i><b>6.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="data_reduction.html"><a href="data_reduction.html#further-reading-5"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="point_estimation.html"><a href="point_estimation.html"><i class="fa fa-check"></i><b>7</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="7.1" data-path="point_estimation.html"><a href="point_estimation.html#chapter-notes-6"><i class="fa fa-check"></i><b>7.1</b> Chapter Notes</a>
<ul>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#method-of-moments"><i class="fa fa-check"></i>Method of Moments</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i>Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#bayes-estimators"><i class="fa fa-check"></i>Bayes Estimators</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#mean-squared-error"><i class="fa fa-check"></i>Mean Squared Error</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#best-unbiased-estimators"><i class="fa fa-check"></i>Best Unbiased Estimators</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#sufficiency-and-unbiasedness"><i class="fa fa-check"></i>Sufficiency and Unbiasedness</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#loss-function-optimality"><i class="fa fa-check"></i>Loss Function Optimality</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="point_estimation.html"><a href="point_estimation.html#questions-6"><i class="fa fa-check"></i><b>7.2</b> Questions</a></li>
<li class="chapter" data-level="" data-path="point_estimation.html"><a href="point_estimation.html#further-reading-6"><i class="fa fa-check"></i>Further Reading</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on ‘Statistical Inference’ by Casella &amp; Berger</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="point_estimation" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Point Estimation</h1>
<div id="chapter-notes-6" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Chapter Notes</h2>
<p>This chapter is all about techniques for estimating point values of some parameter <span class="math inline">\(\theta\)</span>. Our estimate is calculated from our sample, but we take <span class="math inline">\(\theta\)</span> to be some property of the larger population that is the object of our inference. The chapter first introduces ways to derive estimators (functions of the sample that produce estimates), and then introduces methods of evaluting these estimators.</p>
<p>The definition of point estimator in the chapter is very broad - any statistic can be a point estimator, but obviously not all statistics will be reasonable estimators for any particular parameter. How do we find reasonable estimators? I’ll make notes on three of the four methods described in the chapter:</p>
<ul>
<li>Method of Moments</li>
<li>Maximum Likelihoods</li>
<li>Bayesian Estimators</li>
</ul>
<div id="method-of-moments" class="section level3 unnumbered">
<h3>Method of Moments</h3>
<p>The idea here is that you draw a sample and then equate the first <span class="math inline">\(k\)</span> sample moments to the first <span class="math inline">\(k\)</span> population moments, and then solve for the paramters of interest. Here are the definition of sample moments <span class="math inline">\(m_i\)</span> and population moments <span class="math inline">\(\mu&#39;_i\)</span> (see chapter 2):</p>
<p><span class="math display">\[
\begin{aligned}
m_1 &amp;= \frac{1}{n} \sum_{i=1}^n X_i, &amp;&amp; \mu&#39;_1 = EX \\
m_2 &amp;= \frac{1}{n} \sum_{i=1}^n X_i^2, &amp;&amp; \mu&#39;_2 = EX^2 \\
&amp;\vdots\\
m_k &amp;= \frac{1}{n} \sum_{i=1}^n X_i^k, &amp;&amp; \mu&#39;_k = EX^k \\
\end{aligned}
\]</span></p>
<p>Equating them gives you a system of simultaneous equations:</p>
<p><span class="math display">\[
\begin{aligned}
m_1 &amp;= \mu&#39;_1(\theta_1, \dots, \theta_k)\\
m_2 &amp;= \mu&#39;_2(\theta_1, \dots, \theta_k)\\
&amp;\vdots\\
m_k &amp;= \mu&#39;_k(\theta_1, \dots, \theta_k)\\
\end{aligned}
\]</span></p>
<p>For example, suppose you have a sample of iid random variables <span class="math inline">\(X_1, \dots, X_n\)</span> that are <span class="math inline">\(\text{Normal}(\mu. \sigma^2)\)</span> distributed. We’re looking to produces estimates for parameters <span class="math inline">\(\theta_1 = \mu\)</span> and <span class="math inline">\(\theta_2 = \sigma^2\)</span>.</p>
<p>Our sample moments are <span class="math inline">\(m_1 = \overline{X}\)</span> and <span class="math inline">\(m_2 = \frac{1}{n} \sum X_i^2\)</span> and our population moments are <span class="math inline">\(\mu_1&#39; = \mu\)</span> and <span class="math inline">\(\mu_2&#39; =\mu^2 + \sigma^2\)</span>. Equating them gives method of moments estimates:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde\mu &amp;= \overline{X}\\
\tilde\sigma^2 &amp;= \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline X = \frac{1}{n} \sum_{i=1}^n(X_i - \overline X)^2\\
\end{aligned}
\]</span>
These seem pretty plausible! Although not that our method of moments estimator for the variance is not the unbiased estimator <span class="math inline">\(S^2\)</span>.</p>
<p>The next example in the chapter concerns a sample <span class="math inline">\(X_1, \dots, X_n\)</span> that is iid <span class="math inline">\(\text{Binomial}(k,p)\)</span>. For the purposes of this example, we want to treat <em>both</em> <span class="math inline">\(p\)</span> and <span class="math inline">\(k\)</span> as unknown. Equating moments gets us to:</p>
$$
<span class="math display">\[\begin{aligned}
\overline X &amp;= kp\\
\frac{1}{n}\sum X_i^2 &amp;= kp(1-p) + k^2p^2\\

\end{aligned}\]</span>
<p>$$</p>
<p>Since for a binomially distributed random variable <span class="math inline">\(X\)</span> we have <span class="math inline">\(E(X) = kp\)</span> and <span class="math inline">\(E(X^2) = \text{Var}(X) + E(X)^2 = kp(1-p) + k^2p^2\)</span>.</p>
<p>Solving for <span class="math inline">\(p\)</span> and <span class="math inline">\(k\)</span> with a little straightforward algebra gets us to:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde p &amp;= \frac{\overline X}{\tilde k} \\
\tilde k &amp;=\frac{\overline X^2}{\overline X - \frac{1}{n} \sum (X_{i}-\overline X )^2}
\end{aligned}
\]</span></p>
<p>Looking at the denominator of our estimate of <span class="math inline">\(k\)</span>, you can see that it will become negative if the sample mean is smaller than the sample variance, but we know that <span class="math inline">\(k\)</span> must be positive. Here the range of our method of moments estimate does not coincide with the range of the parameter to be estimated. The chapter does state that these are not the best estimators of the population parameters. But we have gained something - the estimate for <span class="math inline">\(p\)</span> may be intuitively obvious but we probably wouldn’t have been able to come up with that estimate of <span class="math inline">\(k\)</span> without the method of moments.</p>
</div>
<div id="maximum-likelihood-estimators" class="section level3 unnumbered">
<h3>Maximum Likelihood Estimators</h3>
<p>Our next technique for finding estimators is the method of maximum likelihood. The likelihood function was introduced in the previous chapter. For an iid sample with pdf/pmf <span class="math inline">\(f(x_i | \theta_1 \dots, \theta_k)\)</span> the likelihood function is:</p>
<p><span class="math display">\[
L(\mathbf \theta | \mathbf x) = L(\theta_1, \dots, \theta_n | x_1, \dots, x_m) = \prod_{i=1}^n f(x_i | \theta_1, \dots, \theta_k)
\]</span></p>
<p>The maximum likelihood parameter value <span class="math inline">\(\hat \theta(\mathbf x)\)</span> is the parameter value for which the likelihood function attains it’s maximum value (considered as a function of <span class="math inline">\(\theta\)</span>) for a given realised sample <span class="math inline">\(\mathbf x\)</span>.</p>
<p>Here, unlike the method of moments, the range of our parameter estimate will always coincide with the range of the parameter. There are two problems with implementing and understanding the properties of MLEs:</p>
<ul>
<li>Finding the global maximum and verifying that it is a maximum - often simple differentiation but other approaches are possible. The chapter includes an example of finding the MLE for the mean of a normal distribution by finding a global upper bound for the likelihood and showing that this bound is only obtained at <span class="math inline">\(\theta = \overline X\)</span>.</li>
<li>Numerical sensitivity - how sensitive is our MLE to small changes in the sample?</li>
</ul>
<p>If we’re going to use differentiation to find an MLE, it’s often easier to work with log likelihoods instead of likelihoods directly. This works out since the log function is strictly increasing and so the extrema coincide. For example, consider the Bernoulli likelihood</p>
<p><span class="math display">\[
L(p | \mathbf x) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^y(1-p)^{n-y}
\]</span></p>
<p>for <span class="math inline">\(y = \sum x_i\)</span>. This gets easier to differentiate after taking logs:</p>
<p><span class="math display">\[
\log L(p | \mathbf x) = y \log p + (n-y) \log (1-p).
\]</span></p>
<p>Differentiating and setting to 0 gives:</p>
<p><span class="math display">\[
0 = \frac{y}{p} - \frac{n-y}{1-p}
\]</span></p>
<p>And some algebra returns the possible solution <span class="math inline">\(p = \frac{y}{n}\)</span>, which can be verified as a maximum.</p>
<p>The invariance property of MLEs states that if <span class="math inline">\(\hat \theta\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\tau(\hat \theta)\)</span> is the MLE for <span class="math inline">\(\tau( \theta)\)</span> for any function <span class="math inline">\(\tau\)</span>. E.g. if <span class="math inline">\(\theta\)</span> is the mean of a normal distribution then the maximum likelihood estimate of <span class="math inline">\(\sin(\theta)\)</span> is <span class="math inline">\(\sin(\overline X)\)</span>.</p>
<p>If <span class="math inline">\(\tau\)</span> is one-to-one then it’s easy to see that maximising the likelihood as a function of <span class="math inline">\(\theta\)</span> or maxmimising it as a function of <span class="math inline">\(\tau(\theta)\)</span> makes no difference, but the result holds for any function <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>There’s a discussion at the end of the section about numerical instability of MLEs. This occurs when the likelihood function is quite flat near its maximum or there is no finite maximum. For example the MLEs for <span class="math inline">\(k\)</span> and <span class="math inline">\(p\)</span> in binomial sampling can be quite unstable. The chapter says that this is usually not a problem when finding MLEs explicitly but if you’re using a numerical approach it is worth investigating the stability around your estimate.</p>
</div>
<div id="bayes-estimators" class="section level3 unnumbered">
<h3>Bayes Estimators</h3>
<p>Revisit.</p>
</div>
<div id="mean-squared-error" class="section level3 unnumbered">
<h3>Mean Squared Error</h3>
<p>The rest of the chapter is concerned with methods of evaluating estimators. The firs method discussed is mean squared error.</p>
<p><strong>Definition 7.3.1 Mean Squared Error</strong></p>
<p>The <em>mean squared error (MSE)</em> of an estimator <span class="math inline">\(W\)</span> of a parameter <span class="math inline">\(\theta\)</span> is the function of <span class="math inline">\(\theta\)</span> defined by <span class="math inline">\(E_\theta(W-\theta)^2\)</span>.</p>
<p>It measures the average squared difference between the estimator <span class="math inline">\(W\)</span> and the parameter <span class="math inline">\(\theta\)</span>. Any increasing function of the absolute difference could be reasonable, but MSE is quite tractable analystically and has the useful interpretation:</p>
<p><span class="math display">\[
E_\theta(W-\theta)^2 = \text{Var}_\theta(W) + (E_\theta W - \theta)^2 = \text{Var}_\theta(W)+(\text{Bias}_\theta W)^2
\]</span></p>
<p>where <span class="math inline">\(\text{Bias}_\theta W = E_\theta W - \theta\)</span>.</p>
<p>So a small MSE implies a small combined bias and variance. If an estimator is unbiased, its MSE is equal to its variance. So for example a Normal MSE would have:</p>
<p><span class="math display">\[
\begin{aligned}
E(\overline X - \mu)^2 &amp;= \text{Var}(\overline X) = \frac{\sigma^2}{n} \\
E(S^2 - \sigma^2)^2 &amp;= \text{Var}(S^2) = \frac{2\sigma^4}{n-1}
\end{aligned}
\]</span></p>
<p>However controlling bias does not guarantee a controlled MSE, and in fact the chapter shows that the biased estimator <span class="math inline">\(\hat\sigma^2\)</span> of <span class="math inline">\(\sigma^2\)</span> has a smaller MSE than <span class="math inline">\(S^2\)</span>. The chapter stresses that this does not mean that <span class="math inline">\(\hat\sigma^2\)</span> should necessarily be preferred as an estimator over <span class="math inline">\(S^2\)</span> - it will, for example, systemically underestimate the variance.</p>
<p>Sometimes the MSEs of two estimators will cross each other over the parameter range - one will have lower MSE in some portion of the parameter space, and higher MSE in some other portion.</p>
</div>
<div id="best-unbiased-estimators" class="section level3 unnumbered">
<h3>Best Unbiased Estimators</h3>
<p>The problem of finding a best estimator is difficult in part because of the huge class of candidates. One way of restricting the class of estimators is to consider only the unbiased ones. A best unbiased estimator <span class="math inline">\(W*\)</span> (if it exists) is one with uniformly smallest variance. I.e. the estimator <span class="math inline">\(W*\)</span> is unbiased and for any other unbiased estimator <span class="math inline">\(W\)</span> the variance of <span class="math inline">\(W*\)</span> is less than or equal to the variance of <span class="math inline">\(W\)</span> for all <span class="math inline">\(\theta\)</span>.</p>
<p>One way of finding a best unbiased estimator is to specify a lower bound <span class="math inline">\(B(\theta)\)</span> on the variance and then show that the estimator attains it. One example of this approach is the Cramér-Rao lower bound.</p>
<p><strong>Theorem 7.3.8 Cramér-Rao Inequality</strong></p>
<p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a sample with pdf <span class="math inline">\(f(\mathbf x | \theta)\)</span>, and let <span class="math inline">\(W(\mathbf X) = W(X_1, \dots, X_n)\)</span> be any estimator satisfying</p>
<p><span class="math display">\[
\frac{d}{d\theta} E_\theta W(\mathbf X) = \int_{\mathcal X} \frac{\partial}{\partial \theta}[W(\mathbf x)f(\mathbf x| \theta)] d\mathbf x
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\text{Var}_\theta W(\mathbf X) &lt; \infty.
\]</span>
Then</p>
<p><span class="math display">\[
\text{Var}_\theta(W(\mathbf X)) \geq \frac{\left(\frac{d}{d\theta} E_\theta W(\mathbf X)\right)^2}{E_\theta \left( \left(\frac{\partial}{\partial \theta}\log f(\mathbf X | \theta)\right)^2\right)}
\]</span></p>
<p>Which gives a lower bound of the variance of an estimator, though there’s no guarantee than any unbiased estimator will attain this lower bound. Here are the conditions for attainment of the Cramér-Rao lower bound.</p>
<p><strong>Corollary 7.3.15 Attainment</strong>
Let <span class="math inline">\(X_1, \dots, X_n\)</span> be iid <span class="math inline">\(f(x | \theta)\)</span>, where <span class="math inline">\(f(x | \theta)\)</span> satisfies the conditions of the Cramér-Rao Theorem. Let <span class="math inline">\(L(\theta | \mathbf{x})=\prod_{i=1}^n f\left(x_{i} | \theta\right)\)</span> denote the likelihood function. If <span class="math inline">\(W(\mathbf{X})=W\left(X_{1}, \dots, X_{n}\right)\)</span> is any unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, then <span class="math inline">\(W(\mathbf{X})\)</span> attains the Cramér-Rao Lower Bound if and only if
<span class="math display">\[
a(\theta)[W(\mathbf{x})-\tau(\theta)]=\frac{\partial}{\partial \theta} \log L(\theta \mid \mathbf{x})
\]</span>
for some function <span class="math inline">\(a(\theta)\)</span>.</p>
</div>
<div id="sufficiency-and-unbiasedness" class="section level3 unnumbered">
<h3>Sufficiency and Unbiasedness</h3>
<p><strong>Theorem 7.3.17 Rao-Blackwell</strong>
Let <span class="math inline">\(W\)</span> be any unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, and let <span class="math inline">\(T\)</span> be a sufficient statistic for <span class="math inline">\(\theta\)</span>. Define <span class="math inline">\(\phi(T)= E(W \mid T)\)</span>. Then <span class="math inline">\(E_{\theta} \phi(T)=\tau(\theta)\)</span> and <span class="math inline">\(\text{Var}_{\theta} \phi(T) \leq \text{Var}_{\theta} W\)</span> for all <span class="math inline">\(\theta\)</span>; that is, <span class="math inline">\(\phi(T)\)</span> is a uniformly better unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>Conditioning any unbiased estimator on a sufficient statistic will result in a uniform improvement - when we’re looking for best unbiased estimators we only need to consider statistics that are functions of a sufficient statistic.</p>
<p>Need to revisit this section for the relationship between completeness and best unbiased estimators.</p>
</div>
<div id="loss-function-optimality" class="section level3 unnumbered">
<h3>Loss Function Optimality</h3>
<p>We can generalise the MSE with the concept of a loss function - any non-negative function that generally increases with the distance between an action/ decision <span class="math inline">\(a\)</span> and parameter. E.g.</p>
<p>Absolute error loss:</p>
<p><span class="math display">\[
L(\theta, a) = |a-\theta|
\]</span></p>
<p>Squared error loss:</p>
<p><span class="math display">\[
L(\theta, a) = (a-\theta)^2
\]</span></p>
<p>If circumstances dictate, we may want to penalise overestimates more than overestimates, which can be done with something like this loss function:</p>
<p><span class="math display">\[
L(\theta, a)= \begin{cases}
(a-\theta)^{2} &amp; \text { if } a&lt;\theta \\
10(a-\theta)^{2} &amp; \text { if } a \geq \theta\end{cases}
\]</span></p>
<p>Evaluating estimators with loss functions is a branch of <em>decision theory</em>. With a decision-theoretic appraoch we’ll evaluate estimators <span class="math inline">\(\delta(\mathbf x)\)</span> of <span class="math inline">\(\theta\)</span> using the risk function:</p>
<p><span class="math display">\[
R(\theta, \delta) = E_\theta L(\theta , \delta(\mathbf X))
\]</span>
I.e. the average loss that we’ll experience by using the estimator. We’d like the risk function to be small for all possible values of <span class="math inline">\(\theta\)</span>. Like we saw with MSEs above, often risk functions of two candidate estimators will cross in parameter space. In fact the risk function of the squared error loss is the MSE.</p>
</div>
</div>
<div id="questions-6" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Questions</h2>
</div>
<div id="further-reading-6" class="section level2 unnumbered">
<h2>Further Reading</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data_reduction.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-point_estimation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

# Point Estimation {#point_estimation}

## Chapter Notes

This chapter is all about techniques for estimating point values of some parameter $\theta$. Our estimate is calculated from our sample, but we take $\theta$ to be some property of the larger population that is the object of our inference. The chapter first introduces ways to derive estimators (functions of the sample that produce estimates), and then introduces methods of evaluting these estimators.

The definition of point estimator in the chapter is very broad - any statistic can be a point estimator, but obviously not all statistics will be reasonable estimators for any particular parameter. How do we find reasonable estimators? I'll make notes on three of the four methods described in the chapter:

* Method of Moments
* Maximum Likelihoods
* Bayesian Estimators


### Method of Moments {-}

The idea here is that you draw a sample and then equate the first $k$ sample moments to the first $k$ population moments, and then solve for the paramters of interest. Here are the definition of sample moments $m_i$ and population moments $\mu'_i$ (see chapter 2):

$$
\begin{aligned}
m_1 &= \frac{1}{n} \sum_{i=1}^n X_i, && \mu'_1 = EX \\
m_2 &= \frac{1}{n} \sum_{i=1}^n X_i^2, && \mu'_2 = EX^2 \\
&\vdots\\
m_k &= \frac{1}{n} \sum_{i=1}^n X_i^k, && \mu'_k = EX^k \\
\end{aligned}
$$

Equating them gives you a system of simultaneous equations:

$$
\begin{aligned}
m_1 &= \mu'_1(\theta_1, \dots, \theta_k)\\
m_2 &= \mu'_2(\theta_1, \dots, \theta_k)\\
&\vdots\\
m_k &= \mu'_k(\theta_1, \dots, \theta_k)\\
\end{aligned}
$$

For example, suppose you have a sample of iid random variables $X_1, \dots, X_n$ that are $\text{Normal}(\mu. \sigma^2)$ distributed. We're looking to produces estimates for parameters $\theta_1 = \mu$ and $\theta_2 = \sigma^2$.

Our sample moments are $m_1 = \overline{X}$ and $m_2 = \frac{1}{n} \sum X_i^2$ and our population moments are $\mu_1' = \mu$ and $\mu_2' =\mu^2 + \sigma^2$. Equating them gives method of moments estimates:

$$
\begin{aligned}
\tilde\mu &= \overline{X}\\
\tilde\sigma^2 &= \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline X = \frac{1}{n} \sum_{i=1}^n(X_i - \overline X)^2\\
\end{aligned}
$$
These seem pretty plausible! Although not that our method of moments estimator for the variance is not the unbiased estimator $S^2$.

The next example in the chapter concerns a sample $X_1, \dots, X_n$ that is iid $\text{Binomial}(k,p)$. For the purposes of this example, we want to treat *both* $p$ and $k$ as unknown. Equating moments gets us to:

$$
\begin{aligned}
\overline X &= kp\\
\frac{1}{n}\sum X_i^2 &= kp(1-p) + k^2p^2\\

\end{aligned}
$$

Since for a binomially distributed random variable $X$ we have $E(X) = kp$ and $E(X^2) = \text{Var}(X) + E(X)^2 = kp(1-p) + k^2p^2$. 

Solving for $p$ and $k$ with a little straightforward algebra gets us to:

$$
\begin{aligned}
\tilde p &= \frac{\overline X}{\tilde k} \\
\tilde k &=\frac{\overline X^2}{\overline X - \frac{1}{n} \sum (X_{i}-\overline X )^2}
\end{aligned}
$$

Looking at the denominator of our estimate of $k$, you can see that it will become negative if the sample mean is smaller than the sample variance, but we know that $k$ must be positive. Here the range of our method of moments estimate does not coincide with the range of the parameter to be estimated. The chapter does state that these are not the best estimators of the population parameters. But we have gained something - the estimate for $p$ may be intuitively obvious but we probably wouldn't have been able to come up with that estimate of $k$ without the method of moments.


### Maximum Likelihood Estimators {-}

Our next technique for finding estimators is the method of maximum likelihood. The likelihood function was introduced in the previous chapter. For an iid sample with pdf/pmf $f(x_i | \theta_1 \dots, \theta_k)$ the likelihood function is:

$$
L(\mathbf \theta | \mathbf x) = L(\theta_1, \dots, \theta_n | x_1, \dots, x_m) = \prod_{i=1}^n f(x_i | \theta_1, \dots, \theta_k)
$$

The maximum likelihood parameter value $\hat \theta(\mathbf x)$ is the parameter value for which the likelihood function attains it's maximum value (considered as a function of $\theta$) for a given realised sample $\mathbf x$.

Here, unlike the method of moments, the range of our parameter estimate will always coincide with the range of the parameter. There are two problems with implementing and understanding the properties of MLEs:

* Finding the global maximum and verifying that it is a maximum - often simple differentiation but other approaches are possible. The chapter includes an example of finding the MLE for the mean of a normal distribution by finding a global upper bound for the likelihood and showing that this bound is only obtained at $\theta = \overline X$.
* Numerical sensitivity - how sensitive is our MLE to small changes in the sample?

If we're going to use differentiation to find an MLE, it's often easier to work with log likelihoods instead of likelihoods directly. This works out since the log function is strictly increasing and so the extrema coincide. For example, consider the Bernoulli likelihood

$$
L(p | \mathbf x) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^y(1-p)^{n-y}
$$

for $y = \sum x_i$. This gets easier to differentiate after taking logs:

$$
\log L(p | \mathbf x) = y \log p + (n-y) \log (1-p).
$$

Differentiating and setting to 0 gives:

$$
0 = \frac{y}{p} - \frac{n-y}{1-p}
$$

And some algebra returns the possible solution $p = \frac{y}{n}$, which can be verified as a maximum.


The invariance property of MLEs states that if $\hat \theta$ is the MLE for $\theta$, then $\tau(\hat \theta)$ is the MLE for $\tau( \theta)$ for any function $\tau$. E.g. if $\theta$ is the mean of a normal distribution then the maximum likelihood estimate of $\sin(\theta)$ is $\sin(\overline X)$.

If $\tau$ is one-to-one then it's easy to see that maximising the likelihood as a function of $\theta$ or maxmimising it as a function of  $\tau(\theta)$ makes no difference, but the result holds for any function $\tau(\theta)$.

There's a discussion at the end of the section about numerical instability of MLEs. This occurs when the likelihood function is quite flat near its maximum or there is no finite maximum. For example the MLEs for $k$ and $p$ in binomial sampling can be quite unstable. The chapter says that this is usually not a problem when finding MLEs explicitly but if you're using a numerical approach it is worth investigating the stability around your estimate.

### Bayes Estimators {-}

Revisit.

### Mean Squared Error {-}

The rest of the chapter is concerned with methods of evaluating estimators. The firs method discussed is mean squared error.

**Definition 7.3.1 Mean Squared Error**

The *mean squared error (MSE)* of an estimator $W$ of a parameter $\theta$ is the function of $\theta$ defined by $E_\theta(W-\theta)^2$.

It measures the average squared difference between the estimator $W$ and the parameter $\theta$. Any increasing function of the absolute difference could be reasonable, but MSE is quite tractable analystically and has the useful interpretation:

$$
E_\theta(W-\theta)^2 = \text{Var}_\theta(W) + (E_\theta W - \theta)^2 = \text{Var}_\theta(W)+(\text{Bias}_\theta W)^2
$$

where $\text{Bias}_\theta W = E_\theta W - \theta$.

So a small MSE implies a small combined bias and variance. If an estimator is unbiased, its MSE is equal to its variance. So for example a Normal MSE would have:

$$
\begin{aligned}
E(\overline X - \mu)^2 &= \text{Var}(\overline X) = \frac{\sigma^2}{n} \\
E(S^2 - \sigma^2)^2 &= \text{Var}(S^2) = \frac{2\sigma^4}{n-1}
\end{aligned}
$$

However controlling bias does not guarantee a controlled MSE, and in fact the chapter shows that the biased estimator $\hat\sigma^2$ of $\sigma^2$ has a smaller MSE than $S^2$. The chapter stresses that this does not mean that $\hat\sigma^2$ should necessarily be preferred as an estimator over $S^2$ - it will, for example, systemically underestimate the variance.

Sometimes the MSEs of two estimators will cross each other over the parameter range - one will have lower MSE in some portion of the parameter space, and higher MSE in some other portion.

### Best Unbiased Estimators {-}

The problem of finding a best estimator is difficult in part because of the huge class of candidates. One way of restricting the class of estimators is to consider only the unbiased ones. A best unbiased estimator $W*$ (if it exists) is one with uniformly smallest variance. I.e. the estimator $W*$ is unbiased and for any other unbiased estimator $W$ the variance of $W*$ is less than or equal to the variance of $W$ for all $\theta$.

One way of finding a best unbiased estimator is to specify a lower bound $B(\theta)$ on the variance and then show that the estimator attains it. One example of this approach is the Cramér-Rao lower bound.

**Theorem 7.3.8 Cramér-Rao Inequality**

Let $X_1, \dots, X_n$ be a sample with pdf $f(\mathbf x | \theta)$, and let $W(\mathbf X) = W(X_1, \dots, X_n)$ be any estimator satisfying

$$
\frac{d}{d\theta} E_\theta W(\mathbf X) = \int_{\mathcal X} \frac{\partial}{\partial \theta}[W(\mathbf x)f(\mathbf x| \theta)] d\mathbf x
$$

and 

$$
\text{Var}_\theta W(\mathbf X) < \infty.
$$
Then 

$$
\text{Var}_\theta(W(\mathbf X)) \geq \frac{\left(\frac{d}{d\theta} E_\theta W(\mathbf X)\right)^2}{E_\theta \left( \left(\frac{\partial}{\partial \theta}\log f(\mathbf X | \theta)\right)^2\right)}
$$

Which gives a lower bound of the variance of an estimator, though there's no guarantee than any unbiased estimator will attain this lower bound. Here are the conditions for attainment of the Cramér-Rao lower bound.

**Corollary 7.3.15 Attainment** 
Let $X_1, \dots, X_n$ be iid $f(x | \theta)$, where $f(x | \theta)$ satisfies the conditions of the Cramér-Rao Theorem. Let $L(\theta | \mathbf{x})=\prod_{i=1}^n f\left(x_{i} | \theta\right)$ denote the likelihood function. If $W(\mathbf{X})=W\left(X_{1}, \dots, X_{n}\right)$ is any unbiased estimator of $\tau(\theta)$, then $W(\mathbf{X})$ attains the Cramér-Rao Lower Bound if and only if
$$
a(\theta)[W(\mathbf{x})-\tau(\theta)]=\frac{\partial}{\partial \theta} \log L(\theta \mid \mathbf{x})
$$
for some function $a(\theta)$.

### Sufficiency and Unbiasedness {-}

**Theorem 7.3.17 Rao-Blackwell** 
Let $W$ be any unbiased estimator of $\tau(\theta)$, and let $T$ be a sufficient statistic for $\theta$. Define $\phi(T)= E(W \mid T)$. Then $E_{\theta} \phi(T)=\tau(\theta)$ and $\text{Var}_{\theta} \phi(T) \leq \text{Var}_{\theta} W$ for all $\theta$; that is, $\phi(T)$ is a uniformly better unbiased estimator of $\tau(\theta)$.

Conditioning any unbiased estimator on a sufficient statistic will result in a uniform improvement - when we're looking for best unbiased estimators we only need to consider statistics that are functions of a sufficient statistic.

Need to revisit this section for the relationship between completeness and best unbiased estimators.


### Loss Function Optimality {-}

We can generalise the MSE with the concept of a loss function - any non-negative function that generally increases with the distance between an action/ decision $a$ and parameter. E.g.

Absolute error loss:

$$
L(\theta, a) = |a-\theta|
$$

Squared error loss:

$$
L(\theta, a) = (a-\theta)^2
$$

If circumstances dictate, we may want to penalise overestimates more than overestimates, which can be done with something like this loss function:

$$
L(\theta, a)= \begin{cases}
(a-\theta)^{2} & \text { if } a<\theta \\
10(a-\theta)^{2} & \text { if } a \geq \theta\end{cases}
$$

Evaluating estimators with loss functions is a branch of *decision theory*. With a decision-theoretic appraoch we'll evaluate estimators $\delta(\mathbf x)$ of $\theta$ using the risk function:

$$
R(\theta, \delta) = E_\theta L(\theta , \delta(\mathbf X))
$$
I.e. the average loss that we'll experience by using the estimator. We'd like the risk function to be small for all possible values of $\theta$. Like we saw with MSEs above, often risk functions of two candidate estimators will cross in parameter space. In fact the risk function of the squared error loss is the MSE.



## Questions




## Further Reading {-}
